{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10_pytorch_tf.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxlXJ8t8uShLm2OtzltF9J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c2af21b4f55040569cf9fac75642e9cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_519999d59980440994f4232c6df81419","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_353250367c8847939d47f7d4e6689762","IPY_MODEL_5911be5bb01d48c984a096e12798577c"]}},"519999d59980440994f4232c6df81419":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"353250367c8847939d47f7d4e6689762":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bfd5b76465b64d34aa7b0b57311f8f79","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_08b54800524240168f88472f9b5d755a"}},"5911be5bb01d48c984a096e12798577c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fc5d4404242845b2beaa450b9a1a1155","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 81204638.07it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a552e96cf5c042b0a34f62e1673e7ecb"}},"bfd5b76465b64d34aa7b0b57311f8f79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"08b54800524240168f88472f9b5d755a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc5d4404242845b2beaa450b9a1a1155":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a552e96cf5c042b0a34f62e1673e7ecb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f3d6312effa44c68d2da6ca05f0873e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2ac9d6a8d11445ef93bb4979a7cbed50","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c5cad1227fd54317b31d9005be590950","IPY_MODEL_4fca1546d0374390937756d984b2cf4e"]}},"2ac9d6a8d11445ef93bb4979a7cbed50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c5cad1227fd54317b31d9005be590950":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b3f0f8cc29542ee9a37c56242279160","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31e7dd1820724a50902355fae0e86abe"}},"4fca1546d0374390937756d984b2cf4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_07e679370f6f4c0fa4534c93d1cdf44a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [03:09&lt;00:00, 247kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6ffe308d69c4c9b968856d74ba86089"}},"7b3f0f8cc29542ee9a37c56242279160":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"31e7dd1820724a50902355fae0e86abe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07e679370f6f4c0fa4534c93d1cdf44a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e6ffe308d69c4c9b968856d74ba86089":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69119f15c09f4383be2ab67eac9b9117":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_88969592ed8c405db5c177552e8dcb21","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ef62d208ce254076aea2a93654448e35","IPY_MODEL_6e30be58085a4691acddc8020fd19e61"]}},"88969592ed8c405db5c177552e8dcb21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef62d208ce254076aea2a93654448e35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_00248effddd94374a0514417c5d3ae42","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553507836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553507836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_daaec97779654628a70d9cd8d4c58aa3"}},"6e30be58085a4691acddc8020fd19e61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bfb470a6de7f4881b870a4919138ebdc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:45&lt;00:00, 12.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4542dc9505804cba8056f46a7cf89937"}},"00248effddd94374a0514417c5d3ae42":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"daaec97779654628a70d9cd8d4c58aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bfb470a6de7f4881b870a4919138ebdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4542dc9505804cba8056f46a7cf89937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"YfVJyLm6xPsd"},"source":["# local pc --> colab 파일 업로드\r\n","#  여러개 동시 선택 가능 해야 됨\r\n","from google.colab import files\r\n","\r\n","uploaded = files.upload()\r\n","for fn in uploaded.keys():\r\n","    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTXF5lSeyCW9"},"source":["# cifar 10으로 validation accuracy 높이기\r\n","## 1. pytorch\r\n","-  https://github.com/kuangliu/pytorch-cifar ==> pytorch 코드가 있는데 모델별로 92~95%\r\n","\r\n","- pytorch tutorial의 간단한 모델로 하면, val acc = 65%정도 나온다. https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\r\n","\r\n","- transfer learning: pretrained weight를 고정하면 train이 잘 안된다. 고정하지 않아야 한다.\r\n","- vgg16, resnet18로 data augmentation 적용해서 train하면 val acc=83~85%정도 나온다. vgg16은 lr=0.00005로 해야 한다.\r\n","    * https://github.com/kuangliu/pytorch-cifar --> vgg16(BN이 포함되어 있다)으로 MultiStepLR, 344epoch(190분 소요) . 93.8%\r\n","    * pytorch vgg16(lr=0.00005): 100 epoch train acc = 98%, test acc = 83% 정도 나온다. ==> vgg는 모델이 커서 느리다.\r\n","    * pytorch resnet18 + MultiStepLR:\r\n","        - MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)\r\n","        - 150 epoch 전에 정체되다 150넘어서 lr이 조정되니 77% --> 84%\r\n","        - 214 epoch에 85.5% \r\n","    * pytorch resnet18 + torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  mode = 'max',threshold_mode='abs',threshold=0.001, factor=0.6,patience=10, min_lr=0.0001,verbose=True)\r\n","        - 216 epoch에 87.6% 이후, 더 이상 개선이 안됨\r\n","    * resnet18(kuangliu)+ReduceLROnPlateau: 모델 구조가 좀 다르다. 95.2%(150 epoch)\r\n","    * pytorch vgg16_bn+MultiStepLR: 168 epoch, 92.9%\r\n","\r\n","![image.png](data:image/png;base64,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)\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"WO2pp-UByAFB"},"source":["import torch\r\n","import torchvision\r\n","import torchvision.transforms as transforms\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import torchvision.models as models\r\n","\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import time\r\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["c2af21b4f55040569cf9fac75642e9cc","519999d59980440994f4232c6df81419","353250367c8847939d47f7d4e6689762","5911be5bb01d48c984a096e12798577c","bfd5b76465b64d34aa7b0b57311f8f79","08b54800524240168f88472f9b5d755a","fc5d4404242845b2beaa450b9a1a1155","a552e96cf5c042b0a34f62e1673e7ecb"]},"id":"WO5-y3FZyAuS","executionInfo":{"status":"ok","timestamp":1612444845459,"user_tz":-540,"elapsed":7773,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"f52438dc-9023-4c46-8d7c-afe3904d8fab"},"source":["#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\r\n","\r\n","transform_train = transforms.Compose([\r\n","    transforms.RandomCrop(32, padding=4),  # padding 후, 지정한 크기로 잘라낸다. https://chloes-dl.com/2019/11/13/pytorch101-data-preprocessing-and-augmentation-part-1/\r\n","    transforms.RandomHorizontalFlip(),\r\n","    transforms.ToTensor(),\r\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n","])\r\n","\r\n","transform_test = transforms.Compose([\r\n","    transforms.ToTensor(),\r\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n","])\r\n","\r\n","\r\n","\r\n","batch_size = 128\r\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)  # torchvision.datasets.cifar.CIFAR10\r\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\r\n","\r\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\r\n","\r\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n","print(f'train data: {len(trainset)}, test data: {len(testset)}')\r\n","print(f'# of mini-batch: {len(trainset)//batch_size}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2af21b4f55040569cf9fac75642e9cc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","train data: 50000, test data: 10000\n","# of mini-batch: 390\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["0f3d6312effa44c68d2da6ca05f0873e","2ac9d6a8d11445ef93bb4979a7cbed50","c5cad1227fd54317b31d9005be590950","4fca1546d0374390937756d984b2cf4e","7b3f0f8cc29542ee9a37c56242279160","31e7dd1820724a50902355fae0e86abe","07e679370f6f4c0fa4534c93d1cdf44a","e6ffe308d69c4c9b968856d74ba86089"]},"id":"BJL0CmjF23c5","executionInfo":{"status":"ok","timestamp":1612444849374,"user_tz":-540,"elapsed":2568,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"289214cc-db31-4e68-f4ff-9351cc629459"},"source":["cfg = {\r\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\r\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\r\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\r\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\r\n","}\r\n","\r\n","\r\n","class x_VGG(nn.Module):  # beginner-blitz-cifar10-tutorial-py에 있는 VGG\r\n","    def __init__(self, vgg_name):\r\n","        super(x_VGG, self).__init__()\r\n","        self.features = self._make_layers(cfg[vgg_name])\r\n","        self.classifier = nn.Linear(512, 10)\r\n","\r\n","    def forward(self, x):\r\n","        out = self.features(x)\r\n","        out = out.view(out.size(0), -1)\r\n","        out = self.classifier(out)\r\n","        return out\r\n","\r\n","    def _make_layers(self, cfg):\r\n","        layers = []\r\n","        in_channels = 3\r\n","        for x in cfg:\r\n","            if x == 'M':\r\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\r\n","            else:\r\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\r\n","                           nn.BatchNorm2d(x),\r\n","                           nn.ReLU(inplace=True)]\r\n","                in_channels = x\r\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\r\n","        return nn.Sequential(*layers)\r\n","\r\n","class MyVGG(nn.Module):\r\n","    def __init__(self):\r\n","        super(MyVGG, self).__init__()\r\n","        mode = 2\r\n","        self.flatten = nn.Flatten()\r\n","        if mode ==1:\r\n","            self.vgg = models.vgg16_bn(pretrained=True,progress=True)\r\n","            self.fc = nn.Linear(1000,10)\r\n","        else:\r\n","            self.vgg = models.vgg16_bn(pretrained=True,progress=True).features\r\n","            self.fc = nn.Linear(512,10)\r\n","    def forward(self, x):\r\n","        x = self.vgg(x)\r\n","        x = self.flatten(x)\r\n","        x = self.fc(x)\r\n","        return x\r\n","\r\n","my_resnet = models.resnet18(pretrained=True)\r\n","my_resnet.fc = nn.Linear(my_resnet.fc.in_features,10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f3d6312effa44c68d2da6ca05f0873e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jNki9l623ha","colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["69119f15c09f4383be2ab67eac9b9117","88969592ed8c405db5c177552e8dcb21","ef62d208ce254076aea2a93654448e35","6e30be58085a4691acddc8020fd19e61","00248effddd94374a0514417c5d3ae42","daaec97779654628a70d9cd8d4c58aa3","bfb470a6de7f4881b870a4919138ebdc","4542dc9505804cba8056f46a7cf89937"]},"executionInfo":{"status":"ok","timestamp":1612431265873,"user_tz":-540,"elapsed":18685,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"15136271-6757-46fc-a45c-9fc73072c6c9"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n","\r\n","model_selection = 1  # 1: resnet, 2: vgg, 3. x_VGG\r\n","\r\n","if model_selection ==1:\r\n","    net = my_resnet\r\n","elif model_selection ==2:\r\n","    net = MyVGG()   # lr=0.00005 또는 Adam말고 SGD로 해야됨.\r\n","else:\r\n","    net = x_VGG('VGG16')\r\n","\r\n","\r\n","net.to(device)   \r\n","criterion = nn.CrossEntropyLoss()\r\n","\r\n","if model_selection ==1:\r\n","    #optimizer = optim.SGD(net.parameters(), lr=0.001)\r\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # \r\n","    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)  # train이 정체되다 150 epoch을 지나면서, lr이 바뀌니 확 좋아짐.\r\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  mode = 'max',threshold_mode='abs',threshold=0.001, factor=0.6,patience=10, min_lr=0.0001,verbose=True)\r\n","\r\n","\r\n","elif model_selection ==2:\r\n","    #optimizer = optim.Adam(net.parameters(), lr=0.0001,betas=(0.5, 0.9))\r\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # \r\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)\r\n","\r\n","else:\r\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # for x_VGG\r\n","    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\r\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69119f15c09f4383be2ab67eac9b9117","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553507836.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L4NtIEXC23oM","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1612436753994,"user_tz":-540,"elapsed":5464494,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"36b6a1c0-baca-445f-c161-5804129b6c8a"},"source":["s_time = time.time()\r\n","best_acc = 0\r\n","best_duration=0\r\n","for epoch in range(400):  # loop over the dataset multiple times\r\n","\r\n","    running_loss = []\r\n","    acc = 0\r\n","    total = 0\r\n","    net.train()\r\n","    for i, data in enumerate(trainloader):\r\n","        # get the inputs\r\n","        inputs, labels = data\r\n","        inputs = inputs.to(device)\r\n","        labels = labels.to(device)\r\n","        # zero the parameter gradients\r\n","        optimizer.zero_grad()\r\n","\r\n","        # forward + backward + optimize\r\n","        outputs = net(inputs)\r\n","        loss = criterion(outputs, labels)\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        \r\n","        _, pred = outputs.max(axis=-1)\r\n","        acc  += (pred==labels).float().sum().item()\r\n","        total += len(labels)\r\n","\r\n","\r\n","        # print statistics\r\n","        running_loss.append(loss.item())\r\n","    print('[epoch: %d] loss: %.3f, train acc: %.3f elapsed: %.2f' % (epoch + 1, np.mean(running_loss), acc/total , time.time()-s_time),end=\"\\t\" )\r\n","    #scheduler.step()\r\n","    acc = 0\r\n","    total=0\r\n","    net.eval()\r\n","    with torch.no_grad():\r\n","        for i, data in enumerate(testloader):\r\n","            inputs, labels = data\r\n","            inputs = inputs.to(device)\r\n","            outputs = net(inputs)\r\n","            \r\n","            _, pred = outputs.max(axis=-1)\r\n","            acc  += (pred.cpu()==labels).float().sum().item()\r\n","            total += len(labels)\r\n","    if acc/total > best_acc:\r\n","        print('test acc: %.3f, best: %.3f ===== new best ' % (acc/total,best_acc ))\r\n","        best_acc = acc/total\r\n","        best_duration = 0\r\n","    else:\r\n","        best_duration += 1\r\n","        print('test acc: %.3f, best: %.3f, best duration: %d ' % (acc/total,best_acc, best_duration ) )\r\n","    scheduler.step(acc/total)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[epoch: 1] loss: 0.958, train acc: 0.678 elapsed: 28.92\n","test acc: 0.709, bsest: 0.000 \n","[epoch: 2] loss: 0.667, train acc: 0.776 elapsed: 60.76\n","test acc: 0.767, bsest: 0.709 \n","[epoch: 3] loss: 0.580, train acc: 0.807 elapsed: 93.43\n","test acc: 0.739, bsest: 0.767 \n","[epoch: 4] loss: 0.545, train acc: 0.817 elapsed: 126.05\n","test acc: 0.760, bsest: 0.767 \n","[epoch: 5] loss: 0.515, train acc: 0.827 elapsed: 158.48\n","test acc: 0.766, bsest: 0.767 \n","[epoch: 6] loss: 0.498, train acc: 0.836 elapsed: 191.10\n","test acc: 0.795, bsest: 0.767 \n","[epoch: 7] loss: 0.494, train acc: 0.835 elapsed: 223.65\n","test acc: 0.811, bsest: 0.795 \n","[epoch: 8] loss: 0.483, train acc: 0.838 elapsed: 256.18\n","test acc: 0.791, bsest: 0.811 \n","[epoch: 9] loss: 0.468, train acc: 0.846 elapsed: 288.81\n","test acc: 0.767, bsest: 0.811 \n","[epoch: 10] loss: 0.470, train acc: 0.844 elapsed: 321.49\n","test acc: 0.763, bsest: 0.811 \n","[epoch: 11] loss: 0.464, train acc: 0.846 elapsed: 354.09\n","test acc: 0.767, bsest: 0.811 \n","[epoch: 12] loss: 0.453, train acc: 0.849 elapsed: 386.62\n","test acc: 0.805, bsest: 0.811 \n","[epoch: 13] loss: 0.453, train acc: 0.849 elapsed: 419.17\n","test acc: 0.797, bsest: 0.811 \n","[epoch: 14] loss: 0.441, train acc: 0.855 elapsed: 451.83\n","test acc: 0.822, bsest: 0.811 \n","[epoch: 15] loss: 0.443, train acc: 0.853 elapsed: 484.40\n","test acc: 0.780, bsest: 0.822 \n","[epoch: 16] loss: 0.438, train acc: 0.854 elapsed: 516.95\n","test acc: 0.771, bsest: 0.822 \n","[epoch: 17] loss: 0.430, train acc: 0.856 elapsed: 549.51\n","test acc: 0.834, bsest: 0.822 \n","[epoch: 18] loss: 0.427, train acc: 0.860 elapsed: 582.18\n","test acc: 0.807, bsest: 0.834 \n","[epoch: 19] loss: 0.436, train acc: 0.855 elapsed: 614.74\n","test acc: 0.742, bsest: 0.834 \n","[epoch: 20] loss: 0.422, train acc: 0.860 elapsed: 647.33\n","test acc: 0.775, bsest: 0.834 \n","[epoch: 21] loss: 0.424, train acc: 0.860 elapsed: 679.96\n","test acc: 0.842, bsest: 0.834 \n","[epoch: 22] loss: 0.422, train acc: 0.859 elapsed: 712.50\n","test acc: 0.735, bsest: 0.842 \n","[epoch: 23] loss: 0.418, train acc: 0.862 elapsed: 745.08\n","test acc: 0.757, bsest: 0.842 \n","[epoch: 24] loss: 0.421, train acc: 0.860 elapsed: 777.77\n","test acc: 0.800, bsest: 0.842 \n","[epoch: 25] loss: 0.418, train acc: 0.862 elapsed: 810.34\n","test acc: 0.833, bsest: 0.842 \n","[epoch: 26] loss: 0.421, train acc: 0.860 elapsed: 842.99\n","test acc: 0.800, bsest: 0.842 \n","[epoch: 27] loss: 0.415, train acc: 0.862 elapsed: 875.56\n","test acc: 0.798, bsest: 0.842 \n","[epoch: 28] loss: 0.406, train acc: 0.866 elapsed: 908.07\n","test acc: 0.808, bsest: 0.842 \n","[epoch: 29] loss: 0.408, train acc: 0.864 elapsed: 940.66\n","test acc: 0.830, bsest: 0.842 \n","[epoch: 30] loss: 0.401, train acc: 0.867 elapsed: 973.38\n","test acc: 0.793, bsest: 0.842 \n","[epoch: 31] loss: 0.411, train acc: 0.863 elapsed: 1005.92\n","test acc: 0.819, bsest: 0.842 \n","[epoch: 32] loss: 0.406, train acc: 0.865 elapsed: 1038.44\n","test acc: 0.823, bsest: 0.842 \n","[epoch: 33] loss: 0.407, train acc: 0.864 elapsed: 1070.97\n","test acc: 0.734, bsest: 0.842 \n","[epoch: 34] loss: 0.404, train acc: 0.868 elapsed: 1103.59\n","test acc: 0.818, bsest: 0.842 \n","[epoch: 35] loss: 0.402, train acc: 0.865 elapsed: 1136.09\n","test acc: 0.839, bsest: 0.842 \n","[epoch: 36] loss: 0.400, train acc: 0.870 elapsed: 1168.67\n","test acc: 0.735, bsest: 0.842 \n","[epoch: 37] loss: 0.404, train acc: 0.867 elapsed: 1201.23\n","test acc: 0.799, bsest: 0.842 \n","[epoch: 38] loss: 0.400, train acc: 0.865 elapsed: 1233.80\n","test acc: 0.813, bsest: 0.842 \n","[epoch: 39] loss: 0.402, train acc: 0.867 elapsed: 1266.39\n","test acc: 0.812, bsest: 0.842 \n","[epoch: 40] loss: 0.404, train acc: 0.866 elapsed: 1298.95\n","test acc: 0.825, bsest: 0.842 \n","[epoch: 41] loss: 0.399, train acc: 0.868 elapsed: 1331.43\n","test acc: 0.788, bsest: 0.842 \n","[epoch: 42] loss: 0.401, train acc: 0.866 elapsed: 1363.88\n","test acc: 0.813, bsest: 0.842 \n","[epoch: 43] loss: 0.394, train acc: 0.870 elapsed: 1396.36\n","test acc: 0.848, bsest: 0.842 \n","[epoch: 44] loss: 0.395, train acc: 0.869 elapsed: 1428.95\n","test acc: 0.782, bsest: 0.848 \n","[epoch: 45] loss: 0.400, train acc: 0.868 elapsed: 1461.76\n","test acc: 0.717, bsest: 0.848 \n","[epoch: 46] loss: 0.402, train acc: 0.866 elapsed: 1494.41\n","test acc: 0.822, bsest: 0.848 \n","[epoch: 47] loss: 0.393, train acc: 0.869 elapsed: 1527.04\n","test acc: 0.763, bsest: 0.848 \n","[epoch: 48] loss: 0.400, train acc: 0.867 elapsed: 1559.66\n","test acc: 0.809, bsest: 0.848 \n","[epoch: 49] loss: 0.390, train acc: 0.870 elapsed: 1592.46\n","test acc: 0.813, bsest: 0.848 \n","[epoch: 50] loss: 0.391, train acc: 0.872 elapsed: 1625.06\n","test acc: 0.789, bsest: 0.848 \n","[epoch: 51] loss: 0.391, train acc: 0.871 elapsed: 1657.66\n","test acc: 0.829, bsest: 0.848 \n","[epoch: 52] loss: 0.391, train acc: 0.870 elapsed: 1690.20\n","test acc: 0.785, bsest: 0.848 \n","[epoch: 53] loss: 0.396, train acc: 0.868 elapsed: 1722.72\n","test acc: 0.782, bsest: 0.848 \n","[epoch: 54] loss: 0.391, train acc: 0.870 elapsed: 1755.25\n","test acc: 0.830, bsest: 0.848 \n","[epoch: 55] loss: 0.384, train acc: 0.873 elapsed: 1787.73\n","test acc: 0.822, bsest: 0.848 \n","[epoch: 56] loss: 0.385, train acc: 0.872 elapsed: 1820.23\n","test acc: 0.801, bsest: 0.848 \n","[epoch: 57] loss: 0.389, train acc: 0.870 elapsed: 1852.71\n","test acc: 0.781, bsest: 0.848 \n","[epoch: 58] loss: 0.392, train acc: 0.869 elapsed: 1885.18\n","test acc: 0.805, bsest: 0.848 \n","[epoch: 59] loss: 0.394, train acc: 0.868 elapsed: 1917.61\n","test acc: 0.838, bsest: 0.848 \n","[epoch: 60] loss: 0.386, train acc: 0.872 elapsed: 1950.08\n","test acc: 0.756, bsest: 0.848 \n","[epoch: 61] loss: 0.384, train acc: 0.874 elapsed: 1982.60\n","test acc: 0.827, bsest: 0.848 \n","[epoch: 62] loss: 0.384, train acc: 0.873 elapsed: 2015.05\n","test acc: 0.823, bsest: 0.848 \n","[epoch: 63] loss: 0.384, train acc: 0.871 elapsed: 2047.58\n","test acc: 0.804, bsest: 0.848 \n","[epoch: 64] loss: 0.381, train acc: 0.874 elapsed: 2080.07\n","test acc: 0.743, bsest: 0.848 \n","[epoch: 65] loss: 0.390, train acc: 0.871 elapsed: 2112.58\n","test acc: 0.860, bsest: 0.848 \n","[epoch: 66] loss: 0.389, train acc: 0.871 elapsed: 2145.07\n","test acc: 0.834, bsest: 0.860 \n","[epoch: 67] loss: 0.386, train acc: 0.872 elapsed: 2177.52\n","test acc: 0.758, bsest: 0.860 \n","[epoch: 68] loss: 0.389, train acc: 0.873 elapsed: 2209.99\n","test acc: 0.784, bsest: 0.860 \n","[epoch: 69] loss: 0.385, train acc: 0.873 elapsed: 2242.39\n","test acc: 0.792, bsest: 0.860 \n","[epoch: 70] loss: 0.382, train acc: 0.874 elapsed: 2274.84\n","test acc: 0.852, bsest: 0.860 \n","[epoch: 71] loss: 0.385, train acc: 0.872 elapsed: 2307.34\n","test acc: 0.794, bsest: 0.860 \n","[epoch: 72] loss: 0.377, train acc: 0.875 elapsed: 2339.86\n","test acc: 0.781, bsest: 0.860 \n","[epoch: 73] loss: 0.393, train acc: 0.869 elapsed: 2372.35\n","test acc: 0.787, bsest: 0.860 \n","[epoch: 74] loss: 0.386, train acc: 0.873 elapsed: 2404.83\n","test acc: 0.813, bsest: 0.860 \n","[epoch: 75] loss: 0.376, train acc: 0.876 elapsed: 2437.23\n","test acc: 0.826, bsest: 0.860 \n","[epoch: 76] loss: 0.385, train acc: 0.874 elapsed: 2469.66\n","test acc: 0.828, bsest: 0.860 \n","[epoch: 77] loss: 0.382, train acc: 0.874 elapsed: 2502.06\n","test acc: 0.841, bsest: 0.860 \n","[epoch: 78] loss: 0.382, train acc: 0.874 elapsed: 2534.52\n","test acc: 0.838, bsest: 0.860 \n","[epoch: 79] loss: 0.385, train acc: 0.874 elapsed: 2566.94\n","test acc: 0.803, bsest: 0.860 \n","[epoch: 80] loss: 0.378, train acc: 0.875 elapsed: 2599.40\n","test acc: 0.774, bsest: 0.860 \n","[epoch: 81] loss: 0.382, train acc: 0.873 elapsed: 2631.80\n","test acc: 0.830, bsest: 0.860 \n","[epoch: 82] loss: 0.389, train acc: 0.871 elapsed: 2664.20\n","test acc: 0.706, bsest: 0.860 \n","[epoch: 83] loss: 0.383, train acc: 0.873 elapsed: 2696.60\n","test acc: 0.808, bsest: 0.860 \n","[epoch: 84] loss: 0.379, train acc: 0.874 elapsed: 2728.96\n","test acc: 0.818, bsest: 0.860 \n","[epoch: 85] loss: 0.377, train acc: 0.875 elapsed: 2761.34\n","test acc: 0.807, bsest: 0.860 \n","[epoch: 86] loss: 0.388, train acc: 0.872 elapsed: 2793.80\n","test acc: 0.768, bsest: 0.860 \n","[epoch: 87] loss: 0.379, train acc: 0.876 elapsed: 2826.22\n","test acc: 0.846, bsest: 0.860 \n","[epoch: 88] loss: 0.383, train acc: 0.874 elapsed: 2858.69\n","test acc: 0.818, bsest: 0.860 \n","[epoch: 89] loss: 0.379, train acc: 0.875 elapsed: 2891.13\n","test acc: 0.831, bsest: 0.860 \n","[epoch: 90] loss: 0.376, train acc: 0.875 elapsed: 2923.55\n","test acc: 0.825, bsest: 0.860 \n","[epoch: 91] loss: 0.386, train acc: 0.872 elapsed: 2955.99\n","test acc: 0.832, bsest: 0.860 \n","[epoch: 92] loss: 0.374, train acc: 0.876 elapsed: 2988.43\n","test acc: 0.822, bsest: 0.860 \n","[epoch: 93] loss: 0.385, train acc: 0.873 elapsed: 3020.82\n","test acc: 0.818, bsest: 0.860 \n","[epoch: 94] loss: 0.380, train acc: 0.874 elapsed: 3053.24\n","test acc: 0.816, bsest: 0.860 \n","[epoch: 95] loss: 0.375, train acc: 0.875 elapsed: 3085.66\n","test acc: 0.775, bsest: 0.860 \n","[epoch: 96] loss: 0.378, train acc: 0.875 elapsed: 3118.07\n","test acc: 0.836, bsest: 0.860 \n","[epoch: 97] loss: 0.380, train acc: 0.876 elapsed: 3150.49\n","test acc: 0.814, bsest: 0.860 \n","[epoch: 98] loss: 0.384, train acc: 0.873 elapsed: 3182.93\n","test acc: 0.819, bsest: 0.860 \n","[epoch: 99] loss: 0.377, train acc: 0.876 elapsed: 3215.35\n","test acc: 0.816, bsest: 0.860 \n","[epoch: 100] loss: 0.380, train acc: 0.874 elapsed: 3247.78\n","test acc: 0.807, bsest: 0.860 \n","[epoch: 101] loss: 0.377, train acc: 0.876 elapsed: 3280.16\n","test acc: 0.798, bsest: 0.860 \n","[epoch: 102] loss: 0.371, train acc: 0.878 elapsed: 3312.56\n","test acc: 0.819, bsest: 0.860 \n","[epoch: 103] loss: 0.382, train acc: 0.872 elapsed: 3344.90\n","test acc: 0.835, bsest: 0.860 \n","[epoch: 104] loss: 0.373, train acc: 0.877 elapsed: 3377.31\n","test acc: 0.779, bsest: 0.860 \n","[epoch: 105] loss: 0.375, train acc: 0.876 elapsed: 3409.73\n","test acc: 0.838, bsest: 0.860 \n","[epoch: 106] loss: 0.378, train acc: 0.875 elapsed: 3442.12\n","test acc: 0.803, bsest: 0.860 \n","[epoch: 107] loss: 0.379, train acc: 0.874 elapsed: 3474.56\n","test acc: 0.724, bsest: 0.860 \n","[epoch: 108] loss: 0.375, train acc: 0.875 elapsed: 3507.01\n","test acc: 0.709, bsest: 0.860 \n","[epoch: 109] loss: 0.374, train acc: 0.875 elapsed: 3539.37\n","test acc: 0.829, bsest: 0.860 \n","[epoch: 110] loss: 0.379, train acc: 0.875 elapsed: 3571.74\n","test acc: 0.804, bsest: 0.860 \n","[epoch: 111] loss: 0.377, train acc: 0.877 elapsed: 3604.11\n","test acc: 0.790, bsest: 0.860 \n","[epoch: 112] loss: 0.373, train acc: 0.877 elapsed: 3636.48\n","test acc: 0.834, bsest: 0.860 \n","[epoch: 113] loss: 0.380, train acc: 0.874 elapsed: 3668.86\n","test acc: 0.812, bsest: 0.860 \n","[epoch: 114] loss: 0.377, train acc: 0.875 elapsed: 3701.22\n","test acc: 0.833, bsest: 0.860 \n","[epoch: 115] loss: 0.372, train acc: 0.877 elapsed: 3733.62\n","test acc: 0.811, bsest: 0.860 \n","[epoch: 116] loss: 0.381, train acc: 0.874 elapsed: 3766.07\n","test acc: 0.843, bsest: 0.860 \n","[epoch: 117] loss: 0.374, train acc: 0.876 elapsed: 3798.44\n","test acc: 0.841, bsest: 0.860 \n","[epoch: 118] loss: 0.380, train acc: 0.874 elapsed: 3830.81\n","test acc: 0.791, bsest: 0.860 \n","[epoch: 119] loss: 0.379, train acc: 0.875 elapsed: 3863.20\n","test acc: 0.854, bsest: 0.860 \n","[epoch: 120] loss: 0.370, train acc: 0.879 elapsed: 3895.56\n","test acc: 0.842, bsest: 0.860 \n","[epoch: 121] loss: 0.376, train acc: 0.876 elapsed: 3927.99\n","test acc: 0.778, bsest: 0.860 \n","[epoch: 122] loss: 0.384, train acc: 0.874 elapsed: 3960.36\n","test acc: 0.800, bsest: 0.860 \n","[epoch: 123] loss: 0.377, train acc: 0.875 elapsed: 3992.74\n","test acc: 0.764, bsest: 0.860 \n","[epoch: 124] loss: 0.374, train acc: 0.875 elapsed: 4025.10\n","test acc: 0.815, bsest: 0.860 \n","[epoch: 125] loss: 0.374, train acc: 0.875 elapsed: 4057.45\n","test acc: 0.808, bsest: 0.860 \n","[epoch: 126] loss: 0.372, train acc: 0.877 elapsed: 4089.81\n","test acc: 0.830, bsest: 0.860 \n","[epoch: 127] loss: 0.379, train acc: 0.874 elapsed: 4122.14\n","test acc: 0.836, bsest: 0.860 \n","[epoch: 128] loss: 0.377, train acc: 0.876 elapsed: 4154.50\n","test acc: 0.816, bsest: 0.860 \n","[epoch: 129] loss: 0.378, train acc: 0.876 elapsed: 4186.87\n","test acc: 0.802, bsest: 0.860 \n","[epoch: 130] loss: 0.378, train acc: 0.875 elapsed: 4219.23\n","test acc: 0.747, bsest: 0.860 \n","[epoch: 131] loss: 0.378, train acc: 0.876 elapsed: 4251.54\n","test acc: 0.818, bsest: 0.860 \n","[epoch: 132] loss: 0.370, train acc: 0.878 elapsed: 4283.95\n","test acc: 0.731, bsest: 0.860 \n","[epoch: 133] loss: 0.373, train acc: 0.876 elapsed: 4316.30\n","test acc: 0.773, bsest: 0.860 \n","[epoch: 134] loss: 0.382, train acc: 0.874 elapsed: 4348.69\n","test acc: 0.809, bsest: 0.860 \n","[epoch: 135] loss: 0.368, train acc: 0.878 elapsed: 4381.09\n","test acc: 0.827, bsest: 0.860 \n","[epoch: 136] loss: 0.373, train acc: 0.876 elapsed: 4413.50\n","test acc: 0.815, bsest: 0.860 \n","[epoch: 137] loss: 0.372, train acc: 0.878 elapsed: 4445.83\n","test acc: 0.844, bsest: 0.860 \n","[epoch: 138] loss: 0.377, train acc: 0.876 elapsed: 4478.16\n","test acc: 0.845, bsest: 0.860 \n","[epoch: 139] loss: 0.378, train acc: 0.875 elapsed: 4510.52\n","test acc: 0.824, bsest: 0.860 \n","[epoch: 140] loss: 0.371, train acc: 0.876 elapsed: 4542.76\n","test acc: 0.817, bsest: 0.860 \n","[epoch: 141] loss: 0.370, train acc: 0.876 elapsed: 4575.09\n","test acc: 0.837, bsest: 0.860 \n","[epoch: 142] loss: 0.369, train acc: 0.879 elapsed: 4607.40\n","test acc: 0.796, bsest: 0.860 \n","[epoch: 143] loss: 0.379, train acc: 0.874 elapsed: 4639.72\n","test acc: 0.831, bsest: 0.860 \n","[epoch: 144] loss: 0.370, train acc: 0.879 elapsed: 4672.05\n","test acc: 0.746, bsest: 0.860 \n","[epoch: 145] loss: 0.377, train acc: 0.875 elapsed: 4704.40\n","test acc: 0.816, bsest: 0.860 \n","[epoch: 146] loss: 0.373, train acc: 0.876 elapsed: 4736.76\n","test acc: 0.787, bsest: 0.860 \n","[epoch: 147] loss: 0.374, train acc: 0.875 elapsed: 4769.12\n","test acc: 0.851, bsest: 0.860 \n","[epoch: 148] loss: 0.372, train acc: 0.877 elapsed: 4801.52\n","test acc: 0.770, bsest: 0.860 \n","[epoch: 149] loss: 0.370, train acc: 0.878 elapsed: 4833.86\n","test acc: 0.828, bsest: 0.860 \n","[epoch: 150] loss: 0.378, train acc: 0.874 elapsed: 4866.20\n","test acc: 0.771, bsest: 0.860 \n","[epoch: 151] loss: 0.197, train acc: 0.935 elapsed: 4898.56\n","test acc: 0.917, bsest: 0.860 \n","[epoch: 152] loss: 0.144, train acc: 0.951 elapsed: 4930.99\n","test acc: 0.921, bsest: 0.917 \n","[epoch: 153] loss: 0.126, train acc: 0.958 elapsed: 4963.37\n","test acc: 0.926, bsest: 0.921 \n","[epoch: 154] loss: 0.107, train acc: 0.964 elapsed: 4995.75\n","test acc: 0.926, bsest: 0.926 \n","[epoch: 155] loss: 0.098, train acc: 0.967 elapsed: 5028.16\n","test acc: 0.929, bsest: 0.926 \n","[epoch: 156] loss: 0.088, train acc: 0.971 elapsed: 5060.47\n","test acc: 0.926, bsest: 0.929 \n","[epoch: 157] loss: 0.078, train acc: 0.974 elapsed: 5092.78\n","test acc: 0.926, bsest: 0.929 \n","[epoch: 158] loss: 0.076, train acc: 0.974 elapsed: 5125.17\n","test acc: 0.926, bsest: 0.929 \n","[epoch: 159] loss: 0.067, train acc: 0.978 elapsed: 5157.52\n","test acc: 0.927, bsest: 0.929 \n","[epoch: 160] loss: 0.066, train acc: 0.978 elapsed: 5189.85\n","test acc: 0.929, bsest: 0.929 \n","[epoch: 161] loss: 0.060, train acc: 0.980 elapsed: 5222.22\n","test acc: 0.929, bsest: 0.929 \n","[epoch: 162] loss: 0.054, train acc: 0.982 elapsed: 5254.58\n","test acc: 0.925, bsest: 0.929 \n","[epoch: 163] loss: 0.057, train acc: 0.981 elapsed: 5286.97\n","test acc: 0.926, bsest: 0.929 \n","[epoch: 164] loss: 0.053, train acc: 0.982 elapsed: 5319.38\n","test acc: 0.928, bsest: 0.929 \n","[epoch: 165] loss: 0.051, train acc: 0.983 elapsed: 5351.70\n","test acc: 0.929, bsest: 0.929 \n","[epoch: 166] loss: 0.051, train acc: 0.983 elapsed: 5384.05\n","test acc: 0.917, bsest: 0.929 \n","[epoch: 167] loss: 0.051, train acc: 0.983 elapsed: 5416.34\n","test acc: 0.921, bsest: 0.929 \n","[epoch: 168] loss: 0.051, train acc: 0.982 elapsed: 5448.70\n","test acc: 0.925, bsest: 0.929 \n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-63245575eca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0macc\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"_RWsdOpb4h3_"},"source":["# test accuracy\r\n","correct = 0\r\n","total = 0\r\n","labels_all = []\r\n","predicted_all = []\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        images, labels = data\r\n","        outputs = net(images.to(device))\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","        total += labels.size(0)\r\n","        correct += (predicted.cpu() == labels).sum().item()\r\n","\r\n","        labels_all.extend(labels.numpy())\r\n","        predicted_all.extend(predicted.cpu().numpy())\r\n","\r\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9rel3E14wU5"},"source":["from sklearn.metrics import confusion_matrix\r\n","import seaborn as sns\r\n","confusion_matrix = confusion_matrix(labels_all, predicted_all)  # numpy array\r\n","sns.heatmap(confusion_matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g',cmap=\"YlGnBu\")  # cmap=\"Blues\"\r\n","plt.xlabel('Prediction')\r\n","plt.ylabel('Label')\r\n","plt.tight_layout()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wfh5GO-Z40BO"},"source":["### torchsummay로 모델 구조 살펴보기\r\n","- pip install torchsummary"]},{"cell_type":"code","metadata":{"id":"5PHpFk8F44Ct"},"source":["from torchsummary import summary\r\n","#my_resnet.to('cpu')\r\n","\r\n","net = MyVGG()\r\n","summary(net, input_size=(3, 32, 32),device='cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5XS6Ukb47Sy"},"source":["## 2. Tensorflow\r\n","- tensorflow는 resnet18을 제공하지 않는다. resnet50이 resnet중에는 제일 작은 모델\r\n","- resnet.preprocess_input은 별 효과 없다. 자체적인 augmentation이 낫다.\r\n","- resnet50이 규모가 있기 때문에, classifier의 fc에 dropout을 적용하는 것이 낫다.\r\n","- clipnorm은 별 효과 없다.\r\n","\r\n","    - resnet50: 100 epoch, train acc = 99%, val acc = 79%~80%\r\n","    - resnet18: 200 epoch, train acc = 95%, val acc = 80%\r\n","\r\n","- pytorch만큼 성능이 안 나옴. 개선책\r\n","    * https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/ 여기 있는 좀 더 간단한 구조로 하면 90%까지 나옴\r\n","    * resnet 모델 수정 1: 초반의 7x7을 3x3으로 수정하고, maxpooling 제거: 87.52%(100 epoch)\r\n","    * kuangliu의 pytorch 코드를 Tensorflow 코드로 동일하게 변환"]},{"cell_type":"code","metadata":{"id":"oK-Hyv4280oP","executionInfo":{"status":"ok","timestamp":1612571206248,"user_tz":-540,"elapsed":1291,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import tensorflow as tf\r\n","from tensorflow.keras.applications import resnet\r\n","import matplotlib.pyplot as plt\r\n","import os\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEXreONY480s","executionInfo":{"status":"ok","timestamp":1612570995014,"user_tz":-540,"elapsed":1913,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["# https://github.com/calmisential/TensorFlow2.0_ResNet/tree/master/models\r\n","\r\n","NUM_CLASSES = 10\r\n","\r\n","class BasicBlock(tf.keras.layers.Layer):\r\n","\r\n","    def __init__(self, filter_num, stride=1):\r\n","        super(BasicBlock, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=stride,\r\n","                                            padding=\"same\",use_bias=False)\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=1,\r\n","                                            padding=\"same\",use_bias=False)\r\n","        self.bn2 = tf.keras.layers.BatchNormalization()\r\n","        if stride != 1:\r\n","            self.downsample = tf.keras.Sequential()\r\n","            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                                       kernel_size=(1, 1),\r\n","                                                       strides=stride,use_bias=False))\r\n","            self.downsample.add(tf.keras.layers.BatchNormalization())\r\n","        else:\r\n","            self.downsample = lambda x: x\r\n","\r\n","    def call(self, inputs, training=None, **kwargs):\r\n","        residual = self.downsample(inputs)\r\n","\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.bn2(x, training=training)\r\n","\r\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\r\n","\r\n","        return output\r\n","\r\n","\r\n","class BottleNeck(tf.keras.layers.Layer):\r\n","    def __init__(self, filter_num, stride=1):\r\n","        super(BottleNeck, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(1, 1),\r\n","                                            strides=1,\r\n","                                            padding='same',use_bias=False)\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=stride,\r\n","                                            padding='same',use_bias=False)\r\n","        self.bn2 = tf.keras.layers.BatchNormalization()\r\n","        self.conv3 = tf.keras.layers.Conv2D(filters=filter_num * 4,\r\n","                                            kernel_size=(1, 1),\r\n","                                            strides=1,\r\n","                                            padding='same',use_bias=False)\r\n","        self.bn3 = tf.keras.layers.BatchNormalization()\r\n","\r\n","        self.downsample = tf.keras.Sequential()\r\n","        self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num * 4,\r\n","                                                   kernel_size=(1, 1),\r\n","                                                   strides=stride,use_bias=False))\r\n","        self.downsample.add(tf.keras.layers.BatchNormalization())\r\n","\r\n","    def call(self, inputs, training=None, **kwargs):\r\n","        residual = self.downsample(inputs)\r\n","\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.bn2(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = self.bn3(x, training=training)\r\n","\r\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\r\n","\r\n","        return output\r\n","\r\n","\r\n","def make_basic_block_layer(filter_num, blocks, stride=1):\r\n","    res_block = tf.keras.Sequential()\r\n","    res_block.add(BasicBlock(filter_num, stride=stride))\r\n","\r\n","    for _ in range(1, blocks):\r\n","        res_block.add(BasicBlock(filter_num, stride=1))\r\n","\r\n","    return res_block\r\n","\r\n","\r\n","def make_bottleneck_layer(filter_num, blocks, stride=1):\r\n","    res_block = tf.keras.Sequential()\r\n","    res_block.add(BottleNeck(filter_num, stride=stride))\r\n","\r\n","    for _ in range(1, blocks):\r\n","        res_block.add(BottleNeck(filter_num, stride=1))\r\n","\r\n","    return res_block\r\n","class ResNetTypeI(tf.keras.Model):\r\n","    def __init__(self, layer_params):\r\n","        super(ResNetTypeI, self).__init__()\r\n","\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\r\n","                                            kernel_size=(7, 7),\r\n","                                            strides=2,\r\n","                                            padding=\"same\",use_bias=False)\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\r\n","                                               strides=2,\r\n","                                               padding=\"same\")\r\n","\r\n","        self.layer1 = make_basic_block_layer(filter_num=64,\r\n","                                             blocks=layer_params[0])\r\n","        self.layer2 = make_basic_block_layer(filter_num=128,\r\n","                                             blocks=layer_params[1],\r\n","                                             stride=2)\r\n","        self.layer3 = make_basic_block_layer(filter_num=256,\r\n","                                             blocks=layer_params[2],\r\n","                                             stride=2)\r\n","        self.layer4 = make_basic_block_layer(filter_num=512,\r\n","                                             blocks=layer_params[3],\r\n","                                             stride=2)\r\n","\r\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\r\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=None)\r\n","\r\n","    def call(self, inputs, training=None, mask=None):\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.pool1(x)\r\n","        x = self.layer1(x, training=training)\r\n","        x = self.layer2(x, training=training)\r\n","        x = self.layer3(x, training=training)\r\n","        x = self.layer4(x, training=training)\r\n","        x = self.avgpool(x)\r\n","        output = self.fc(x)\r\n","\r\n","        return output\r\n","\r\n","\r\n","class ResNetTypeII(tf.keras.Model):\r\n","    def __init__(self, layer_params):\r\n","        super(ResNetTypeII, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\r\n","                                            kernel_size=(7, 7),\r\n","                                            strides=2,\r\n","                                            padding=\"same\",use_bias=False)\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\r\n","                                               strides=2,\r\n","                                               padding=\"same\")\r\n","\r\n","        self.layer1 = make_bottleneck_layer(filter_num=64,\r\n","                                            blocks=layer_params[0])\r\n","        self.layer2 = make_bottleneck_layer(filter_num=128,\r\n","                                            blocks=layer_params[1],\r\n","                                            stride=2)\r\n","        self.layer3 = make_bottleneck_layer(filter_num=256,\r\n","                                            blocks=layer_params[2],\r\n","                                            stride=2)\r\n","        self.layer4 = make_bottleneck_layer(filter_num=512,\r\n","                                            blocks=layer_params[3],\r\n","                                            stride=2)\r\n","\r\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\r\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=None)\r\n","\r\n","    def call(self, inputs, training=None, mask=None):\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.pool1(x)\r\n","        x = self.layer1(x, training=training)\r\n","        x = self.layer2(x, training=training)\r\n","        x = self.layer3(x, training=training)\r\n","        x = self.layer4(x, training=training)\r\n","        x = self.avgpool(x)\r\n","        output = self.fc(x)\r\n","\r\n","        return output\r\n","\r\n","\r\n","def resnet_18():\r\n","    return ResNetTypeI(layer_params=[2, 2, 2, 2])\r\n","\r\n","\r\n","def resnet_34():\r\n","    return ResNetTypeI(layer_params=[3, 4, 6, 3])\r\n","\r\n","\r\n","def resnet_50():\r\n","    return ResNetTypeII(layer_params=[3, 4, 6, 3])\r\n","\r\n","\r\n","def resnet_101():\r\n","    return ResNetTypeII(layer_params=[3, 4, 23, 3])\r\n","\r\n","\r\n","def resnet_152():\r\n","    return ResNetTypeII(layer_params=[3, 8, 36, 3])\r\n","\r\n","\r\n","class CommonLayers (tf.keras.Model):\r\n","    def __init__ (self, flag = 0):\r\n","        super().__init__()\r\n","        self.flag = flag\r\n","        if flag != 0:\r\n","            self.conv = tf.keras.layers.Conv2D(filters = flag, kernel_size = 3, padding=\"SAME\", kernel_initializer='glorot_uniform', use_bias=False)\r\n","        self.bn = tf.keras.layers.BatchNormalization ()\r\n","\r\n","    def call (self, inputs):\r\n","        if self.flag == 0:\r\n","            return tf.nn.relu (self.bn (inputs))\r\n","        else:\r\n","            return tf.nn.relu (self.bn (self.conv (inputs)))\r\n","\r\n","\r\n","\r\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXwDIQu70_o","executionInfo":{"status":"ok","timestamp":1612570999048,"user_tz":-540,"elapsed":1127,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["def plot_history(history):\r\n","    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\r\n","\r\n","        :param history: Training history of model\r\n","        :return:\r\n","    \"\"\"\r\n","\r\n","    fig, axs = plt.subplots(1,2)\r\n","\r\n","    # create accuracy sublpot\r\n","    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\r\n","    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\r\n","    axs[0].set_ylabel(\"Accuracy\")\r\n","    axs[0].legend(loc=\"lower right\")\r\n","    axs[0].set_title(\"Accuracy eval\")\r\n","\r\n","    # create error sublpot\r\n","    axs[1].plot(history.history[\"loss\"], label=\"train error\")\r\n","    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\r\n","    axs[1].set_ylabel(\"Error\")\r\n","    axs[1].set_xlabel(\"Epoch\")\r\n","    axs[1].legend(loc=\"upper right\")\r\n","    axs[1].set_title(\"Error eval\")\r\n","\r\n","    plt.show()\r\n","def my_preprocessing(image,label):\r\n","\r\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\r\n","\r\n","    image = tf.image.random_flip_left_right(image)  # 확률 50%로 고정되어 있음.\r\n","    image = tf.image.random_brightness(image, max_delta=0.3)\r\n","    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)  # 채도 조절\r\n","    # random crop\r\n","    shape = tf.shape(image)  # batch size 알아내기\r\n","    image = tf.image.resize_with_pad(image, 40,40)\r\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\r\n","\r\n","    image = tf.clip_by_value(image, 0.0, 1.0)\r\n","\r\n","    return image, label\r\n","\r\n","def my_preprocessing2(image,label):\r\n","\r\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\r\n","    # random crop\r\n","    shape = tf.shape(image)  # batch size 알아내기\r\n","    image = tf.image.resize_with_pad(image, 36,36)\r\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\r\n","    image = tf.image.random_flip_left_right(image)\r\n","\r\n","    image = tf.clip_by_value(image, 0.0, 1.0)\r\n","    \r\n","\r\n","    return image, label\r\n","def normalize(image,label):\r\n","    return (image - [0.4914, 0.4822, 0.4465] )/ [0.2023, 0.1994, 0.2010], label\r\n","\r\n","def pad_and_random_crop(image):\r\n","    image = tf.image.resize_with_pad(image, 36,36)\r\n","    image = tf.image.random_crop(image,(32,32,3))   \r\n","    return image\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"buOnVIWR71E4","executionInfo":{"status":"ok","timestamp":1612571955962,"user_tz":-540,"elapsed":591,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["model_selection = 3   # 1: resnet50, 2: resnet18\r\n","\r\n","if model_selection==1:\r\n","    base_model = resnet.ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))  # weights=None, weights='imagenet'\r\n","    base_model.trainable = True\r\n","\r\n","    my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dropout(0.5),tf.keras.layers.Dense(10)])\r\n","elif model_selection==2:\r\n","    my_resnet = resnet_18()\r\n","else:\r\n","    base_model = ResNetX18()\r\n","    my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaXJdNoF8DEz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612571939946,"user_tz":-540,"elapsed":1393,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"f2d8e777-e33b-447e-e207-b027e11f3cd6"},"source":["# data loading\r\n","batch_size = 128\r\n","\r\n","DATA_MODE = 1 # 1: tf.data.Dataset  2: tf.ImageDataGenerator\r\n","\r\n","if DATA_MODE==2:\r\n","    print('Data from  from_tensor_slices')\r\n","    resnet_pre = False\r\n","\r\n","    if resnet_pre:\r\n","        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","        print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","        train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\r\n","        train_dataset = train_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","\r\n","        train_dataset = train_dataset.shuffle(1000).batch(batch_size)\r\n","\r\n","\r\n","        valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test, tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\r\n","        valid_dataset = valid_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","        valid_dataset = valid_dataset.batch(batch_size)\r\n","\r\n","    else:\r\n","        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","        print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","        train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\r\n","\r\n","        train_dataset = train_dataset.shuffle(1000).batch(batch_size)\r\n","        train_dataset = train_dataset.map(my_preprocessing2)\r\n","        train_dataset = train_dataset.map(normalize)\r\n","\r\n","        valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test/255., tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\r\n","\r\n","        valid_dataset = valid_dataset.batch(batch_size)\r\n","        valid_dataset = valid_dataset.map(normalize)\r\n","else:\r\n","    print('Data from  ImageDataGenerator')\r\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","\r\n","    train_datagen = ImageDataGenerator(rescale=1./255,\r\n","        rotation_range=15,\r\n","        width_shift_range=0.1,\r\n","        height_shift_range=0.1,\r\n","        horizontal_flip=True,\r\n","        preprocessing_function=None,\r\n","        )\r\n","    train_dataset = train_datagen.flow(x_train, y_train.reshape(-1), batch_size=batch_size,shuffle=True)\r\n","\r\n","    valid_datagen = ImageDataGenerator(rescale=1./255)\r\n","    valid_dataset = valid_datagen.flow(x_test, y_test.reshape(-1), batch_size=batch_size)    \r\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Data from  ImageDataGenerator\n","(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rNcgOvl7-Aoj","executionInfo":{"status":"ok","timestamp":1612571966793,"user_tz":-540,"elapsed":600,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["initial_learning_rate = 0.1\r\n","lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\r\n","                            monitor='val_accuracy',  # val loss를 기준으로...\r\n","                            factor=0.6,          # callback 호출시 lr = factor*lr\r\n","                            patience=10,         # patience epoch동안 monitor값이 좋아지지 않으면 callback 작동\r\n","                            cooldown=0,         # lr이 변경된 후, cooldonw동안에는 callback이 작동 안 한다.\r\n","                            min_lr=0.0001, \r\n","                            verbose = 1,min_delta=0.001)\r\n","\r\n","\r\n","\r\n","optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate,momentum=0.9)  # lr or learning_rate\r\n","\r\n","my_resnet.compile(optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\r\n","model_filename = \"Epoch-{epoch:02d}-{val_accuracy:.4f}\"\r\n","checkpoint_path = os.path.join('models/', model_filename)\r\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,mode='auto',save_best_only=True,monitor='val_accuracy')"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRm0a3ms7k2K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2128b046-1211-4116-f4c8-b7e9e385f28e"},"source":["history = my_resnet.fit(train_dataset,epochs=200,validation_data=valid_dataset,validation_freq=1, verbose=1,callbacks=[cp_callback,lr_callback],initial_epoch=111)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","391/391 [==============================] - 46s 112ms/step - loss: 3.1407 - accuracy: 0.1752 - val_loss: 2.1312 - val_accuracy: 0.2606\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.26060, saving model to models/Epoch-01-0.2606\n","Epoch 2/200\n","391/391 [==============================] - 45s 115ms/step - loss: 1.6244 - accuracy: 0.3941 - val_loss: 2.0992 - val_accuracy: 0.3371\n","\n","Epoch 00002: val_accuracy improved from 0.26060 to 0.33710, saving model to models/Epoch-02-0.3371\n","Epoch 3/200\n","391/391 [==============================] - 46s 118ms/step - loss: 1.4166 - accuracy: 0.4800 - val_loss: 1.7199 - val_accuracy: 0.4453\n","\n","Epoch 00003: val_accuracy improved from 0.33710 to 0.44530, saving model to models/Epoch-03-0.4453\n","Epoch 4/200\n","391/391 [==============================] - 48s 122ms/step - loss: 1.2180 - accuracy: 0.5557 - val_loss: 1.2162 - val_accuracy: 0.5846\n","\n","Epoch 00004: val_accuracy improved from 0.44530 to 0.58460, saving model to models/Epoch-04-0.5846\n","Epoch 5/200\n","391/391 [==============================] - 48s 122ms/step - loss: 1.0722 - accuracy: 0.6116 - val_loss: 1.2738 - val_accuracy: 0.5575\n","\n","Epoch 00005: val_accuracy did not improve from 0.58460\n","Epoch 6/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.9460 - accuracy: 0.6658 - val_loss: 1.1680 - val_accuracy: 0.6168\n","\n","Epoch 00006: val_accuracy improved from 0.58460 to 0.61680, saving model to models/Epoch-06-0.6168\n","Epoch 7/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.8292 - accuracy: 0.7083 - val_loss: 1.0940 - val_accuracy: 0.6409\n","\n","Epoch 00007: val_accuracy improved from 0.61680 to 0.64090, saving model to models/Epoch-07-0.6409\n","Epoch 8/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.7665 - accuracy: 0.7285 - val_loss: 1.3011 - val_accuracy: 0.5886\n","\n","Epoch 00008: val_accuracy did not improve from 0.64090\n","Epoch 9/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.7051 - accuracy: 0.7524 - val_loss: 0.6821 - val_accuracy: 0.7656\n","\n","Epoch 00009: val_accuracy improved from 0.64090 to 0.76560, saving model to models/Epoch-09-0.7656\n","Epoch 10/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.6253 - accuracy: 0.7794 - val_loss: 0.7303 - val_accuracy: 0.7588\n","\n","Epoch 00010: val_accuracy did not improve from 0.76560\n","Epoch 11/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.5822 - accuracy: 0.7970 - val_loss: 0.9026 - val_accuracy: 0.7147\n","\n","Epoch 00011: val_accuracy did not improve from 0.76560\n","Epoch 12/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.5351 - accuracy: 0.8163 - val_loss: 0.6674 - val_accuracy: 0.7745\n","\n","Epoch 00012: val_accuracy improved from 0.76560 to 0.77450, saving model to models/Epoch-12-0.7745\n","Epoch 13/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.4839 - accuracy: 0.8310 - val_loss: 0.7756 - val_accuracy: 0.7680\n","\n","Epoch 00013: val_accuracy did not improve from 0.77450\n","Epoch 14/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.4557 - accuracy: 0.8407 - val_loss: 0.6934 - val_accuracy: 0.7726\n","\n","Epoch 00014: val_accuracy did not improve from 0.77450\n","Epoch 15/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.4227 - accuracy: 0.8516 - val_loss: 0.5238 - val_accuracy: 0.8224\n","\n","Epoch 00015: val_accuracy improved from 0.77450 to 0.82240, saving model to models/Epoch-15-0.8224\n","Epoch 16/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.3961 - accuracy: 0.8634 - val_loss: 0.7012 - val_accuracy: 0.7766\n","\n","Epoch 00016: val_accuracy did not improve from 0.82240\n","Epoch 17/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.3711 - accuracy: 0.8716 - val_loss: 0.6123 - val_accuracy: 0.8058\n","\n","Epoch 00017: val_accuracy did not improve from 0.82240\n","Epoch 18/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.3566 - accuracy: 0.8759 - val_loss: 0.7285 - val_accuracy: 0.7751\n","\n","Epoch 00018: val_accuracy did not improve from 0.82240\n","Epoch 19/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.3355 - accuracy: 0.8826 - val_loss: 0.5070 - val_accuracy: 0.8331\n","\n","Epoch 00019: val_accuracy improved from 0.82240 to 0.83310, saving model to models/Epoch-19-0.8331\n","Epoch 20/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.3172 - accuracy: 0.8895 - val_loss: 0.4859 - val_accuracy: 0.8480\n","\n","Epoch 00020: val_accuracy improved from 0.83310 to 0.84800, saving model to models/Epoch-20-0.8480\n","Epoch 21/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.2951 - accuracy: 0.8962 - val_loss: 0.5504 - val_accuracy: 0.8288\n","\n","Epoch 00021: val_accuracy did not improve from 0.84800\n","Epoch 22/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.2808 - accuracy: 0.9021 - val_loss: 0.6387 - val_accuracy: 0.8177\n","\n","Epoch 00022: val_accuracy did not improve from 0.84800\n","Epoch 23/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.2604 - accuracy: 0.9081 - val_loss: 0.4513 - val_accuracy: 0.8581\n","\n","Epoch 00023: val_accuracy improved from 0.84800 to 0.85810, saving model to models/Epoch-23-0.8581\n","Epoch 24/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.2440 - accuracy: 0.9135 - val_loss: 0.5368 - val_accuracy: 0.8430\n","\n","Epoch 00024: val_accuracy did not improve from 0.85810\n","Epoch 25/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.2357 - accuracy: 0.9172 - val_loss: 0.4762 - val_accuracy: 0.8524\n","\n","Epoch 00025: val_accuracy did not improve from 0.85810\n","Epoch 26/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.2326 - accuracy: 0.9168 - val_loss: 0.4622 - val_accuracy: 0.8565\n","\n","Epoch 00026: val_accuracy did not improve from 0.85810\n","Epoch 27/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.2207 - accuracy: 0.9217 - val_loss: 0.5235 - val_accuracy: 0.8429\n","\n","Epoch 00027: val_accuracy did not improve from 0.85810\n","Epoch 28/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.2047 - accuracy: 0.9288 - val_loss: 0.3923 - val_accuracy: 0.8811\n","\n","Epoch 00028: val_accuracy improved from 0.85810 to 0.88110, saving model to models/Epoch-28-0.8811\n","Epoch 29/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1972 - accuracy: 0.9289 - val_loss: 0.5159 - val_accuracy: 0.8520\n","\n","Epoch 00029: val_accuracy did not improve from 0.88110\n","Epoch 30/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1866 - accuracy: 0.9349 - val_loss: 0.4410 - val_accuracy: 0.8715\n","\n","Epoch 00030: val_accuracy did not improve from 0.88110\n","Epoch 31/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1796 - accuracy: 0.9368 - val_loss: 0.5099 - val_accuracy: 0.8583\n","\n","Epoch 00031: val_accuracy did not improve from 0.88110\n","Epoch 32/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1725 - accuracy: 0.9389 - val_loss: 0.5467 - val_accuracy: 0.8494\n","\n","Epoch 00032: val_accuracy did not improve from 0.88110\n","Epoch 33/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1567 - accuracy: 0.9439 - val_loss: 0.4215 - val_accuracy: 0.8772\n","\n","Epoch 00033: val_accuracy did not improve from 0.88110\n","Epoch 34/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1491 - accuracy: 0.9478 - val_loss: 0.4810 - val_accuracy: 0.8696\n","\n","Epoch 00034: val_accuracy did not improve from 0.88110\n","Epoch 35/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1427 - accuracy: 0.9494 - val_loss: 0.5543 - val_accuracy: 0.8533\n","\n","Epoch 00035: val_accuracy did not improve from 0.88110\n","Epoch 36/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1375 - accuracy: 0.9528 - val_loss: 0.4367 - val_accuracy: 0.8734\n","\n","Epoch 00036: val_accuracy did not improve from 0.88110\n","Epoch 37/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1369 - accuracy: 0.9511 - val_loss: 0.4336 - val_accuracy: 0.8771\n","\n","Epoch 00037: val_accuracy did not improve from 0.88110\n","Epoch 38/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1294 - accuracy: 0.9540 - val_loss: 0.4142 - val_accuracy: 0.8844\n","\n","Epoch 00038: val_accuracy improved from 0.88110 to 0.88440, saving model to models/Epoch-38-0.8844\n","Epoch 39/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1244 - accuracy: 0.9571 - val_loss: 0.5407 - val_accuracy: 0.8631\n","\n","Epoch 00039: val_accuracy did not improve from 0.88440\n","Epoch 40/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1148 - accuracy: 0.9600 - val_loss: 0.5949 - val_accuracy: 0.8524\n","\n","Epoch 00040: val_accuracy did not improve from 0.88440\n","Epoch 41/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1121 - accuracy: 0.9597 - val_loss: 0.5341 - val_accuracy: 0.8669\n","\n","Epoch 00041: val_accuracy did not improve from 0.88440\n","Epoch 42/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1119 - accuracy: 0.9592 - val_loss: 0.4115 - val_accuracy: 0.8874\n","\n","Epoch 00042: val_accuracy improved from 0.88440 to 0.88740, saving model to models/Epoch-42-0.8874\n","Epoch 43/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.1069 - accuracy: 0.9615 - val_loss: 0.5615 - val_accuracy: 0.8696\n","\n","Epoch 00043: val_accuracy did not improve from 0.88740\n","Epoch 44/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0995 - accuracy: 0.9641 - val_loss: 0.4707 - val_accuracy: 0.8830\n","\n","Epoch 00044: val_accuracy did not improve from 0.88740\n","Epoch 45/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.1017 - accuracy: 0.9648 - val_loss: 0.5307 - val_accuracy: 0.8701\n","\n","Epoch 00045: val_accuracy did not improve from 0.88740\n","Epoch 46/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0968 - accuracy: 0.9658 - val_loss: 0.5305 - val_accuracy: 0.8803\n","\n","Epoch 00046: val_accuracy did not improve from 0.88740\n","Epoch 47/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0963 - accuracy: 0.9667 - val_loss: 0.4643 - val_accuracy: 0.8890\n","\n","Epoch 00047: val_accuracy improved from 0.88740 to 0.88900, saving model to models/Epoch-47-0.8890\n","Epoch 48/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0833 - accuracy: 0.9707 - val_loss: 0.4303 - val_accuracy: 0.8891\n","\n","Epoch 00048: val_accuracy improved from 0.88900 to 0.88910, saving model to models/Epoch-48-0.8891\n","Epoch 49/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0812 - accuracy: 0.9705 - val_loss: 0.5280 - val_accuracy: 0.8811\n","\n","Epoch 00049: val_accuracy did not improve from 0.88910\n","Epoch 50/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0806 - accuracy: 0.9722 - val_loss: 0.4375 - val_accuracy: 0.8912\n","\n","Epoch 00050: val_accuracy improved from 0.88910 to 0.89120, saving model to models/Epoch-50-0.8912\n","Epoch 51/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0801 - accuracy: 0.9719 - val_loss: 0.5010 - val_accuracy: 0.8847\n","\n","Epoch 00051: val_accuracy did not improve from 0.89120\n","Epoch 52/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0764 - accuracy: 0.9729 - val_loss: 0.4410 - val_accuracy: 0.8967\n","\n","Epoch 00052: val_accuracy improved from 0.89120 to 0.89670, saving model to models/Epoch-52-0.8967\n","Epoch 53/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0711 - accuracy: 0.9749 - val_loss: 0.4727 - val_accuracy: 0.8946\n","\n","Epoch 00053: val_accuracy did not improve from 0.89670\n","Epoch 54/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0740 - accuracy: 0.9748 - val_loss: 0.5334 - val_accuracy: 0.8818\n","\n","Epoch 00054: val_accuracy did not improve from 0.89670\n","Epoch 55/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0714 - accuracy: 0.9750 - val_loss: 0.5437 - val_accuracy: 0.8813\n","\n","Epoch 00055: val_accuracy did not improve from 0.89670\n","Epoch 56/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0748 - accuracy: 0.9734 - val_loss: 0.4876 - val_accuracy: 0.8896\n","\n","Epoch 00056: val_accuracy did not improve from 0.89670\n","Epoch 57/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0641 - accuracy: 0.9777 - val_loss: 0.7201 - val_accuracy: 0.8597\n","\n","Epoch 00057: val_accuracy did not improve from 0.89670\n","Epoch 58/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 0.6201 - val_accuracy: 0.8715\n","\n","Epoch 00058: val_accuracy did not improve from 0.89670\n","Epoch 59/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0565 - accuracy: 0.9794 - val_loss: 0.5806 - val_accuracy: 0.8786\n","\n","Epoch 00059: val_accuracy did not improve from 0.89670\n","Epoch 60/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0564 - accuracy: 0.9796 - val_loss: 0.4947 - val_accuracy: 0.8879\n","\n","Epoch 00060: val_accuracy did not improve from 0.89670\n","Epoch 61/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0622 - accuracy: 0.9782 - val_loss: 0.4846 - val_accuracy: 0.8967\n","\n","Epoch 00061: val_accuracy did not improve from 0.89670\n","Epoch 62/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0507 - accuracy: 0.9832 - val_loss: 0.4977 - val_accuracy: 0.8866\n","\n","Epoch 00062: val_accuracy did not improve from 0.89670\n","\n","Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.06000000089406967.\n","Epoch 63/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0383 - accuracy: 0.9867 - val_loss: 0.4665 - val_accuracy: 0.9018\n","\n","Epoch 00063: val_accuracy improved from 0.89670 to 0.90180, saving model to models/Epoch-63-0.9018\n","Epoch 64/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0285 - accuracy: 0.9901 - val_loss: 0.4491 - val_accuracy: 0.9063\n","\n","Epoch 00064: val_accuracy improved from 0.90180 to 0.90630, saving model to models/Epoch-64-0.9063\n","Epoch 65/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.4832 - val_accuracy: 0.9052\n","\n","Epoch 00065: val_accuracy did not improve from 0.90630\n","Epoch 66/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.4954 - val_accuracy: 0.9036\n","\n","Epoch 00066: val_accuracy did not improve from 0.90630\n","Epoch 67/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.4913 - val_accuracy: 0.9036\n","\n","Epoch 00067: val_accuracy did not improve from 0.90630\n","Epoch 68/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 0.5710 - val_accuracy: 0.8946\n","\n","Epoch 00068: val_accuracy did not improve from 0.90630\n","Epoch 69/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.4932 - val_accuracy: 0.9053\n","\n","Epoch 00069: val_accuracy did not improve from 0.90630\n","Epoch 70/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.5545 - val_accuracy: 0.8983\n","\n","Epoch 00070: val_accuracy did not improve from 0.90630\n","Epoch 71/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.4874 - val_accuracy: 0.9105\n","\n","Epoch 00071: val_accuracy improved from 0.90630 to 0.91050, saving model to models/Epoch-71-0.9105\n","Epoch 72/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.5146 - val_accuracy: 0.9050\n","\n","Epoch 00072: val_accuracy did not improve from 0.91050\n","Epoch 73/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0189 - accuracy: 0.9933 - val_loss: 0.5007 - val_accuracy: 0.9087\n","\n","Epoch 00073: val_accuracy did not improve from 0.91050\n","Epoch 74/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.5214 - val_accuracy: 0.9061\n","\n","Epoch 00074: val_accuracy did not improve from 0.91050\n","Epoch 75/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.5620 - val_accuracy: 0.9035\n","\n","Epoch 00075: val_accuracy did not improve from 0.91050\n","Epoch 76/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0170 - accuracy: 0.9940 - val_loss: 0.5379 - val_accuracy: 0.9034\n","\n","Epoch 00076: val_accuracy did not improve from 0.91050\n","Epoch 77/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.4545 - val_accuracy: 0.9136\n","\n","Epoch 00077: val_accuracy improved from 0.91050 to 0.91360, saving model to models/Epoch-77-0.9136\n","Epoch 78/200\n","391/391 [==============================] - 48s 122ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.5242 - val_accuracy: 0.9062\n","\n","Epoch 00078: val_accuracy did not improve from 0.91360\n","Epoch 79/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0180 - accuracy: 0.9942 - val_loss: 0.5765 - val_accuracy: 0.9037\n","\n","Epoch 00079: val_accuracy did not improve from 0.91360\n","Epoch 80/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.4673 - val_accuracy: 0.9112\n","\n","Epoch 00080: val_accuracy did not improve from 0.91360\n","Epoch 81/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.5219 - val_accuracy: 0.9055\n","\n","Epoch 00081: val_accuracy did not improve from 0.91360\n","Epoch 82/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.5181 - val_accuracy: 0.9076\n","\n","Epoch 00082: val_accuracy did not improve from 0.91360\n","Epoch 83/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.4998 - val_accuracy: 0.9092\n","\n","Epoch 00083: val_accuracy did not improve from 0.91360\n","Epoch 84/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.5104 - val_accuracy: 0.9115\n","\n","Epoch 00084: val_accuracy did not improve from 0.91360\n","Epoch 85/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.5130 - val_accuracy: 0.9047\n","\n","Epoch 00085: val_accuracy did not improve from 0.91360\n","Epoch 86/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.5670 - val_accuracy: 0.9007\n","\n","Epoch 00086: val_accuracy did not improve from 0.91360\n","Epoch 87/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.4699 - val_accuracy: 0.9130\n","\n","Epoch 00087: val_accuracy did not improve from 0.91360\n","\n","Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.03600000143051147.\n","Epoch 88/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.5206 - val_accuracy: 0.9115\n","\n","Epoch 00088: val_accuracy did not improve from 0.91360\n","Epoch 89/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.5257 - val_accuracy: 0.9123\n","\n","Epoch 00089: val_accuracy did not improve from 0.91360\n","Epoch 90/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.5805 - val_accuracy: 0.9102\n","\n","Epoch 00090: val_accuracy did not improve from 0.91360\n","Epoch 91/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.5184 - val_accuracy: 0.9157\n","\n","Epoch 00091: val_accuracy improved from 0.91360 to 0.91570, saving model to models/Epoch-91-0.9157\n","Epoch 92/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.5301 - val_accuracy: 0.9117\n","\n","Epoch 00092: val_accuracy did not improve from 0.91570\n","Epoch 93/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.5330 - val_accuracy: 0.9131\n","\n","Epoch 00093: val_accuracy did not improve from 0.91570\n","Epoch 94/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.5792 - val_accuracy: 0.9092\n","\n","Epoch 00094: val_accuracy did not improve from 0.91570\n","Epoch 95/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.5166 - val_accuracy: 0.9138\n","\n","Epoch 00095: val_accuracy did not improve from 0.91570\n","Epoch 96/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.5286 - val_accuracy: 0.9127\n","\n","Epoch 00096: val_accuracy did not improve from 0.91570\n","Epoch 97/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.5565 - val_accuracy: 0.9122\n","\n","Epoch 00097: val_accuracy did not improve from 0.91570\n","Epoch 98/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0069 - accuracy: 0.9974 - val_loss: 0.5377 - val_accuracy: 0.9147\n","\n","Epoch 00098: val_accuracy did not improve from 0.91570\n","Epoch 99/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.5165 - val_accuracy: 0.9168\n","\n","Epoch 00099: val_accuracy improved from 0.91570 to 0.91680, saving model to models/Epoch-99-0.9168\n","Epoch 100/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.5698 - val_accuracy: 0.9101\n","\n","Epoch 00100: val_accuracy did not improve from 0.91680\n","Epoch 101/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.5517 - val_accuracy: 0.9133\n","\n","Epoch 00101: val_accuracy did not improve from 0.91680\n","Epoch 102/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.5930 - val_accuracy: 0.9118\n","\n","Epoch 00102: val_accuracy did not improve from 0.91680\n","Epoch 103/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.6059 - val_accuracy: 0.9092\n","\n","Epoch 00103: val_accuracy did not improve from 0.91680\n","Epoch 104/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.5819 - val_accuracy: 0.9099\n","\n","Epoch 00104: val_accuracy did not improve from 0.91680\n","Epoch 105/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 0.5520 - val_accuracy: 0.9113\n","\n","Epoch 00105: val_accuracy did not improve from 0.91680\n","Epoch 106/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.6261 - val_accuracy: 0.9105\n","\n","Epoch 00106: val_accuracy did not improve from 0.91680\n","Epoch 107/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.5783 - val_accuracy: 0.9129\n","\n","Epoch 00107: val_accuracy did not improve from 0.91680\n","Epoch 108/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.5918 - val_accuracy: 0.9114\n","\n","Epoch 00108: val_accuracy did not improve from 0.91680\n","Epoch 109/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.5505 - val_accuracy: 0.9151\n","\n","Epoch 00109: val_accuracy did not improve from 0.91680\n","\n","Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.02160000130534172.\n","Epoch 110/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.5654 - val_accuracy: 0.9130\n","\n","Epoch 00110: val_accuracy did not improve from 0.91680\n","Epoch 111/200\n","391/391 [==============================] - 48s 123ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.5820 - val_accuracy: 0.9114\n","\n","Epoch 00111: val_accuracy did not improve from 0.91680\n","Epoch 112/200\n"," 56/391 [===>..........................] - ETA: 38s - loss: 0.0024 - accuracy: 0.9992"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fI_5INQR-BPG"},"source":["## 3. Tensorflow 중간규모 Model\r\n","- https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\r\n","- 여기 keras코드를 tensorflow코드로 재구성\r\n","- ImageDataGenerator로 data augmentation\r\n","\r\n","- lr=0.001로 시작해서,213 epoch 이후, val acc가 더 이상 좋아지지 않아(88.59%), lr=0.0001로 train 이어가기\r\n","- 288 epoch에 90.53%"]},{"cell_type":"code","metadata":{"id":"7N15JZWj-Le8","executionInfo":{"status":"ok","timestamp":1612571736836,"user_tz":-540,"elapsed":605,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import tensorflow as tf\r\n","from tensorflow.keras.applications import resnet\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","import os"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_c3n38x-fIt"},"source":["class MyModel(tf.keras.Model):\r\n","    def __init__(self,input_shape):\r\n","        super(MyModel, self).__init__()\r\n","\r\n","        weight_decay = 1e-4\r\n","\r\n","        self.model = tf.keras.models.Sequential()\r\n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=input_shape))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.2))      \r\n","\r\n","\r\n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.3))    \r\n","\r\n","\r\n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.4))   \r\n","\r\n","        self.model.add(tf.keras.layers.Flatten())\r\n","\r\n","\r\n","        self.built = True  # summary가 제대로 작동한다.\r\n","\r\n","    def call(self,x,training=None):  # training의 default 값으로 None이 좋다(Ture/False보다)\r\n","        output = self.model(x)\r\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJv42Qmu-p-E","executionInfo":{"status":"ok","timestamp":1612571740880,"user_tz":-540,"elapsed":1304,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"6949e0b6-1f95-4b2b-b95c-df50b443c83e"},"source":["batch_size = 128\r\n","\r\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","\r\n","train_datagen = ImageDataGenerator(rescale=1./255,\r\n","    rotation_range=15,\r\n","    width_shift_range=0.1,\r\n","    height_shift_range=0.1,\r\n","    horizontal_flip=True,\r\n","    )\r\n","train_dataset = train_datagen.flow(x_train, y_train.reshape(-1), batch_size=batch_size,shuffle=True)\r\n","\r\n","valid_datagen = ImageDataGenerator(rescale=1./255)\r\n","valid_dataset = valid_datagen.flow(x_test, y_test.reshape(-1), batch_size=batch_size)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dsOcVfpZNdVD","executionInfo":{"status":"ok","timestamp":1612571763248,"user_tz":-540,"elapsed":586,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["# base_model = MyModel(input_shape=(32,32,3))\r\n","# model = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])\r\n","\r\n","base_model = ResNetX18()\r\n","model = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLaF-6uz-smN","executionInfo":{"status":"ok","timestamp":1612571765306,"user_tz":-540,"elapsed":589,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["initial_learning_rate = 0.1\r\n","lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\r\n","                            monitor='val_accuracy',  # val loss를 기준으로...\r\n","                            factor=0.6,          # callback 호출시 lr = factor*lr\r\n","                            patience=10,         # patience epoch동안 monitor값이 좋아지지 않으면 callback 작동\r\n","                            cooldown=0,         # lr이 변경된 후, cooldonw동안에는 callback이 작동 안 한다.\r\n","                            min_lr=0.0001, \r\n","                            verbose = 1,min_delta=0.001)\r\n","es_callback = tf.keras.callbacks.EarlyStopping(\r\n","                            monitor=\"val_accuracy\",\r\n","                            min_delta=0.003,  # 이 값 이상으로 변화되어야 향상으로 인정.\r\n","                            patience=40,\r\n","                            verbose=1,\r\n","                            mode=\"auto\",\r\n","                            baseline=None,\r\n","                            restore_best_weights=True,  # stop될때 마지막이 아닌, best weight로 복원\r\n","                        )\r\n","\r\n","\r\n","\r\n","\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, clipnorm=0.5)  # lr or learning_rate\r\n","\r\n","model.compile(optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\r\n","model_filename = \"Epoch-{epoch:02d}-{val_accuracy:.4f}\"\r\n","checkpoint_path = os.path.join('models/', model_filename)\r\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,mode='auto',save_best_only=True,monitor='val_accuracy')"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-X4GpPo-2fm","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"error","timestamp":1612571823769,"user_tz":-540,"elapsed":56233,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"930e504c-b41d-452c-b43b-add5a65f69d2"},"source":["history = model.fit(train_dataset,epochs=400,steps_per_epoch=len(x_train)/batch_size,validation_data=valid_dataset,validation_freq=1, verbose=1,callbacks=[cp_callback,lr_callback],initial_epoch=0)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch 1/400\n","390/390 [==============================] - 48s 114ms/step - loss: 3.7592 - accuracy: 0.1604 - val_loss: 2.4506 - val_accuracy: 0.1491\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.14910, saving model to models/Epoch-01-0.1491\n","Epoch 2/400\n"," 65/390 [===>..........................] - ETA: 35s - loss: 1.8685 - accuracy: 0.2622"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-c548ee05c826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"tS0lKXTEFUtI"},"source":["## 4. pytorch로된 kuangliu의 코드를 tesorflow 코드로 동일하게 변환"]},{"cell_type":"code","metadata":{"id":"xID6cvIjGCRy"},"source":["import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import tensorflow.keras.backend as K\r\n","from functools import partial\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAcsIuYaFVOG","executionInfo":{"status":"ok","timestamp":1612571133778,"user_tz":-540,"elapsed":695,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["class BasicBlockX(tf.keras.Model): # 논문의 resnet구조로 다르다.\r\n","    expansion = 1\r\n","\r\n","    def __init__(self, in_planes, planes, stride=1):\r\n","        super(BasicBlockX, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(planes,kernel_size=3, strides=stride, padding=\"SAME\", use_bias=False)\r\n","        \r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.conv2 = tf.keras.layers.Conv2D(planes,kernel_size=3, strides=1, padding=\"SAME\", use_bias=False)\r\n","        self.bn2 = tf.keras.layers.BatchNormalization()\r\n","\r\n","        self.shortcut = tf.keras.models.Sequential()\r\n","        if stride != 1 or in_planes != self.expansion*planes:\r\n","            self.shortcut.add(tf.keras.layers.Conv2D(self.expansion*planes,kernel_size=1, strides=stride, padding=\"SAME\", use_bias=False))\r\n","            self.shortcut.add(tf.keras.layers.BatchNormalization())\r\n","\r\n","    def call(self, x, training=None):\r\n","        out = tf.nn.relu(self.bn1(self.conv1(x),training))\r\n","        out = self.bn2(self.conv2(out),training)\r\n","        out += self.shortcut(x,training)\r\n","        out = tf.nn.relu(out)\r\n","        return out\r\n","\r\n","\r\n","class ResNetX(tf.keras.Model):\r\n","    def __init__(self, block, num_blocks, num_classes=10):\r\n","        super(ResNetX, self).__init__()\r\n","        self.in_planes = 64\r\n","\r\n","        self.conv1 = tf.keras.layers.Conv2D(64,kernel_size=3, strides=1, padding=\"SAME\", use_bias=False) \r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        \r\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\r\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\r\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\r\n","        \r\n","        self.flatten = tf.keras.layers.Flatten()\r\n","        self.linear = tf.keras.layers.Dense(units=num_classes)\r\n","        \r\n","        self.built = True\r\n","\r\n","    def _make_layer(self, block, planes, num_blocks, stride):\r\n","        strides = [stride] + [1]*(num_blocks-1)\r\n","        layers = []\r\n","        for stride in strides:\r\n","            layers.append(block(self.in_planes, planes, stride))\r\n","            self.in_planes = planes * block.expansion\r\n","        return tf.keras.models.Sequential(layers)\r\n","    def build(self,input_shape):\r\n","        #super(ResNetX, self).build(input_shape)\r\n","        super().build(input_shape)\r\n","    \r\n","    def call(self, x,training=None):\r\n","        out = tf.nn.relu(self.bn1(self.conv1(x),training))\r\n","        out = self.layer1(out,training)\r\n","        out = self.layer2(out,training)\r\n","        out = self.layer3(out,training)\r\n","        out = self.layer4(out,training)\r\n","        out = tf.nn.avg_pool2d(out, ksize=4,strides=4,padding='VALID')\r\n","        out = self.flatten(out)\r\n","        out = self.linear(out)\r\n","        return out\r\n","\r\n","\r\n","def ResNetX18():\r\n","    return ResNetX(BasicBlockX, [2, 2, 2, 2])\r\n","def ResNet34():\r\n","    return ResNetX(BasicBlockX, [3, 4, 6, 3])\r\n","def ResNet50():\r\n","    return ResNetX(BottleneckX, [3, 4, 6, 3])\r\n","def ResNet101():\r\n","    return ResNetX(BottleneckX, [3, 4, 23, 3])\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxoYWVR_FVSs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlGO3zumFVWv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsW-8m6OFVaF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OX4B1MhFVeL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBlPha-tP5W_"},"source":["## TEST code"]},{"cell_type":"code","metadata":{"id":"t0RjFfG0EJey"},"source":["#! rm models/Epoch-4*\r\n","! rm -r models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CR8iiu0nRIFe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612444866115,"user_tz":-540,"elapsed":1059,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"686c1df0-0a30-4af0-8537-316fbcd60445"},"source":["my_resnet"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8aITfETNBHz","executionInfo":{"status":"ok","timestamp":1612445847613,"user_tz":-540,"elapsed":1243,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"c7791973-d137-4091-d4e1-5bfaab41ed84"},"source":["net = resnet_18()\r\n","net.build(input_shape=(None,32,32,3))\r\n","net.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"res_net_type_i_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_100 (Conv2D)          multiple                  9408      \n","_________________________________________________________________\n","batch_normalization_100 (Bat multiple                  256       \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","sequential_35 (Sequential)   (None, 8, 8, 64)          148480    \n","_________________________________________________________________\n","sequential_36 (Sequential)   (None, 4, 4, 128)         526848    \n","_________________________________________________________________\n","sequential_38 (Sequential)   (None, 2, 2, 256)         2102272   \n","_________________________________________________________________\n","sequential_40 (Sequential)   (None, 1, 1, 512)         8398848   \n","_________________________________________________________________\n","global_average_pooling2d_5 ( multiple                  0         \n","_________________________________________________________________\n","dense_5 (Dense)              multiple                  5130      \n","=================================================================\n","Total params: 11,191,242\n","Trainable params: 11,181,642\n","Non-trainable params: 9,600\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-U05nmFDNziy","executionInfo":{"status":"ok","timestamp":1612445253449,"user_tz":-540,"elapsed":630,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"c6e3925f-ce8f-4fbc-f527-be118c4b9ed1"},"source":["from torchsummary import summary\r\n","\r\n","summary(my_resnet, input_size=(3, 32, 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           9,408\n","       BatchNorm2d-2           [-1, 64, 16, 16]             128\n","              ReLU-3           [-1, 64, 16, 16]               0\n","         MaxPool2d-4             [-1, 64, 8, 8]               0\n","            Conv2d-5             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-6             [-1, 64, 8, 8]             128\n","              ReLU-7             [-1, 64, 8, 8]               0\n","            Conv2d-8             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-9             [-1, 64, 8, 8]             128\n","             ReLU-10             [-1, 64, 8, 8]               0\n","       BasicBlock-11             [-1, 64, 8, 8]               0\n","           Conv2d-12             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-13             [-1, 64, 8, 8]             128\n","             ReLU-14             [-1, 64, 8, 8]               0\n","           Conv2d-15             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-16             [-1, 64, 8, 8]             128\n","             ReLU-17             [-1, 64, 8, 8]               0\n","       BasicBlock-18             [-1, 64, 8, 8]               0\n","           Conv2d-19            [-1, 128, 4, 4]          73,728\n","      BatchNorm2d-20            [-1, 128, 4, 4]             256\n","             ReLU-21            [-1, 128, 4, 4]               0\n","           Conv2d-22            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-23            [-1, 128, 4, 4]             256\n","           Conv2d-24            [-1, 128, 4, 4]           8,192\n","      BatchNorm2d-25            [-1, 128, 4, 4]             256\n","             ReLU-26            [-1, 128, 4, 4]               0\n","       BasicBlock-27            [-1, 128, 4, 4]               0\n","           Conv2d-28            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-29            [-1, 128, 4, 4]             256\n","             ReLU-30            [-1, 128, 4, 4]               0\n","           Conv2d-31            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-32            [-1, 128, 4, 4]             256\n","             ReLU-33            [-1, 128, 4, 4]               0\n","       BasicBlock-34            [-1, 128, 4, 4]               0\n","           Conv2d-35            [-1, 256, 2, 2]         294,912\n","      BatchNorm2d-36            [-1, 256, 2, 2]             512\n","             ReLU-37            [-1, 256, 2, 2]               0\n","           Conv2d-38            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-39            [-1, 256, 2, 2]             512\n","           Conv2d-40            [-1, 256, 2, 2]          32,768\n","      BatchNorm2d-41            [-1, 256, 2, 2]             512\n","             ReLU-42            [-1, 256, 2, 2]               0\n","       BasicBlock-43            [-1, 256, 2, 2]               0\n","           Conv2d-44            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-45            [-1, 256, 2, 2]             512\n","             ReLU-46            [-1, 256, 2, 2]               0\n","           Conv2d-47            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-48            [-1, 256, 2, 2]             512\n","             ReLU-49            [-1, 256, 2, 2]               0\n","       BasicBlock-50            [-1, 256, 2, 2]               0\n","           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n","             ReLU-53            [-1, 512, 1, 1]               0\n","           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n","           Conv2d-56            [-1, 512, 1, 1]         131,072\n","      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n","             ReLU-58            [-1, 512, 1, 1]               0\n","       BasicBlock-59            [-1, 512, 1, 1]               0\n","           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n","             ReLU-62            [-1, 512, 1, 1]               0\n","           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n","             ReLU-65            [-1, 512, 1, 1]               0\n","       BasicBlock-66            [-1, 512, 1, 1]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                   [-1, 10]           5,130\n","================================================================\n","Total params: 11,181,642\n","Trainable params: 11,181,642\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.29\n","Params size (MB): 42.65\n","Estimated Total Size (MB): 43.95\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"id":"qNVvHDnjOM7r","executionInfo":{"status":"error","timestamp":1612447897276,"user_tz":-540,"elapsed":643,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"1b8a4a22-703e-4894-c431-1b93ca76b066"},"source":["model2 = ResNet18()\r\n","model2.build(input_shape=(None,32,32,3))\r\n","model2.summary()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-2e90d877a9d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    406\u001b[0m               \u001b[0;31m# Has invalid call signature with unknown positional arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m               raise ValueError(\n\u001b[0;32m--> 408\u001b[0;31m                   \u001b[0;34m'Currently, you cannot build your model if it has '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                   \u001b[0;34m'positional or keyword arguments that are not '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                   \u001b[0;34m'inputs to the model, but are required for its '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Currently, you cannot build your model if it has positional or keyword arguments that are not inputs to the model, but are required for its `call` method. Instead, in order to instantiate and build your model, `call` your model on real tensor data with all expected call arguments."]}]},{"cell_type":"code","metadata":{"id":"AGGeIlb3YdLY"},"source":[""],"execution_count":null,"outputs":[]}]}