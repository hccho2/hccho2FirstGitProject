{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10_pytorch_tf.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9tpKHG5qBc7Fdhzrbwlve"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3e0e7fec5a3a472f802bbeba08c2e98f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d586504f9d254401997a9f63704d9ef3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4b385576794e4506afc7c88341dda861","IPY_MODEL_787c9bddfd984371ad2500fcef5b2fb3"]}},"d586504f9d254401997a9f63704d9ef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b385576794e4506afc7c88341dda861":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ec1841e462ae4b1ba198943e8a7cfb1e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_40e7834c1e964b258569fa09c5c3d0d2"}},"787c9bddfd984371ad2500fcef5b2fb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b1889782edf4471aca10e7e27211262","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 52135015.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d53eed2d237746009d2d5bbfdb267728"}},"ec1841e462ae4b1ba198943e8a7cfb1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"40e7834c1e964b258569fa09c5c3d0d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b1889782edf4471aca10e7e27211262":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d53eed2d237746009d2d5bbfdb267728":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"053bee6b02b14149b387a74a6922c052":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8ea2e7dada2340be9b0fe80a554f1f3e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_08ba02bdb818440fb610500328331333","IPY_MODEL_287f049beeec4e4fa02155b775ef206c"]}},"8ea2e7dada2340be9b0fe80a554f1f3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08ba02bdb818440fb610500328331333":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_338279ffed9742dab8bdecf48b83d38a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1a4bd5b848a48e09242c52c3e7fa487"}},"287f049beeec4e4fa02155b775ef206c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42991d652e754c8d993d13fa25c6f502","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [17:43&lt;00:00, 44.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5aa5f2e0d43a4729a043863f672fa7ac"}},"338279ffed9742dab8bdecf48b83d38a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f1a4bd5b848a48e09242c52c3e7fa487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42991d652e754c8d993d13fa25c6f502":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5aa5f2e0d43a4729a043863f672fa7ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"YfVJyLm6xPsd"},"source":["# local pc --> colab 파일 업로드\r\n","#  여러개 동시 선택 가능 해야 됨\r\n","from google.colab import files\r\n","\r\n","uploaded = files.upload()\r\n","for fn in uploaded.keys():\r\n","    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTXF5lSeyCW9"},"source":["# cifar 10으로 validation accuracy 높이기\r\n","## 1. pytorch\r\n","-  https://github.com/kuangliu/pytorch-cifar ==> pytorch 코드가 있는데 모델별로 92~95%\r\n","\r\n","- pytorch tutorial의 간단한 모델로 하면, val acc = 65%정도 나온다. https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\r\n","\r\n","- transfer learning: pretrained weight를 고정하면 train이 잘 안된다. 고정하지 않아야 한다.\r\n","- vgg16, resnet18로 data augmentation 적용해서 train하면 val acc=83~85%정도 나온다. vgg16은 lr=0.00005로 해야 한다.\r\n","    * https://github.com/kuangliu/pytorch-cifar --> 성능이 제일 낫다. 86%\r\n","    * pytorch VGG16(lr=0.00005): 100 epoch train acc = 98%, test acc = 83% 정도 나온다. ==> vgg는 모델이 커서 느리다.\r\n","    * pytorch resnet18(lr=0.001): 100 epoch train acc = 97%, test acc = 84% \r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"WO2pp-UByAFB","executionInfo":{"status":"ok","timestamp":1612355252911,"user_tz":-540,"elapsed":4309,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import torch\r\n","import torchvision\r\n","import torchvision.transforms as transforms\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import torchvision.models as models\r\n","\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import time\r\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["3e0e7fec5a3a472f802bbeba08c2e98f","d586504f9d254401997a9f63704d9ef3","4b385576794e4506afc7c88341dda861","787c9bddfd984371ad2500fcef5b2fb3","ec1841e462ae4b1ba198943e8a7cfb1e","40e7834c1e964b258569fa09c5c3d0d2","0b1889782edf4471aca10e7e27211262","d53eed2d237746009d2d5bbfdb267728"]},"id":"WO5-y3FZyAuS","executionInfo":{"status":"ok","timestamp":1612355484955,"user_tz":-540,"elapsed":7688,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"2042dcd7-7205-4f60-94f2-b1f0b890d9d6"},"source":["#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\r\n","\r\n","transform_train = transforms.Compose([\r\n","    transforms.RandomCrop(32, padding=4),  # padding 후, 지정한 크기로 잘라낸다. https://chloes-dl.com/2019/11/13/pytorch101-data-preprocessing-and-augmentation-part-1/\r\n","    transforms.RandomHorizontalFlip(),\r\n","    transforms.ToTensor(),\r\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n","])\r\n","\r\n","transform_test = transforms.Compose([\r\n","    transforms.ToTensor(),\r\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n","])\r\n","\r\n","\r\n","\r\n","batch_size = 128\r\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)  # torchvision.datasets.cifar.CIFAR10\r\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\r\n","\r\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\r\n","\r\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n","print(f'train data: {len(trainset)}, test data: {len(testset)}')\r\n","print(f'# of mini-batch: {len(trainset)//batch_size}')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e0e7fec5a3a472f802bbeba08c2e98f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","train data: 50000, test data: 10000\n","# of mini-batch: 390\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["053bee6b02b14149b387a74a6922c052","8ea2e7dada2340be9b0fe80a554f1f3e","08ba02bdb818440fb610500328331333","287f049beeec4e4fa02155b775ef206c","338279ffed9742dab8bdecf48b83d38a","f1a4bd5b848a48e09242c52c3e7fa487","42991d652e754c8d993d13fa25c6f502","5aa5f2e0d43a4729a043863f672fa7ac"]},"id":"BJL0CmjF23c5","executionInfo":{"status":"ok","timestamp":1612355592330,"user_tz":-540,"elapsed":1152,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"05554fba-46c7-4a6a-a5f7-8a58511eb069"},"source":["cfg = {\r\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\r\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\r\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\r\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\r\n","}\r\n","\r\n","\r\n","class x_VGG(nn.Module):  # beginner-blitz-cifar10-tutorial-py에 있는 VGG\r\n","    def __init__(self, vgg_name):\r\n","        super(x_VGG, self).__init__()\r\n","        self.features = self._make_layers(cfg[vgg_name])\r\n","        self.classifier = nn.Linear(512, 10)\r\n","\r\n","    def forward(self, x):\r\n","        out = self.features(x)\r\n","        out = out.view(out.size(0), -1)\r\n","        out = self.classifier(out)\r\n","        return out\r\n","\r\n","    def _make_layers(self, cfg):\r\n","        layers = []\r\n","        in_channels = 3\r\n","        for x in cfg:\r\n","            if x == 'M':\r\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\r\n","            else:\r\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\r\n","                           nn.BatchNorm2d(x),\r\n","                           nn.ReLU(inplace=True)]\r\n","                in_channels = x\r\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\r\n","        return nn.Sequential(*layers)\r\n","\r\n","class MyVGG(nn.Module):\r\n","    def __init__(self):\r\n","        super(MyVGG, self).__init__()\r\n","        mode = 2\r\n","        self.flatten = nn.Flatten()\r\n","        if mode ==1:\r\n","            self.vgg = models.vgg16_bn(pretrained=True,progress=True)\r\n","            self.fc = nn.Linear(1000,10)\r\n","        else:\r\n","            self.vgg = models.vgg16_bn(pretrained=True,progress=True).features\r\n","            self.fc = nn.Linear(512,10)\r\n","    def forward(self, x):\r\n","        x = self.vgg(x)\r\n","        x = self.flatten(x)\r\n","        x = F.relu(self.fc(x))\r\n","        return x\r\n","\r\n","my_resnet = models.resnet18(pretrained=True)\r\n","my_resnet.fc = nn.Linear(my_resnet.fc.in_features,10)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"053bee6b02b14149b387a74a6922c052","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jNki9l623ha","executionInfo":{"status":"ok","timestamp":1612355606442,"user_tz":-540,"elapsed":584,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n","\r\n","#net = x_VGG('VGG16')\r\n","#net = MyVGG()  # lr=0.00005 또는 Adam말고 SGD로 해야됨.\r\n","net = my_resnet\r\n","net.to(device)\r\n","criterion = nn.CrossEntropyLoss()\r\n","\r\n","#optimizer = optim.Adam(net.parameters(), lr=0.0001,betas=(0.5, 0.9))\r\n","#optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # for x_VGG\r\n","optimizer = optim.SGD(net.parameters(), lr=0.001)\r\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4NtIEXC23oM"},"source":["s_time = time.time()\r\n","for epoch in range(200):  # loop over the dataset multiple times\r\n","\r\n","    running_loss = []\r\n","    acc = 0\r\n","    total = 0\r\n","    net.train()\r\n","    for i, data in enumerate(trainloader):\r\n","        # get the inputs\r\n","        inputs, labels = data\r\n","        inputs = inputs.to(device)\r\n","        labels = labels.to(device)\r\n","        # zero the parameter gradients\r\n","        optimizer.zero_grad()\r\n","\r\n","        # forward + backward + optimize\r\n","        outputs = net(inputs)\r\n","        loss = criterion(outputs, labels)\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        \r\n","        _, pred = outputs.max(axis=-1)\r\n","        acc  += (pred==labels).float().sum().item()\r\n","        total += len(labels)\r\n","\r\n","\r\n","        # print statistics\r\n","        running_loss.append(loss.item())\r\n","    print('[epoch: %d] loss: %.3f, train acc: %.3f elapsed: %.2f' % (epoch + 1, np.mean(running_loss), acc/total , time.time()-s_time))\r\n","    scheduler.step()\r\n","    acc = 0\r\n","    total=0\r\n","    net.eval()\r\n","    with torch.no_grad():\r\n","        for i, data in enumerate(testloader):\r\n","            inputs, labels = data\r\n","            inputs = inputs.to(device)\r\n","            outputs = net(inputs)\r\n","            \r\n","            _, pred = outputs.max(axis=-1)\r\n","            acc  += (pred.cpu()==labels).float().sum().item()\r\n","            total += len(labels)\r\n","    print('test acc: %.3f ' % (acc/total ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RWsdOpb4h3_"},"source":["# test accuracy\r\n","correct = 0\r\n","total = 0\r\n","labels_all = []\r\n","predicted_all = []\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        images, labels = data\r\n","        outputs = net(images.to(device))\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","        total += labels.size(0)\r\n","        correct += (predicted.cpu() == labels).sum().item()\r\n","\r\n","        labels_all.extend(labels.numpy())\r\n","        predicted_all.extend(predicted.cpu().numpy())\r\n","\r\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9rel3E14wU5"},"source":["from sklearn.metrics import confusion_matrix\r\n","import seaborn as sns\r\n","confusion_matrix = confusion_matrix(labels_all, predicted_all)  # numpy array\r\n","sns.heatmap(confusion_matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g',cmap=\"YlGnBu\")  # cmap=\"Blues\"\r\n","plt.xlabel('Prediction')\r\n","plt.ylabel('Label')\r\n","plt.tight_layout()\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wfh5GO-Z40BO"},"source":["### torchsummay로 모델 구조 살펴보기\r\n","- pip install torchsummary"]},{"cell_type":"code","metadata":{"id":"5PHpFk8F44Ct"},"source":["from torchsummary import summary\r\n","#my_resnet.to('cpu')\r\n","\r\n","net = MyVGG()\r\n","summary(net, input_size=(3, 32, 32),device='cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5XS6Ukb47Sy"},"source":["## 2. Tensorflow\r\n","- tensorflow는 resnet18을 제공하지 않는다. resnet50이 resnet중에는 제일 작은 모델\r\n","- resnet.preprocess_input은 별 효과 없다. 자체적인 augmentation이 낫다.\r\n","- resnet50이 규모가 있기 때문에, classifier의 fc에 dropout을 적용하는 것이 낫다.\r\n","- clipnorm은 별 효과 없다.\r\n","\r\n","    - resnet50: 100 epoch, train acc = 99%, val acc = 79%~80%\r\n","    - resnet18: 200 epoch, train acc = 95%, val acc = 80%\r\n","\r\n","- pytorch만큼 성능이 안 나옴. \r\n","- https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/ 여기 있는 좀 더 간단한 구조로 하면 90%까지 나옴"]},{"cell_type":"code","metadata":{"id":"oK-Hyv4280oP","executionInfo":{"status":"ok","timestamp":1612356740126,"user_tz":-540,"elapsed":572,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import tensorflow as tf\r\n","from tensorflow.keras.applications import resnet\r\n","import matplotlib.pyplot as plt"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEXreONY480s","executionInfo":{"status":"ok","timestamp":1612356742497,"user_tz":-540,"elapsed":829,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["# https://github.com/calmisential/TensorFlow2.0_ResNet/tree/master/models\r\n","\r\n","NUM_CLASSES = 10\r\n","\r\n","class BasicBlock(tf.keras.layers.Layer):\r\n","\r\n","    def __init__(self, filter_num, stride=1):\r\n","        super(BasicBlock, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=stride,\r\n","                                            padding=\"same\")\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=1,\r\n","                                            padding=\"same\")\r\n","        self.bn2 = tf.keras.layers.BatchNormalization()\r\n","        if stride != 1:\r\n","            self.downsample = tf.keras.Sequential()\r\n","            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                                       kernel_size=(1, 1),\r\n","                                                       strides=stride))\r\n","            self.downsample.add(tf.keras.layers.BatchNormalization())\r\n","        else:\r\n","            self.downsample = lambda x: x\r\n","\r\n","    def call(self, inputs, training=None, **kwargs):\r\n","        residual = self.downsample(inputs)\r\n","\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.bn2(x, training=training)\r\n","\r\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\r\n","\r\n","        return output\r\n","\r\n","\r\n","class BottleNeck(tf.keras.layers.Layer):\r\n","    def __init__(self, filter_num, stride=1):\r\n","        super(BottleNeck, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(1, 1),\r\n","                                            strides=1,\r\n","                                            padding='same')\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\r\n","                                            kernel_size=(3, 3),\r\n","                                            strides=stride,\r\n","                                            padding='same')\r\n","        self.bn2 = tf.keras.layers.BatchNormalization()\r\n","        self.conv3 = tf.keras.layers.Conv2D(filters=filter_num * 4,\r\n","                                            kernel_size=(1, 1),\r\n","                                            strides=1,\r\n","                                            padding='same')\r\n","        self.bn3 = tf.keras.layers.BatchNormalization()\r\n","\r\n","        self.downsample = tf.keras.Sequential()\r\n","        self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num * 4,\r\n","                                                   kernel_size=(1, 1),\r\n","                                                   strides=stride))\r\n","        self.downsample.add(tf.keras.layers.BatchNormalization())\r\n","\r\n","    def call(self, inputs, training=None, **kwargs):\r\n","        residual = self.downsample(inputs)\r\n","\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.bn2(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = self.bn3(x, training=training)\r\n","\r\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\r\n","\r\n","        return output\r\n","\r\n","\r\n","def make_basic_block_layer(filter_num, blocks, stride=1):\r\n","    res_block = tf.keras.Sequential()\r\n","    res_block.add(BasicBlock(filter_num, stride=stride))\r\n","\r\n","    for _ in range(1, blocks):\r\n","        res_block.add(BasicBlock(filter_num, stride=1))\r\n","\r\n","    return res_block\r\n","\r\n","\r\n","def make_bottleneck_layer(filter_num, blocks, stride=1):\r\n","    res_block = tf.keras.Sequential()\r\n","    res_block.add(BottleNeck(filter_num, stride=stride))\r\n","\r\n","    for _ in range(1, blocks):\r\n","        res_block.add(BottleNeck(filter_num, stride=1))\r\n","\r\n","    return res_block\r\n","class ResNetTypeI(tf.keras.Model):\r\n","    def __init__(self, layer_params):\r\n","        super(ResNetTypeI, self).__init__()\r\n","\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\r\n","                                            kernel_size=(7, 7),\r\n","                                            strides=2,\r\n","                                            padding=\"same\")\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\r\n","                                               strides=2,\r\n","                                               padding=\"same\")\r\n","\r\n","        self.layer1 = make_basic_block_layer(filter_num=64,\r\n","                                             blocks=layer_params[0])\r\n","        self.layer2 = make_basic_block_layer(filter_num=128,\r\n","                                             blocks=layer_params[1],\r\n","                                             stride=2)\r\n","        self.layer3 = make_basic_block_layer(filter_num=256,\r\n","                                             blocks=layer_params[2],\r\n","                                             stride=2)\r\n","        self.layer4 = make_basic_block_layer(filter_num=512,\r\n","                                             blocks=layer_params[3],\r\n","                                             stride=2)\r\n","\r\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\r\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=tf.keras.activations.softmax)\r\n","\r\n","    def call(self, inputs, training=None, mask=None):\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.pool1(x)\r\n","        x = self.layer1(x, training=training)\r\n","        x = self.layer2(x, training=training)\r\n","        x = self.layer3(x, training=training)\r\n","        x = self.layer4(x, training=training)\r\n","        x = self.avgpool(x)\r\n","        output = self.fc(x)\r\n","\r\n","        return output\r\n","\r\n","\r\n","class ResNetTypeII(tf.keras.Model):\r\n","    def __init__(self, layer_params):\r\n","        super(ResNetTypeII, self).__init__()\r\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\r\n","                                            kernel_size=(7, 7),\r\n","                                            strides=2,\r\n","                                            padding=\"same\")\r\n","        self.bn1 = tf.keras.layers.BatchNormalization()\r\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\r\n","                                               strides=2,\r\n","                                               padding=\"same\")\r\n","\r\n","        self.layer1 = make_bottleneck_layer(filter_num=64,\r\n","                                            blocks=layer_params[0])\r\n","        self.layer2 = make_bottleneck_layer(filter_num=128,\r\n","                                            blocks=layer_params[1],\r\n","                                            stride=2)\r\n","        self.layer3 = make_bottleneck_layer(filter_num=256,\r\n","                                            blocks=layer_params[2],\r\n","                                            stride=2)\r\n","        self.layer4 = make_bottleneck_layer(filter_num=512,\r\n","                                            blocks=layer_params[3],\r\n","                                            stride=2)\r\n","\r\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\r\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=tf.keras.activations.softmax)\r\n","\r\n","    def call(self, inputs, training=None, mask=None):\r\n","        x = self.conv1(inputs)\r\n","        x = self.bn1(x, training=training)\r\n","        x = tf.nn.relu(x)\r\n","        x = self.pool1(x)\r\n","        x = self.layer1(x, training=training)\r\n","        x = self.layer2(x, training=training)\r\n","        x = self.layer3(x, training=training)\r\n","        x = self.layer4(x, training=training)\r\n","        x = self.avgpool(x)\r\n","        output = self.fc(x)\r\n","\r\n","        return output\r\n","\r\n","\r\n","def resnet_18():\r\n","    return ResNetTypeI(layer_params=[2, 2, 2, 2])\r\n","\r\n","\r\n","def resnet_34():\r\n","    return ResNetTypeI(layer_params=[3, 4, 6, 3])\r\n","\r\n","\r\n","def resnet_50():\r\n","    return ResNetTypeII(layer_params=[3, 4, 6, 3])\r\n","\r\n","\r\n","def resnet_101():\r\n","    return ResNetTypeII(layer_params=[3, 4, 23, 3])\r\n","\r\n","\r\n","def resnet_152():\r\n","    return ResNetTypeII(layer_params=[3, 8, 36, 3])"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXwDIQu70_o","executionInfo":{"status":"ok","timestamp":1612356745041,"user_tz":-540,"elapsed":609,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["def plot_history(history):\r\n","    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\r\n","\r\n","        :param history: Training history of model\r\n","        :return:\r\n","    \"\"\"\r\n","\r\n","    fig, axs = plt.subplots(1,2)\r\n","\r\n","    # create accuracy sublpot\r\n","    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\r\n","    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\r\n","    axs[0].set_ylabel(\"Accuracy\")\r\n","    axs[0].legend(loc=\"lower right\")\r\n","    axs[0].set_title(\"Accuracy eval\")\r\n","\r\n","    # create error sublpot\r\n","    axs[1].plot(history.history[\"loss\"], label=\"train error\")\r\n","    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\r\n","    axs[1].set_ylabel(\"Error\")\r\n","    axs[1].set_xlabel(\"Epoch\")\r\n","    axs[1].legend(loc=\"upper right\")\r\n","    axs[1].set_title(\"Error eval\")\r\n","\r\n","    plt.show()\r\n","def my_preprocessing(image,label):\r\n","\r\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\r\n","\r\n","    image = tf.image.random_flip_left_right(image)  # 확률 50%로 고정되어 있음.\r\n","    image = tf.image.random_brightness(image, max_delta=0.3)\r\n","    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)  # 채도 조절\r\n","    # random crop\r\n","    shape = tf.shape(image)  # batch size 알아내기\r\n","    image = tf.image.resize_with_pad(image, 40,40)\r\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\r\n","\r\n","    image = tf.clip_by_value(image, 0.0, 1.0)\r\n","\r\n","    return image, label\r\n","\r\n","def my_preprocessing2(image,label):\r\n","\r\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\r\n","    # random crop\r\n","    shape = tf.shape(image)  # batch size 알아내기\r\n","    image = tf.image.resize_with_pad(image, 36,36)\r\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\r\n","\r\n","    image = tf.clip_by_value(image, 0.0, 1.0)\r\n","    \r\n","\r\n","    return image, label\r\n","def normalize(image,label):\r\n","    return (image - [0.4914, 0.4822, 0.4465] )/ [0.2023, 0.1994, 0.2010], label"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buOnVIWR71E4","executionInfo":{"status":"ok","timestamp":1612356755629,"user_tz":-540,"elapsed":8308,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"e2b62b26-2785-4714-c95c-bf7d635fb2c2"},"source":["base_model = resnet.ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))  # weights=None, weights='imagenet'\r\n","base_model.trainable = True\r\n","\r\n","#my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dense(10)])\r\n","#my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dropout(0.5),tf.keras.layers.Dense(10)])\r\n","#my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dense(256,activation='relu'),tf.keras.layers.Dense(10)])\r\n","\r\n","my_resnet = resnet_18()\r\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LaXJdNoF8DEz"},"source":["# data loading\r\n","batch_size = 128\r\n","resnet_pre = False\r\n","\r\n","if resnet_pre:\r\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","    train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\r\n","    train_dataset = train_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","\r\n","    train_dataset = train_dataset.shuffle(1000).batch(batch_size)\r\n","\r\n","\r\n","    valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test, tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\r\n","    valid_dataset = valid_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","    valid_dataset = valid_dataset.batch(batch_size)\r\n","\r\n","else:\r\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","    train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\r\n","    #train_dataset = train_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","\r\n","    train_dataset = train_dataset.shuffle(1000).batch(batch_size)\r\n","    train_dataset = train_dataset.map(my_preprocessing)\r\n","    train_dataset = train_dataset.map(normalize)\r\n","\r\n","    valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test/255., tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\r\n","    #valid_dataset = valid_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\r\n","\r\n","    valid_dataset = valid_dataset.batch(batch_size)\r\n","    valid_dataset = valid_dataset.map(normalize)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNcgOvl7-Aoj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fI_5INQR-BPG"},"source":["## 3. Tensorflow 중간규모 Model\r\n","- https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\r\n","- 여기 keras코드를 tensorflow코드로 재구성\r\n","- ImageDataGenerator로 data augmentation\r\n","\r\n","- lr=0.001로 시작해서,213 epoch 이후, val acc가 더 이상 좋아지지 않아(88.59%), lr=0.0001로 train 이어가기\r\n","- 288 epoch에 90.53%"]},{"cell_type":"code","metadata":{"id":"7N15JZWj-Le8","executionInfo":{"status":"ok","timestamp":1612357170968,"user_tz":-540,"elapsed":610,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import tensorflow as tf\r\n","from tensorflow.keras.applications import resnet\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_c3n38x-fIt"},"source":["class MyModel(tf.keras.Model):\r\n","    def __init__(self,input_shape):\r\n","        super(MyModel, self).__init__()\r\n","\r\n","        weight_decay = 1e-4\r\n","\r\n","        self.model = tf.keras.models.Sequential()\r\n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=input_shape))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.2))      \r\n","\r\n","\r\n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.3))    \r\n","\r\n","\r\n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())\r\n","        \r\n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n","        self.model.add(tf.keras.layers.BatchNormalization())\r\n","        self.model.add(tf.keras.layers.ELU())        \r\n","        \r\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n","        self.model.add(tf.keras.layers.Dropout(0.4))   \r\n","\r\n","        self.model.add(tf.keras.layers.Flatten())\r\n","\r\n","\r\n","        self.built = True  # summary가 제대로 작동한다.\r\n","\r\n","    def call(self,x,training=None):  # training의 default 값으로 None이 좋다(Ture/False보다)\r\n","        output = self.model(x)\r\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJv42Qmu-p-E"},"source":["batch_size = 128\r\n","\r\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \r\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\r\n","\r\n","train_datagen = ImageDataGenerator(rescale=1./255,\r\n","    rotation_range=15,\r\n","    width_shift_range=0.1,\r\n","    height_shift_range=0.1,\r\n","    horizontal_flip=True,\r\n","    )\r\n","train_dataset = train_datagen.flow(x_train, y_train.reshape(-1), batch_size=batch_size,shuffle=True)\r\n","\r\n","valid_datagen = ImageDataGenerator(rescale=1./255)\r\n","valid_dataset = valid_datagen.flow(x_test, y_test.reshape(-1), batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLaF-6uz-smN"},"source":["import os\r\n","\r\n","initial_learning_rate = 0.001\r\n","\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, clipnorm=0.5)  # lr or learning_rate\r\n","model.compile(optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\r\n","model_filename = \"Epoch-{epoch:02d}-{val_accuracy:.4f}\"\r\n","checkpoint_path = os.path.join('models/', model_filename)\r\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,mode='auto',save_best_only=True,monitor='val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-X4GpPo-2fm"},"source":["history = model.fit(train_dataset,epochs=200,steps_per_epoch=len(x_train)/batch_size,validation_data=valid_dataset,validation_freq=1, verbose=1,callbacks=[cp_callback],initial_epoch=0)"],"execution_count":null,"outputs":[]}]}