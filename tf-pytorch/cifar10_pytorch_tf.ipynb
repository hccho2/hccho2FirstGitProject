{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cifar10_pytorch_tf.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7383efc32e224bf99aae75656d176a63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ba198eb4a2243e1b58a7917023d8c70","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d0eb2e12619c403d8ae1ac42fbad1d5b","IPY_MODEL_0d3a55916d70497fb6bc5772e4b1040e"]}},"5ba198eb4a2243e1b58a7917023d8c70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0eb2e12619c403d8ae1ac42fbad1d5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d68717d05ace4f978165d44691a49e99","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef103562623a45dbb2fff8ba92ca8faa"}},"0d3a55916d70497fb6bc5772e4b1040e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3d2caa125dc64ff3b2baf9e4527b1fca","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 52336097.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8589ecfdc8d04ad0b18170f71f0089da"}},"d68717d05ace4f978165d44691a49e99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ef103562623a45dbb2fff8ba92ca8faa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d2caa125dc64ff3b2baf9e4527b1fca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8589ecfdc8d04ad0b18170f71f0089da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b3c9d7089834c51a6dc1cb54a2c5748":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a04ee292af4d45c3b7b1d54bf3f2d552","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dd4e21a3d7fe4286ac155e7d9665483e","IPY_MODEL_8d333014155c4b5a8c20282207ec260e"]}},"a04ee292af4d45c3b7b1d54bf3f2d552":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd4e21a3d7fe4286ac155e7d9665483e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1c77a081705941009b2ddb50358ed65c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5b52391f70647a38e94098218c4176c"}},"8d333014155c4b5a8c20282207ec260e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c97ad1924e7a4300922263f1e83e2ec9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 169MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fff4289a15f8452c9233f2286bb9cd38"}},"1c77a081705941009b2ddb50358ed65c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b5b52391f70647a38e94098218c4176c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c97ad1924e7a4300922263f1e83e2ec9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fff4289a15f8452c9233f2286bb9cd38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"YfVJyLm6xPsd"},"source":["# local pc --> colab 파일 업로드\n","#  여러개 동시 선택 가능 해야 됨\n","from google.colab import files\n","\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTXF5lSeyCW9"},"source":["# cifar 10으로 validation accuracy 높이기\n","## 1. pytorch\n","-  https://github.com/kuangliu/pytorch-cifar ==> pytorch 코드가 있는데 모델별로 92~95%\n","\n","- pytorch tutorial의 간단한 모델로 하면, val acc = 65%정도 나온다. https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n","\n","- transfer learning: pretrained weight를 고정하면 train이 잘 안된다. 고정하지 않아야 한다.\n","- vgg16, resnet18로 data augmentation 적용해서 train하면 val acc=83~85%정도 나온다. vgg16은 lr=0.00005로 해야 한다.\n","    * https://github.com/kuangliu/pytorch-cifar --> vgg16(BN이 포함되어 있다)으로 MultiStepLR, 344epoch(190분 소요) . 93.8%\n","    * pytorch vgg16(lr=0.00005): 100 epoch train acc = 98%, test acc = 83% 정도 나온다. ==> vgg는 모델이 커서 느리다.\n","    * pytorch resnet18 + MultiStepLR:\n","        - MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)\n","        - 150 epoch 전에 정체되다 150넘어서 lr이 조정되니 77% --> 84%\n","        - 214 epoch에 85.5% \n","    * pytorch resnet18 + torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  mode = 'max',threshold_mode='abs',threshold=0.001, factor=0.6,patience=10, min_lr=0.0001,verbose=True)\n","        - 216 epoch에 87.6% 이후, 더 이상 개선이 안됨\n","    * resnet18(kuangliu)+ReduceLROnPlateau: 모델 구조가 좀 다르다. 95.2%(150 epoch)\n","    * pytorch vgg16_bn+MultiStepLR: 168 epoch, 92.9%\n","\n","![image.png](data:image/png;base64,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)\n","\n"]},{"cell_type":"code","metadata":{"id":"WO2pp-UByAFB","executionInfo":{"status":"ok","timestamp":1612662182345,"user_tz":-540,"elapsed":4834,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision.models as models\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["7383efc32e224bf99aae75656d176a63","5ba198eb4a2243e1b58a7917023d8c70","d0eb2e12619c403d8ae1ac42fbad1d5b","0d3a55916d70497fb6bc5772e4b1040e","d68717d05ace4f978165d44691a49e99","ef103562623a45dbb2fff8ba92ca8faa","3d2caa125dc64ff3b2baf9e4527b1fca","8589ecfdc8d04ad0b18170f71f0089da"]},"id":"WO5-y3FZyAuS","executionInfo":{"status":"ok","timestamp":1612662192368,"user_tz":-540,"elapsed":8676,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"d0bec1df-7ebb-4519-a657-2503f2809aa9"},"source":["#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # padding 후, 지정한 크기로 잘라낸다. https://chloes-dl.com/2019/11/13/pytorch101-data-preprocessing-and-augmentation-part-1/\n","    transforms.RandomHorizontalFlip(),\n","    transforms.Resize((224,224)),\n","\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","\n","\n","batch_size = 128\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)  # torchvision.datasets.cifar.CIFAR10\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","print(f'train data: {len(trainset)}, test data: {len(testset)}')\n","print(f'# of mini-batch: {len(trainset)//batch_size}')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7383efc32e224bf99aae75656d176a63","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","train data: 50000, test data: 10000\n","# of mini-batch: 390\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["8b3c9d7089834c51a6dc1cb54a2c5748","a04ee292af4d45c3b7b1d54bf3f2d552","dd4e21a3d7fe4286ac155e7d9665483e","8d333014155c4b5a8c20282207ec260e","1c77a081705941009b2ddb50358ed65c","b5b52391f70647a38e94098218c4176c","c97ad1924e7a4300922263f1e83e2ec9","fff4289a15f8452c9233f2286bb9cd38"]},"id":"BJL0CmjF23c5","executionInfo":{"status":"ok","timestamp":1612662202058,"user_tz":-540,"elapsed":1752,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}},"outputId":"4c4899ef-b7e4-4ad0-8a3c-59103fe49aec"},"source":["cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","class x_VGG(nn.Module):  # beginner-blitz-cifar10-tutorial-py에 있는 VGG\n","    def __init__(self, vgg_name):\n","        super(x_VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","class MyVGG(nn.Module):\n","    def __init__(self):\n","        super(MyVGG, self).__init__()\n","        self.flatten = nn.Flatten()\n","\n","        self.vgg = models.vgg16_bn(pretrained=True,progress=True).features\n","        self.fc = nn.Linear(512,10)\n","\n","    def forward(self, x):\n","        x = self.vgg(x)\n","        x = self.flatten(x)\n","        x = self.fc(x)\n","        return x\n","\n","my_resnet = models.resnet18(pretrained=True)\n","my_resnet.fc = nn.Linear(my_resnet.fc.in_features,10)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b3c9d7089834c51a6dc1cb54a2c5748","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jNki9l623ha","executionInfo":{"status":"ok","timestamp":1612662237266,"user_tz":-540,"elapsed":411,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_selection = 1  # 1: resnet, 2: vgg, 3. x_VGG\n","\n","if model_selection ==1:\n","    net = my_resnet\n","elif model_selection ==2:\n","    net = MyVGG()   # lr=0.00005 또는 Adam말고 SGD로 해야됨.\n","else:\n","    net = x_VGG('VGG16')\n","\n","\n","net.to(device)   \n","criterion = nn.CrossEntropyLoss()\n","\n","if model_selection ==1:\n","    #optimizer = optim.SGD(net.parameters(), lr=0.001)\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # \n","    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)  # train이 정체되다 150 epoch을 지나면서, lr이 바뀌니 확 좋아짐.\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  mode = 'max',threshold_mode='abs',threshold=0.001, factor=0.6,patience=10, min_lr=0.0001,verbose=True)\n","\n","\n","elif model_selection ==2:\n","    #optimizer = optim.Adam(net.parameters(), lr=0.0001,betas=(0.5, 0.9))\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # \n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)\n","\n","else:\n","    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)  # for x_VGG\n","    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[150,300,450],gamma=0.1)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4NtIEXC23oM","outputId":"6e5b074f-bcf1-4cd3-a3f2-25306277d3bb"},"source":["s_time = time.time()\n","best_acc = 0\n","best_duration=0\n","for epoch in range(400):  # loop over the dataset multiple times\n","\n","    running_loss = []\n","    acc = 0\n","    total = 0\n","    net.train()\n","    for i, data in enumerate(trainloader):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        _, pred = outputs.max(axis=-1)\n","        acc  += (pred==labels).float().sum().item()\n","        total += len(labels)\n","\n","\n","        # print statistics\n","        running_loss.append(loss.item())\n","    print('[epoch: %d] loss: %.3f, train acc: %.3f elapsed: %.2f' % (epoch + 1, np.mean(running_loss), acc/total , time.time()-s_time),end=\"\\t\" )\n","    #scheduler.step()\n","    acc = 0\n","    total=0\n","    net.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader):\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            outputs = net(inputs)\n","            \n","            _, pred = outputs.max(axis=-1)\n","            acc  += (pred.cpu()==labels).float().sum().item()\n","            total += len(labels)\n","    if acc/total > best_acc:\n","        best_acc = acc/total\n","        print('test acc: %.3f, best: %.3f ===== new best ' % (best_acc,best_acc ))\n","        best_duration = 0\n","    else:\n","        best_duration += 1\n","        print('test acc: %.3f, best: %.3f, best duration: %d ' % (acc/total,best_acc, best_duration ) )\n","    scheduler.step(acc/total)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[epoch: 1] loss: 2.689, train acc: 0.181 elapsed: 151.23\ttest acc: 0.307, best: 0.000 ===== new best \n","[epoch: 2] loss: 1.751, train acc: 0.342 elapsed: 324.97\ttest acc: 0.391, best: 0.307 ===== new best \n","[epoch: 3] loss: 1.516, train acc: 0.439 elapsed: 499.75\ttest acc: 0.457, best: 0.391 ===== new best \n","[epoch: 4] loss: 1.329, train acc: 0.517 elapsed: 674.70\ttest acc: 0.504, best: 0.457 ===== new best \n","[epoch: 5] loss: 1.142, train acc: 0.592 elapsed: 850.19\ttest acc: 0.569, best: 0.504 ===== new best \n","[epoch: 6] loss: 0.958, train acc: 0.662 elapsed: 1025.89\ttest acc: 0.607, best: 0.569 ===== new best \n","[epoch: 7] loss: 0.819, train acc: 0.714 elapsed: 1201.12\ttest acc: 0.711, best: 0.607 ===== new best \n","[epoch: 8] loss: 0.726, train acc: 0.747 elapsed: 1376.94\ttest acc: 0.660, best: 0.711, best duration: 1 \n","[epoch: 9] loss: 0.663, train acc: 0.770 elapsed: 1552.54\ttest acc: 0.731, best: 0.711 ===== new best \n","[epoch: 10] loss: 0.621, train acc: 0.785 elapsed: 1728.44\ttest acc: 0.762, best: 0.731 ===== new best \n","[epoch: 11] loss: 0.583, train acc: 0.795 elapsed: 1904.15\ttest acc: 0.755, best: 0.762, best duration: 1 \n","[epoch: 12] loss: 0.561, train acc: 0.806 elapsed: 2079.68\ttest acc: 0.695, best: 0.762, best duration: 2 \n","[epoch: 13] loss: 0.545, train acc: 0.812 elapsed: 2255.19\ttest acc: 0.777, best: 0.762 ===== new best \n","[epoch: 14] loss: 0.521, train acc: 0.819 elapsed: 2431.06\ttest acc: 0.802, best: 0.777 ===== new best \n","[epoch: 15] loss: 0.511, train acc: 0.822 elapsed: 2606.63\ttest acc: 0.788, best: 0.802, best duration: 1 \n","[epoch: 16] loss: 0.499, train acc: 0.826 elapsed: 2782.23\ttest acc: 0.772, best: 0.802, best duration: 2 \n","[epoch: 17] loss: 0.488, train acc: 0.833 elapsed: 2957.58\ttest acc: 0.738, best: 0.802, best duration: 3 \n","[epoch: 18] loss: 0.475, train acc: 0.837 elapsed: 3133.23\ttest acc: 0.786, best: 0.802, best duration: 4 \n","[epoch: 19] loss: 0.470, train acc: 0.838 elapsed: 3308.95\ttest acc: 0.770, best: 0.802, best duration: 5 \n","[epoch: 20] loss: 0.457, train acc: 0.843 elapsed: 3484.47\ttest acc: 0.802, best: 0.802 ===== new best \n","[epoch: 21] loss: 0.450, train acc: 0.844 elapsed: 3659.42\ttest acc: 0.762, best: 0.802, best duration: 1 \n","[epoch: 22] loss: 0.445, train acc: 0.846 elapsed: 3834.45\ttest acc: 0.775, best: 0.802, best duration: 2 \n","[epoch: 23] loss: 0.443, train acc: 0.848 elapsed: 4009.93\ttest acc: 0.789, best: 0.802, best duration: 3 \n","[epoch: 24] loss: 0.428, train acc: 0.853 elapsed: 4185.20\ttest acc: 0.775, best: 0.802, best duration: 4 \n","[epoch: 25] loss: 0.426, train acc: 0.852 elapsed: 4360.59\ttest acc: 0.806, best: 0.802 ===== new best \n","[epoch: 26] loss: 0.420, train acc: 0.855 elapsed: 4535.67\ttest acc: 0.793, best: 0.806, best duration: 1 \n","[epoch: 27] loss: 0.423, train acc: 0.853 elapsed: 4710.16\ttest acc: 0.819, best: 0.806 ===== new best \n","[epoch: 28] loss: 0.413, train acc: 0.857 elapsed: 4884.14\ttest acc: 0.804, best: 0.819, best duration: 1 \n","[epoch: 29] loss: 0.408, train acc: 0.859 elapsed: 5057.88\ttest acc: 0.810, best: 0.819, best duration: 2 \n","[epoch: 30] loss: 0.407, train acc: 0.860 elapsed: 5231.21\ttest acc: 0.817, best: 0.819, best duration: 3 \n","[epoch: 31] loss: 0.402, train acc: 0.862 elapsed: 5404.44\ttest acc: 0.805, best: 0.819, best duration: 4 \n","[epoch: 32] loss: 0.393, train acc: 0.864 elapsed: 5577.82\ttest acc: 0.824, best: 0.819 ===== new best \n","[epoch: 33] loss: 0.398, train acc: 0.862 elapsed: 5750.87\ttest acc: 0.826, best: 0.824 ===== new best \n","[epoch: 34] loss: 0.394, train acc: 0.864 elapsed: 5924.19\ttest acc: 0.821, best: 0.826, best duration: 1 \n","[epoch: 35] loss: 0.390, train acc: 0.866 elapsed: 6097.37\ttest acc: 0.796, best: 0.826, best duration: 2 \n","[epoch: 36] loss: 0.387, train acc: 0.868 elapsed: 6270.53\ttest acc: 0.789, best: 0.826, best duration: 3 \n","[epoch: 37] loss: 0.380, train acc: 0.869 elapsed: 6443.57\ttest acc: 0.815, best: 0.826, best duration: 4 \n","[epoch: 38] loss: 0.375, train acc: 0.870 elapsed: 6616.76\ttest acc: 0.801, best: 0.826, best duration: 5 \n","[epoch: 39] loss: 0.378, train acc: 0.870 elapsed: 6789.72\ttest acc: 0.815, best: 0.826, best duration: 6 \n","[epoch: 40] loss: 0.376, train acc: 0.870 elapsed: 6962.80\ttest acc: 0.823, best: 0.826, best duration: 7 \n","[epoch: 41] loss: 0.363, train acc: 0.875 elapsed: 7135.70\ttest acc: 0.818, best: 0.826, best duration: 8 \n","[epoch: 42] loss: 0.374, train acc: 0.872 elapsed: 7308.42\ttest acc: 0.828, best: 0.826 ===== new best \n","[epoch: 43] loss: 0.366, train acc: 0.874 elapsed: 7481.16\ttest acc: 0.827, best: 0.828, best duration: 1 \n","[epoch: 44] loss: 0.363, train acc: 0.875 elapsed: 7653.70\ttest acc: 0.805, best: 0.828, best duration: 2 \n","[epoch: 45] loss: 0.359, train acc: 0.876 elapsed: 7826.34\ttest acc: 0.835, best: 0.828 ===== new best \n","[epoch: 46] loss: 0.361, train acc: 0.876 elapsed: 7998.71\ttest acc: 0.799, best: 0.835, best duration: 1 \n","[epoch: 47] loss: 0.357, train acc: 0.877 elapsed: 8171.17\ttest acc: 0.843, best: 0.835 ===== new best \n","[epoch: 48] loss: 0.355, train acc: 0.876 elapsed: 8343.68\ttest acc: 0.834, best: 0.843, best duration: 1 \n","[epoch: 49] loss: 0.344, train acc: 0.882 elapsed: 8515.50\ttest acc: 0.805, best: 0.843, best duration: 2 \n","[epoch: 50] loss: 0.353, train acc: 0.878 elapsed: 8687.58\ttest acc: 0.827, best: 0.843, best duration: 3 \n","[epoch: 51] loss: 0.347, train acc: 0.881 elapsed: 8859.61\ttest acc: 0.832, best: 0.843, best duration: 4 \n","[epoch: 52] loss: 0.350, train acc: 0.879 elapsed: 9031.68\ttest acc: 0.825, best: 0.843, best duration: 5 \n","[epoch: 53] loss: 0.352, train acc: 0.879 elapsed: 9203.70\ttest acc: 0.829, best: 0.843, best duration: 6 \n","[epoch: 54] loss: 0.344, train acc: 0.881 elapsed: 9375.60\ttest acc: 0.809, best: 0.843, best duration: 7 \n","[epoch: 55] loss: 0.343, train acc: 0.882 elapsed: 9547.69\ttest acc: 0.842, best: 0.843, best duration: 8 \n","[epoch: 56] loss: 0.340, train acc: 0.882 elapsed: 9719.69\ttest acc: 0.824, best: 0.843, best duration: 9 \n","[epoch: 57] loss: 0.341, train acc: 0.883 elapsed: 9891.37\ttest acc: 0.803, best: 0.843, best duration: 10 \n","[epoch: 58] loss: 0.336, train acc: 0.884 elapsed: 10063.06\ttest acc: 0.786, best: 0.843, best duration: 11 \n","Epoch    58: reducing learning rate of group 0 to 6.0000e-02.\n","[epoch: 59] loss: 0.239, train acc: 0.918 elapsed: 10234.90\ttest acc: 0.852, best: 0.843 ===== new best \n","[epoch: 60] loss: 0.236, train acc: 0.918 elapsed: 10406.60\ttest acc: 0.864, best: 0.852 ===== new best \n","[epoch: 61] loss: 0.240, train acc: 0.918 elapsed: 10578.34\ttest acc: 0.844, best: 0.864, best duration: 1 \n","[epoch: 62] loss: 0.246, train acc: 0.914 elapsed: 10750.20\ttest acc: 0.824, best: 0.864, best duration: 2 \n","[epoch: 63] loss: 0.242, train acc: 0.917 elapsed: 10921.82\ttest acc: 0.861, best: 0.864, best duration: 3 \n","[epoch: 64] loss: 0.246, train acc: 0.916 elapsed: 11093.26\ttest acc: 0.869, best: 0.864 ===== new best \n","[epoch: 65] loss: 0.243, train acc: 0.915 elapsed: 11264.91\ttest acc: 0.847, best: 0.869, best duration: 1 \n","[epoch: 66] loss: 0.248, train acc: 0.915 elapsed: 11436.21\ttest acc: 0.852, best: 0.869, best duration: 2 \n","[epoch: 67] loss: 0.241, train acc: 0.916 elapsed: 11607.81\ttest acc: 0.822, best: 0.869, best duration: 3 \n","[epoch: 68] loss: 0.236, train acc: 0.918 elapsed: 11779.30\ttest acc: 0.874, best: 0.869 ===== new best \n","[epoch: 69] loss: 0.243, train acc: 0.916 elapsed: 11950.81\ttest acc: 0.858, best: 0.874, best duration: 1 \n","[epoch: 70] loss: 0.239, train acc: 0.918 elapsed: 12122.40\ttest acc: 0.881, best: 0.874 ===== new best \n","[epoch: 71] loss: 0.231, train acc: 0.920 elapsed: 12293.98\ttest acc: 0.867, best: 0.881, best duration: 1 \n","[epoch: 72] loss: 0.241, train acc: 0.917 elapsed: 12465.48\ttest acc: 0.835, best: 0.881, best duration: 2 \n","[epoch: 73] loss: 0.231, train acc: 0.921 elapsed: 12637.09\ttest acc: 0.858, best: 0.881, best duration: 3 \n","[epoch: 74] loss: 0.238, train acc: 0.918 elapsed: 12808.71\ttest acc: 0.875, best: 0.881, best duration: 4 \n","[epoch: 75] loss: 0.237, train acc: 0.920 elapsed: 12980.21\ttest acc: 0.879, best: 0.881, best duration: 5 \n","[epoch: 76] loss: 0.231, train acc: 0.921 elapsed: 13152.08\ttest acc: 0.877, best: 0.881, best duration: 6 \n","[epoch: 77] loss: 0.232, train acc: 0.919 elapsed: 13323.71\ttest acc: 0.867, best: 0.881, best duration: 7 \n","[epoch: 78] loss: 0.231, train acc: 0.922 elapsed: 13495.30\ttest acc: 0.857, best: 0.881, best duration: 8 \n","[epoch: 79] loss: 0.229, train acc: 0.922 elapsed: 13667.05\ttest acc: 0.855, best: 0.881, best duration: 9 \n","[epoch: 80] loss: 0.230, train acc: 0.921 elapsed: 13838.41\ttest acc: 0.861, best: 0.881, best duration: 10 \n","[epoch: 81] loss: 0.226, train acc: 0.922 elapsed: 14009.85\ttest acc: 0.822, best: 0.881, best duration: 11 \n","Epoch    81: reducing learning rate of group 0 to 3.6000e-02.\n","[epoch: 82] loss: 0.144, train acc: 0.952 elapsed: 14181.22\ttest acc: 0.904, best: 0.881 ===== new best \n","[epoch: 83] loss: 0.130, train acc: 0.956 elapsed: 14352.80\ttest acc: 0.911, best: 0.904 ===== new best \n","[epoch: 84] loss: 0.134, train acc: 0.954 elapsed: 14524.07\ttest acc: 0.886, best: 0.911, best duration: 1 \n","[epoch: 85] loss: 0.138, train acc: 0.952 elapsed: 14695.70\ttest acc: 0.890, best: 0.911, best duration: 2 \n","[epoch: 86] loss: 0.143, train acc: 0.951 elapsed: 14867.15\ttest acc: 0.880, best: 0.911, best duration: 3 \n","[epoch: 87] loss: 0.149, train acc: 0.950 elapsed: 15038.77\ttest acc: 0.894, best: 0.911, best duration: 4 \n","[epoch: 88] loss: 0.144, train acc: 0.950 elapsed: 15210.26\ttest acc: 0.891, best: 0.911, best duration: 5 \n","[epoch: 89] loss: 0.145, train acc: 0.951 elapsed: 15381.69\ttest acc: 0.888, best: 0.911, best duration: 6 \n","[epoch: 90] loss: 0.156, train acc: 0.946 elapsed: 15553.56\ttest acc: 0.900, best: 0.911, best duration: 7 \n","[epoch: 91] loss: 0.144, train acc: 0.951 elapsed: 15725.30\ttest acc: 0.887, best: 0.911, best duration: 8 \n","[epoch: 92] loss: 0.147, train acc: 0.950 elapsed: 15896.79\ttest acc: 0.880, best: 0.911, best duration: 9 \n","[epoch: 93] loss: 0.149, train acc: 0.948 elapsed: 16068.25\ttest acc: 0.875, best: 0.911, best duration: 10 \n","[epoch: 94] loss: 0.145, train acc: 0.950 elapsed: 16239.95\ttest acc: 0.882, best: 0.911, best duration: 11 \n","Epoch    94: reducing learning rate of group 0 to 2.1600e-02.\n","[epoch: 95] loss: 0.087, train acc: 0.972 elapsed: 16411.39\ttest acc: 0.923, best: 0.911 ===== new best \n","[epoch: 96] loss: 0.062, train acc: 0.981 elapsed: 16582.95\ttest acc: 0.920, best: 0.923, best duration: 1 \n","[epoch: 97] loss: 0.058, train acc: 0.982 elapsed: 16754.62\ttest acc: 0.917, best: 0.923, best duration: 2 \n","[epoch: 98] loss: 0.063, train acc: 0.980 elapsed: 16926.12\ttest acc: 0.910, best: 0.923, best duration: 3 \n","[epoch: 99] loss: 0.066, train acc: 0.979 elapsed: 17097.64\ttest acc: 0.916, best: 0.923, best duration: 4 \n","[epoch: 100] loss: 0.070, train acc: 0.977 elapsed: 17269.15\ttest acc: 0.897, best: 0.923, best duration: 5 \n","[epoch: 101] loss: 0.077, train acc: 0.974 elapsed: 17440.86\ttest acc: 0.912, best: 0.923, best duration: 6 \n","[epoch: 102] loss: 0.080, train acc: 0.973 elapsed: 17612.29\ttest acc: 0.902, best: 0.923, best duration: 7 \n","[epoch: 103] loss: 0.084, train acc: 0.972 elapsed: 17783.66\ttest acc: 0.909, best: 0.923, best duration: 8 \n","[epoch: 104] loss: 0.087, train acc: 0.972 elapsed: 17955.13\ttest acc: 0.890, best: 0.923, best duration: 9 \n","[epoch: 105] loss: 0.087, train acc: 0.971 elapsed: 18126.67\ttest acc: 0.901, best: 0.923, best duration: 10 \n","[epoch: 106] loss: 0.085, train acc: 0.972 elapsed: 18298.10\ttest acc: 0.891, best: 0.923, best duration: 11 \n","Epoch   106: reducing learning rate of group 0 to 1.2960e-02.\n","[epoch: 107] loss: 0.044, train acc: 0.987 elapsed: 18469.60\ttest acc: 0.929, best: 0.923 ===== new best \n","[epoch: 108] loss: 0.027, train acc: 0.993 elapsed: 18641.31\ttest acc: 0.933, best: 0.929 ===== new best \n","[epoch: 109] loss: 0.021, train acc: 0.995 elapsed: 18813.06\ttest acc: 0.930, best: 0.933, best duration: 1 \n","[epoch: 110] loss: 0.020, train acc: 0.996 elapsed: 18984.54\ttest acc: 0.937, best: 0.933 ===== new best \n","[epoch: 111] loss: 0.018, train acc: 0.996 elapsed: 19155.90\ttest acc: 0.934, best: 0.937, best duration: 1 \n","[epoch: 112] loss: 0.018, train acc: 0.996 elapsed: 19327.35\ttest acc: 0.936, best: 0.937, best duration: 2 \n","[epoch: 113] loss: 0.019, train acc: 0.995 elapsed: 19498.80\ttest acc: 0.933, best: 0.937, best duration: 3 \n","[epoch: 114] loss: 0.018, train acc: 0.996 elapsed: 19670.48\ttest acc: 0.929, best: 0.937, best duration: 4 \n","[epoch: 115] loss: 0.018, train acc: 0.996 elapsed: 19842.03\ttest acc: 0.933, best: 0.937, best duration: 5 \n","[epoch: 116] loss: 0.022, train acc: 0.995 elapsed: 20013.56\ttest acc: 0.931, best: 0.937, best duration: 6 \n","[epoch: 117] loss: 0.023, train acc: 0.995 elapsed: 20185.08\ttest acc: 0.932, best: 0.937, best duration: 7 \n","[epoch: 118] loss: 0.034, train acc: 0.990 elapsed: 20356.46\ttest acc: 0.922, best: 0.937, best duration: 8 \n","[epoch: 119] loss: 0.037, train acc: 0.990 elapsed: 20527.86\ttest acc: 0.924, best: 0.937, best duration: 9 \n","[epoch: 120] loss: 0.039, train acc: 0.988 elapsed: 20699.31\ttest acc: 0.911, best: 0.937, best duration: 10 \n","[epoch: 121] loss: 0.044, train acc: 0.987 elapsed: 20870.98\ttest acc: 0.898, best: 0.937, best duration: 11 \n","Epoch   121: reducing learning rate of group 0 to 7.7760e-03.\n","[epoch: 122] loss: 0.022, train acc: 0.995 elapsed: 21042.46\ttest acc: 0.934, best: 0.937, best duration: 12 \n","[epoch: 123] loss: 0.012, train acc: 0.998 elapsed: 21213.91\ttest acc: 0.938, best: 0.937 ===== new best \n","[epoch: 124] loss: 0.010, train acc: 0.999 elapsed: 21385.51\ttest acc: 0.938, best: 0.938, best duration: 1 \n","[epoch: 125] loss: 0.009, train acc: 0.999 elapsed: 21557.03\ttest acc: 0.933, best: 0.938, best duration: 2 \n","[epoch: 126] loss: 0.008, train acc: 0.999 elapsed: 21728.69\ttest acc: 0.939, best: 0.938 ===== new best \n","[epoch: 127] loss: 0.007, train acc: 0.999 elapsed: 21900.25\ttest acc: 0.941, best: 0.939 ===== new best \n","[epoch: 128] loss: 0.007, train acc: 0.999 elapsed: 22071.93\ttest acc: 0.940, best: 0.941, best duration: 1 \n","[epoch: 129] loss: 0.006, train acc: 1.000 elapsed: 22243.59\ttest acc: 0.941, best: 0.941 ===== new best \n","[epoch: 130] loss: 0.006, train acc: 1.000 elapsed: 22414.99\ttest acc: 0.943, best: 0.941 ===== new best \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_RWsdOpb4h3_"},"source":["# test accuracy\n","correct = 0\n","total = 0\n","labels_all = []\n","predicted_all = []\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images.to(device))\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted.cpu() == labels).sum().item()\n","\n","        labels_all.extend(labels.numpy())\n","        predicted_all.extend(predicted.cpu().numpy())\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9rel3E14wU5"},"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","confusion_matrix = confusion_matrix(labels_all, predicted_all)  # numpy array\n","sns.heatmap(confusion_matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g',cmap=\"YlGnBu\")  # cmap=\"Blues\"\n","plt.xlabel('Prediction')\n","plt.ylabel('Label')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wfh5GO-Z40BO"},"source":["### torchsummay로 모델 구조 살펴보기\n","- pip install torchsummary"]},{"cell_type":"code","metadata":{"id":"5PHpFk8F44Ct"},"source":["from torchsummary import summary\n","#my_resnet.to('cpu')\n","\n","net = MyVGG()\n","summary(net, input_size=(3, 32, 32),device='cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5XS6Ukb47Sy"},"source":["## 2. Tensorflow\n","- tensorflow는 resnet18을 제공하지 않는다. resnet50이 resnet중에는 제일 작은 모델\n","- resnet.preprocess_input은 별 효과 없다. 자체적인 augmentation이 낫다.\n","- resnet50이 규모가 있기 때문에, classifier의 fc에 dropout을 적용하는 것이 낫다.\n","- clipnorm은 별 효과 없다.\n","\n","    - resnet50: 100 epoch, train acc = 99%, val acc = 79%~80%\n","    - resnet18: 200 epoch, train acc = 95%, val acc = 80%\n","\n","- pytorch만큼 성능이 안 나옴. 개선책\n","    * https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/ 여기 있는 좀 더 간단한 구조로 하면 90%까지 나옴\n","    * resnet 모델 수정 1: 초반의 7x7을 3x3으로 수정하고, maxpooling 제거: 87.52%(100 epoch)\n","    * kuangliu의 pytorch 코드를 Tensorflow 코드로 동일하게 변환"]},{"cell_type":"code","metadata":{"id":"oK-Hyv4280oP"},"source":["import tensorflow as tf\n","from tensorflow.keras.applications import resnet\n","import matplotlib.pyplot as plt\n","import os\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEXreONY480s"},"source":["# https://github.com/calmisential/TensorFlow2.0_ResNet/tree/master/models ---> resnet original\n","\n","NUM_CLASSES = 10\n","\n","class BasicBlock(tf.keras.layers.Layer):\n","\n","    def __init__(self, filter_num, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(3, 3),\n","                                            strides=stride,\n","                                            padding=\"same\",use_bias=False)\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(3, 3),\n","                                            strides=1,\n","                                            padding=\"same\",use_bias=False)\n","        self.bn2 = tf.keras.layers.BatchNormalization()\n","        if stride != 1:\n","            self.downsample = tf.keras.Sequential()\n","            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n","                                                       kernel_size=(1, 1),\n","                                                       strides=stride,use_bias=False))\n","            self.downsample.add(tf.keras.layers.BatchNormalization())\n","        else:\n","            self.downsample = lambda x: x\n","\n","    def call(self, inputs, training=None, **kwargs):\n","        residual = self.downsample(inputs)\n","\n","        x = self.conv1(inputs)\n","        x = self.bn1(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x, training=training)\n","\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n","\n","        return output\n","\n","\n","class BottleNeck(tf.keras.layers.Layer):\n","    def __init__(self, filter_num, stride=1):\n","        super(BottleNeck, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(1, 1),\n","                                            strides=1,\n","                                            padding='same',use_bias=False)\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(3, 3),\n","                                            strides=stride,\n","                                            padding='same',use_bias=False)\n","        self.bn2 = tf.keras.layers.BatchNormalization()\n","        self.conv3 = tf.keras.layers.Conv2D(filters=filter_num * 4,\n","                                            kernel_size=(1, 1),\n","                                            strides=1,\n","                                            padding='same',use_bias=False)\n","        self.bn3 = tf.keras.layers.BatchNormalization()\n","\n","        self.downsample = tf.keras.Sequential()\n","        self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num * 4,\n","                                                   kernel_size=(1, 1),\n","                                                   strides=stride,use_bias=False))\n","        self.downsample.add(tf.keras.layers.BatchNormalization())\n","\n","    def call(self, inputs, training=None, **kwargs):\n","        residual = self.downsample(inputs)\n","\n","        x = self.conv1(inputs)\n","        x = self.bn1(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.conv3(x)\n","        x = self.bn3(x, training=training)\n","\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n","\n","        return output\n","\n","\n","def make_basic_block_layer(filter_num, blocks, stride=1):\n","    res_block = tf.keras.Sequential()\n","    res_block.add(BasicBlock(filter_num, stride=stride))\n","\n","    for _ in range(1, blocks):\n","        res_block.add(BasicBlock(filter_num, stride=1))\n","\n","    return res_block\n","\n","\n","def make_bottleneck_layer(filter_num, blocks, stride=1):\n","    res_block = tf.keras.Sequential()\n","    res_block.add(BottleNeck(filter_num, stride=stride))\n","\n","    for _ in range(1, blocks):\n","        res_block.add(BottleNeck(filter_num, stride=1))\n","\n","    return res_block\n","class ResNetTypeI(tf.keras.Model):\n","    def __init__(self, layer_params):\n","        super(ResNetTypeI, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n","                                            kernel_size=(7, 7),\n","                                            strides=2,\n","                                            padding=\"same\",use_bias=False)\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n","                                               strides=2,\n","                                               padding=\"same\")\n","\n","        self.layer1 = make_basic_block_layer(filter_num=64,\n","                                             blocks=layer_params[0])\n","        self.layer2 = make_basic_block_layer(filter_num=128,\n","                                             blocks=layer_params[1],\n","                                             stride=2)\n","        self.layer3 = make_basic_block_layer(filter_num=256,\n","                                             blocks=layer_params[2],\n","                                             stride=2)\n","        self.layer4 = make_basic_block_layer(filter_num=512,\n","                                             blocks=layer_params[3],\n","                                             stride=2)\n","\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=None)\n","\n","    def call(self, inputs, training=None, mask=None):\n","        x = self.conv1(inputs)\n","        x = self.bn1(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.pool1(x)\n","        x = self.layer1(x, training=training)\n","        x = self.layer2(x, training=training)\n","        x = self.layer3(x, training=training)\n","        x = self.layer4(x, training=training)\n","        x = self.avgpool(x)\n","        output = self.fc(x)\n","\n","        return output\n","\n","\n","class ResNetTypeII(tf.keras.Model):\n","    def __init__(self, layer_params):\n","        super(ResNetTypeII, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n","                                            kernel_size=(7, 7),\n","                                            strides=2,\n","                                            padding=\"same\",use_bias=False)\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n","                                               strides=2,\n","                                               padding=\"same\")\n","\n","        self.layer1 = make_bottleneck_layer(filter_num=64,\n","                                            blocks=layer_params[0])\n","        self.layer2 = make_bottleneck_layer(filter_num=128,\n","                                            blocks=layer_params[1],\n","                                            stride=2)\n","        self.layer3 = make_bottleneck_layer(filter_num=256,\n","                                            blocks=layer_params[2],\n","                                            stride=2)\n","        self.layer4 = make_bottleneck_layer(filter_num=512,\n","                                            blocks=layer_params[3],\n","                                            stride=2)\n","\n","        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n","        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=None)\n","\n","    def call(self, inputs, training=None, mask=None):\n","        x = self.conv1(inputs)\n","        x = self.bn1(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.pool1(x)\n","        x = self.layer1(x, training=training)\n","        x = self.layer2(x, training=training)\n","        x = self.layer3(x, training=training)\n","        x = self.layer4(x, training=training)\n","        x = self.avgpool(x)\n","        output = self.fc(x)\n","\n","        return output\n","\n","\n","def resnet_18():\n","    return ResNetTypeI(layer_params=[2, 2, 2, 2])\n","\n","\n","def resnet_34():\n","    return ResNetTypeI(layer_params=[3, 4, 6, 3])\n","\n","\n","def resnet_50():\n","    return ResNetTypeII(layer_params=[3, 4, 6, 3])\n","\n","\n","def resnet_101():\n","    return ResNetTypeII(layer_params=[3, 4, 23, 3])\n","\n","\n","def resnet_152():\n","    return ResNetTypeII(layer_params=[3, 8, 36, 3])\n","\n","\n","class CommonLayers (tf.keras.Model):\n","    def __init__ (self, flag = 0):\n","        super().__init__()\n","        self.flag = flag\n","        if flag != 0:\n","            self.conv = tf.keras.layers.Conv2D(filters = flag, kernel_size = 3, padding=\"SAME\", kernel_initializer='glorot_uniform', use_bias=False)\n","        self.bn = tf.keras.layers.BatchNormalization ()\n","\n","    def call (self, inputs):\n","        if self.flag == 0:\n","            return tf.nn.relu (self.bn (inputs))\n","        else:\n","            return tf.nn.relu (self.bn (self.conv (inputs)))\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXwDIQu70_o"},"source":["def plot_history(history):\n","    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n","\n","        :param history: Training history of model\n","        :return:\n","    \"\"\"\n","    fig, axs = plt.subplots(1,2,figsize=(12, 5))\n","    \n","    # create accuracy sublpot\n","    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n","    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n","    axs[0].set_ylabel(\"Accuracy\")\n","    axs[0].legend(loc=\"lower right\")\n","    axs[0].set_title(\"Accuracy eval\")\n","\n","    # create error sublpot\n","    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n","    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n","    axs[1].set_ylabel(\"Error\")\n","    axs[1].set_xlabel(\"Epoch\")\n","    axs[1].legend(loc=\"upper right\")\n","    axs[1].set_title(\"Error eval\")\n","\n","    plt.show()\n","def my_preprocessing(image,label):\n","\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\n","\n","    image = tf.image.random_flip_left_right(image)  # 확률 50%로 고정되어 있음.\n","    image = tf.image.random_brightness(image, max_delta=0.3)\n","    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)  # 채도 조절\n","    # random crop\n","    shape = tf.shape(image)  # batch size 알아내기\n","    image = tf.image.resize_with_pad(image, 40,40)\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\n","\n","    image = tf.clip_by_value(image, 0.0, 1.0)\n","\n","    return image, label\n","\n","def my_preprocessing2(image,label):\n","\n","    image = image/255.   # tf.image.convert_image_dtype는 정수가 들어 왔을 때, 0~1로 변환한다. 넘어온 image에는 resize되면서 0~255사이의 float 값이 들어 있다.\n","    # random crop\n","    shape = tf.shape(image)  # batch size 알아내기\n","    image = tf.image.resize_with_pad(image, 36,36)\n","    image = tf.image.random_crop(image,(shape[0],32,32,3))\n","    image = tf.image.random_flip_left_right(image)\n","\n","    image = tf.clip_by_value(image, 0.0, 1.0)\n","    \n","\n","    return image, label\n","def normalize(image,label):\n","    return (image - [0.4914, 0.4822, 0.4465] )/ [0.2023, 0.1994, 0.2010], label\n","\n","def pad_and_random_crop(image):\n","    image = tf.image.resize_with_pad(image, 36,36)\n","    image = tf.image.random_crop(image,(32,32,3))   \n","    return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"buOnVIWR71E4"},"source":["model_selection = 3   # 1: resnet50, 2: resnet18\n","\n","if model_selection==1:\n","    base_model = resnet.ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))  # weights=None, weights='imagenet'\n","    base_model.trainable = True\n","\n","    my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dropout(0.5),tf.keras.layers.Dense(10)])\n","elif model_selection==2:\n","    #my_resnet = resnet_18()\n","    my_resnet = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Resizing(224,224),resnet_18()])  # resnet에 32x32 대신에 224,224로 resize해서 입력. ImageDataGenerator에서 resize가 원할하기 않아서, 이곳에서 resize\n","else:\n","    base_model = ResNetX18()\n","    my_resnet = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaXJdNoF8DEz"},"source":["# data loading\n","batch_size = 128\n","\n","DATA_MODE = 2 # 1: tf.data.Dataset  2: tf.ImageDataGenerator\n","\n","if DATA_MODE==1:\n","    print('Data from  from_tensor_slices')\n","    resnet_pre = False\n","\n","    if resnet_pre:\n","        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \n","        print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n","        train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\n","        train_dataset = train_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\n","\n","        train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n","\n","\n","        valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test, tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\n","        valid_dataset = valid_dataset.map(lambda a,b: (resnet.preprocess_input(a), b))\n","        valid_dataset = valid_dataset.batch(batch_size)\n","\n","    else:\n","        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \n","        print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n","        train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), tf.cast(y_train.reshape(-1),tf.int64)))\n","\n","        train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n","        train_dataset = train_dataset.map(my_preprocessing2)\n","        train_dataset = train_dataset.map(normalize)\n","\n","        valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(x_test/255., tf.float32), tf.cast(y_test.reshape(-1),tf.int64)))\n","\n","        valid_dataset = valid_dataset.batch(batch_size)\n","        valid_dataset = valid_dataset.map(normalize)\n","else:\n","    print('Data from  ImageDataGenerator')\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \n","    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n","\n","    train_datagen = ImageDataGenerator(rescale=1./255,\n","        rotation_range=15,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        horizontal_flip=True,\n","        preprocessing_function=None,\n","        )\n","    train_dataset = train_datagen.flow(x_train, y_train.reshape(-1), batch_size=batch_size,shuffle=True)\n","\n","    valid_datagen = ImageDataGenerator(rescale=1./255)\n","    valid_dataset = valid_datagen.flow(x_test, y_test.reshape(-1), batch_size=batch_size)    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dIV0BkxBJYW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNcgOvl7-Aoj"},"source":["initial_learning_rate = 0.1\n","lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n","                            monitor='val_accuracy',  # val loss를 기준으로...\n","                            factor=0.6,          # callback 호출시 lr = factor*lr\n","                            patience=10,         # patience epoch동안 monitor값이 좋아지지 않으면 callback 작동\n","                            cooldown=0,         # lr이 변경된 후, cooldonw동안에는 callback이 작동 안 한다.\n","                            min_lr=0.0001, \n","                            verbose = 1,min_delta=0.001)\n","\n","\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate,momentum=0.9)  # lr or learning_rate\n","\n","my_resnet.compile(optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n","model_filename = \"Epoch-{epoch:02d}-{val_accuracy:.4f}\"\n","checkpoint_path = os.path.join('models/', model_filename)\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,mode='auto',save_best_only=True,monitor='val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRm0a3ms7k2K","outputId":"2128b046-1211-4116-f4c8-b7e9e385f28e"},"source":["history = my_resnet.fit(train_dataset,epochs=200,validation_data=valid_dataset,validation_freq=1, verbose=1,callbacks=[cp_callback,lr_callback],initial_epoch=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","  2/391 [..............................] - ETA: 11s - loss: 7.0925 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.0389s). Check your callbacks.\n","391/391 [==============================] - ETA: 0s - loss: 2.5504 - accuracy: 0.2100\n","Epoch 00001: val_accuracy improved from -inf to 0.14340, saving model to models\\Epoch-01-0.1434\n","391/391 [==============================] - 25s 65ms/step - loss: 2.5504 - accuracy: 0.2100 - val_loss: 3.7660 - val_accuracy: 0.1434\n","Epoch 2/200\n","391/391 [==============================] - ETA: 0s - loss: 1.7183 - accuracy: 0.3554\n","Epoch 00002: val_accuracy improved from 0.14340 to 0.38670, saving model to models\\Epoch-02-0.3867\n","391/391 [==============================] - 24s 62ms/step - loss: 1.7183 - accuracy: 0.3554 - val_loss: 1.7686 - val_accuracy: 0.3867\n","Epoch 3/200\n","391/391 [==============================] - ETA: 0s - loss: 1.4984 - accuracy: 0.4488\n","Epoch 00003: val_accuracy improved from 0.38670 to 0.39650, saving model to models\\Epoch-03-0.3965\n","391/391 [==============================] - 24s 62ms/step - loss: 1.4984 - accuracy: 0.4488 - val_loss: 1.6068 - val_accuracy: 0.3965\n","Epoch 4/200\n","391/391 [==============================] - ETA: 0s - loss: 1.3241 - accuracy: 0.5166\n","Epoch 00004: val_accuracy improved from 0.39650 to 0.40640, saving model to models\\Epoch-04-0.4064\n","391/391 [==============================] - 24s 62ms/step - loss: 1.3241 - accuracy: 0.5166 - val_loss: 1.8377 - val_accuracy: 0.4064\n","Epoch 5/200\n","391/391 [==============================] - ETA: 0s - loss: 1.1726 - accuracy: 0.5771\n","Epoch 00005: val_accuracy improved from 0.40640 to 0.59640, saving model to models\\Epoch-05-0.5964\n","391/391 [==============================] - 24s 62ms/step - loss: 1.1726 - accuracy: 0.5771 - val_loss: 1.0934 - val_accuracy: 0.5964\n","Epoch 6/200\n","391/391 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.6281\n","Epoch 00006: val_accuracy improved from 0.59640 to 0.61620, saving model to models\\Epoch-06-0.6162\n","391/391 [==============================] - 24s 62ms/step - loss: 1.0437 - accuracy: 0.6281 - val_loss: 1.1070 - val_accuracy: 0.6162\n","Epoch 7/200\n","391/391 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.6624\n","Epoch 00007: val_accuracy improved from 0.61620 to 0.68220, saving model to models\\Epoch-07-0.6822\n","391/391 [==============================] - 24s 62ms/step - loss: 0.9537 - accuracy: 0.6624 - val_loss: 0.9078 - val_accuracy: 0.6822\n","Epoch 8/200\n","391/391 [==============================] - ETA: 0s - loss: 0.8678 - accuracy: 0.6939\n","Epoch 00008: val_accuracy did not improve from 0.68220\n","391/391 [==============================] - 24s 60ms/step - loss: 0.8678 - accuracy: 0.6939 - val_loss: 1.0205 - val_accuracy: 0.6764\n","Epoch 9/200\n","391/391 [==============================] - ETA: 0s - loss: 0.7817 - accuracy: 0.7230\n","Epoch 00009: val_accuracy did not improve from 0.68220\n","391/391 [==============================] - 24s 60ms/step - loss: 0.7817 - accuracy: 0.7230 - val_loss: 1.0056 - val_accuracy: 0.6608\n","Epoch 10/200\n","391/391 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.7521\n","Epoch 00010: val_accuracy improved from 0.68220 to 0.74190, saving model to models\\Epoch-10-0.7419\n","391/391 [==============================] - 25s 65ms/step - loss: 0.7068 - accuracy: 0.7521 - val_loss: 0.7392 - val_accuracy: 0.7419\n","Epoch 11/200\n","391/391 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.7741\n","Epoch 00011: val_accuracy did not improve from 0.74190\n","391/391 [==============================] - 24s 60ms/step - loss: 0.6496 - accuracy: 0.7741 - val_loss: 0.8552 - val_accuracy: 0.7201\n","Epoch 12/200\n","391/391 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.7885\n","Epoch 00012: val_accuracy improved from 0.74190 to 0.78190, saving model to models\\Epoch-12-0.7819\n","391/391 [==============================] - 24s 62ms/step - loss: 0.6046 - accuracy: 0.7885 - val_loss: 0.6675 - val_accuracy: 0.7819\n","Epoch 13/200\n","391/391 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8051\n","Epoch 00013: val_accuracy did not improve from 0.78190\n","391/391 [==============================] - 24s 60ms/step - loss: 0.5619 - accuracy: 0.8051 - val_loss: 0.6504 - val_accuracy: 0.7810\n","Epoch 14/200\n","391/391 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.8177\n","Epoch 00014: val_accuracy improved from 0.78190 to 0.80910, saving model to models\\Epoch-14-0.8091\n","391/391 [==============================] - 24s 62ms/step - loss: 0.5245 - accuracy: 0.8177 - val_loss: 0.5601 - val_accuracy: 0.8091\n","Epoch 15/200\n","391/391 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.8279\n","Epoch 00015: val_accuracy did not improve from 0.80910\n","391/391 [==============================] - 24s 60ms/step - loss: 0.4965 - accuracy: 0.8279 - val_loss: 0.6008 - val_accuracy: 0.8001\n","Epoch 16/200\n","391/391 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.8373\n","Epoch 00016: val_accuracy improved from 0.80910 to 0.81790, saving model to models\\Epoch-16-0.8179\n","391/391 [==============================] - 25s 64ms/step - loss: 0.4643 - accuracy: 0.8373 - val_loss: 0.5417 - val_accuracy: 0.8179\n","Epoch 17/200\n","391/391 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8476\n","Epoch 00017: val_accuracy did not improve from 0.81790\n","391/391 [==============================] - 24s 60ms/step - loss: 0.4392 - accuracy: 0.8476 - val_loss: 0.6018 - val_accuracy: 0.8040\n","Epoch 18/200\n","391/391 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.8559\n","Epoch 00018: val_accuracy did not improve from 0.81790\n","391/391 [==============================] - 24s 60ms/step - loss: 0.4151 - accuracy: 0.8559 - val_loss: 0.5965 - val_accuracy: 0.8090\n","Epoch 19/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8634\n","Epoch 00019: val_accuracy did not improve from 0.81790\n","391/391 [==============================] - 24s 60ms/step - loss: 0.3927 - accuracy: 0.8634 - val_loss: 0.5839 - val_accuracy: 0.8101\n","Epoch 20/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.8667\n","Epoch 00020: val_accuracy did not improve from 0.81790\n","391/391 [==============================] - 24s 60ms/step - loss: 0.3809 - accuracy: 0.8667 - val_loss: 0.5526 - val_accuracy: 0.8163\n","Epoch 21/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8753\n","Epoch 00021: val_accuracy did not improve from 0.81790\n","391/391 [==============================] - 24s 60ms/step - loss: 0.3562 - accuracy: 0.8753 - val_loss: 0.6021 - val_accuracy: 0.8061\n","Epoch 22/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8799\n","Epoch 00022: val_accuracy improved from 0.81790 to 0.85050, saving model to models\\Epoch-22-0.8505\n","391/391 [==============================] - 25s 63ms/step - loss: 0.3402 - accuracy: 0.8799 - val_loss: 0.4566 - val_accuracy: 0.8505\n","Epoch 23/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8879\n","Epoch 00023: val_accuracy did not improve from 0.85050\n","391/391 [==============================] - 24s 60ms/step - loss: 0.3208 - accuracy: 0.8879 - val_loss: 0.5268 - val_accuracy: 0.8318\n","Epoch 24/200\n","391/391 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8916\n","Epoch 00024: val_accuracy did not improve from 0.85050\n","391/391 [==============================] - 24s 60ms/step - loss: 0.3097 - accuracy: 0.8916 - val_loss: 0.5337 - val_accuracy: 0.8297\n","Epoch 25/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.8976\n","Epoch 00025: val_accuracy did not improve from 0.85050\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2960 - accuracy: 0.8976 - val_loss: 0.4859 - val_accuracy: 0.8428\n","Epoch 26/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9002\n","Epoch 00026: val_accuracy did not improve from 0.85050\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2839 - accuracy: 0.9002 - val_loss: 0.4910 - val_accuracy: 0.8444\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 27/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9049\n","Epoch 00027: val_accuracy improved from 0.85050 to 0.86520, saving model to models\\Epoch-27-0.8652\n","391/391 [==============================] - 25s 63ms/step - loss: 0.2702 - accuracy: 0.9049 - val_loss: 0.4306 - val_accuracy: 0.8652\n","Epoch 28/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9073\n","Epoch 00028: val_accuracy did not improve from 0.86520\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2600 - accuracy: 0.9073 - val_loss: 0.5466 - val_accuracy: 0.8369\n","Epoch 29/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9138\n","Epoch 00029: val_accuracy did not improve from 0.86520\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2446 - accuracy: 0.9138 - val_loss: 0.6711 - val_accuracy: 0.8206\n","Epoch 30/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9154\n","Epoch 00030: val_accuracy improved from 0.86520 to 0.87460, saving model to models\\Epoch-30-0.8746\n","391/391 [==============================] - 25s 65ms/step - loss: 0.2391 - accuracy: 0.9154 - val_loss: 0.4025 - val_accuracy: 0.8746\n","Epoch 31/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9178\n","Epoch 00031: val_accuracy did not improve from 0.87460\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2307 - accuracy: 0.9178 - val_loss: 0.6729 - val_accuracy: 0.8205\n","Epoch 32/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9211\n","Epoch 00032: val_accuracy did not improve from 0.87460\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2229 - accuracy: 0.9211 - val_loss: 0.5017 - val_accuracy: 0.8546\n","Epoch 33/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9252\n","Epoch 00033: val_accuracy did not improve from 0.87460\n","391/391 [==============================] - 24s 60ms/step - loss: 0.2095 - accuracy: 0.9252 - val_loss: 0.4799 - val_accuracy: 0.8493\n","Epoch 34/200\n","391/391 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9277\n","Epoch 00034: val_accuracy improved from 0.87460 to 0.87620, saving model to models\\Epoch-34-0.8762\n","391/391 [==============================] - 25s 65ms/step - loss: 0.2029 - accuracy: 0.9277 - val_loss: 0.4099 - val_accuracy: 0.8762\n","Epoch 35/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9293\n","Epoch 00035: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 60ms/step - loss: 0.1961 - accuracy: 0.9293 - val_loss: 0.4142 - val_accuracy: 0.8722\n","Epoch 36/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9330\n","Epoch 00036: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 60ms/step - loss: 0.1875 - accuracy: 0.9330 - val_loss: 0.4747 - val_accuracy: 0.8586\n","Epoch 37/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9369\n","Epoch 00037: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 60ms/step - loss: 0.1786 - accuracy: 0.9369 - val_loss: 0.6511 - val_accuracy: 0.8365\n","Epoch 38/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9410\n","Epoch 00038: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 60ms/step - loss: 0.1663 - accuracy: 0.9410 - val_loss: 0.4234 - val_accuracy: 0.8720\n","Epoch 39/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9418\n","Epoch 00039: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 60ms/step - loss: 0.1643 - accuracy: 0.9418 - val_loss: 0.5333 - val_accuracy: 0.8595\n","Epoch 40/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9438\n","Epoch 00040: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1587 - accuracy: 0.9438 - val_loss: 0.5194 - val_accuracy: 0.8602\n","Epoch 41/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9471\n","Epoch 00041: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1459 - accuracy: 0.9471 - val_loss: 0.4828 - val_accuracy: 0.8687\n","Epoch 42/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9481\n","Epoch 00042: val_accuracy did not improve from 0.87620\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1446 - accuracy: 0.9481 - val_loss: 0.4735 - val_accuracy: 0.8733\n","Epoch 43/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9490\n","Epoch 00043: val_accuracy improved from 0.87620 to 0.88240, saving model to models\\Epoch-43-0.8824\n","391/391 [==============================] - 26s 65ms/step - loss: 0.1420 - accuracy: 0.9490 - val_loss: 0.4077 - val_accuracy: 0.8824\n","Epoch 44/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9499\n","Epoch 00044: val_accuracy did not improve from 0.88240\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1393 - accuracy: 0.9499 - val_loss: 0.4550 - val_accuracy: 0.8697\n","Epoch 45/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9538\n","Epoch 00045: val_accuracy improved from 0.88240 to 0.88640, saving model to models\\Epoch-45-0.8864\n","391/391 [==============================] - 25s 63ms/step - loss: 0.1293 - accuracy: 0.9538 - val_loss: 0.4230 - val_accuracy: 0.8864\n","Epoch 46/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9560\n","Epoch 00046: val_accuracy did not improve from 0.88640\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1244 - accuracy: 0.9560 - val_loss: 0.4979 - val_accuracy: 0.8687\n","Epoch 47/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9578\n","Epoch 00047: val_accuracy did not improve from 0.88640\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1187 - accuracy: 0.9578 - val_loss: 0.5353 - val_accuracy: 0.8696\n","Epoch 48/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9590\n","Epoch 00048: val_accuracy did not improve from 0.88640\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1145 - accuracy: 0.9590 - val_loss: 0.5407 - val_accuracy: 0.8695\n","Epoch 49/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9583\n","Epoch 00049: val_accuracy did not improve from 0.88640\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1143 - accuracy: 0.9583 - val_loss: 0.5281 - val_accuracy: 0.8726\n","Epoch 50/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9614\n","Epoch 00050: val_accuracy did not improve from 0.88640\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1105 - accuracy: 0.9614 - val_loss: 0.6622 - val_accuracy: 0.8463\n","Epoch 51/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9634\n","Epoch 00051: val_accuracy improved from 0.88640 to 0.89120, saving model to models\\Epoch-51-0.8912\n","391/391 [==============================] - 25s 64ms/step - loss: 0.1031 - accuracy: 0.9634 - val_loss: 0.4316 - val_accuracy: 0.8912\n","Epoch 52/200\n","391/391 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9646\n","Epoch 00052: val_accuracy did not improve from 0.89120\n","391/391 [==============================] - 24s 61ms/step - loss: 0.1007 - accuracy: 0.9646 - val_loss: 0.5149 - val_accuracy: 0.8769\n","Epoch 53/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9649\n","Epoch 00053: val_accuracy did not improve from 0.89120\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0993 - accuracy: 0.9649 - val_loss: 0.4977 - val_accuracy: 0.8800\n","Epoch 54/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9660\n","Epoch 00054: val_accuracy improved from 0.89120 to 0.89370, saving model to models\\Epoch-54-0.8937\n","391/391 [==============================] - 26s 66ms/step - loss: 0.0963 - accuracy: 0.9660 - val_loss: 0.4328 - val_accuracy: 0.8937\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 55/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9671\n","Epoch 00055: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0923 - accuracy: 0.9671 - val_loss: 0.5577 - val_accuracy: 0.8776\n","Epoch 56/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9692\n","Epoch 00056: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 0.5164 - val_accuracy: 0.8736\n","Epoch 57/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9690\n","Epoch 00057: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0859 - accuracy: 0.9690 - val_loss: 0.4588 - val_accuracy: 0.8888\n","Epoch 58/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9735\n","Epoch 00058: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0776 - accuracy: 0.9735 - val_loss: 0.4778 - val_accuracy: 0.8869\n","Epoch 59/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9727\n","Epoch 00059: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0772 - accuracy: 0.9727 - val_loss: 0.4573 - val_accuracy: 0.8873\n","Epoch 60/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9723\n","Epoch 00060: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0772 - accuracy: 0.9723 - val_loss: 0.4984 - val_accuracy: 0.8849\n","Epoch 61/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9741\n","Epoch 00061: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0764 - accuracy: 0.9741 - val_loss: 0.4542 - val_accuracy: 0.8931\n","Epoch 62/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9735\n","Epoch 00062: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0750 - accuracy: 0.9735 - val_loss: 0.6059 - val_accuracy: 0.8695\n","Epoch 63/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9741\n","Epoch 00063: val_accuracy did not improve from 0.89370\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0741 - accuracy: 0.9741 - val_loss: 0.4822 - val_accuracy: 0.8847\n","Epoch 64/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9772\n","Epoch 00064: val_accuracy did not improve from 0.89370\n","\n","Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.06000000089406967.\n","391/391 [==============================] - 24s 61ms/step - loss: 0.0657 - accuracy: 0.9772 - val_loss: 0.5332 - val_accuracy: 0.8831\n","Epoch 65/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9842\n","Epoch 00065: val_accuracy improved from 0.89370 to 0.90190, saving model to models\\Epoch-65-0.9019\n","391/391 [==============================] - 25s 64ms/step - loss: 0.0445 - accuracy: 0.9842 - val_loss: 0.4647 - val_accuracy: 0.9019\n","Epoch 66/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9875\n","Epoch 00066: val_accuracy did not improve from 0.90190\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 0.5002 - val_accuracy: 0.8995\n","Epoch 67/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9876\n","Epoch 00067: val_accuracy did not improve from 0.90190\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0347 - accuracy: 0.9876 - val_loss: 0.5190 - val_accuracy: 0.8967\n","Epoch 68/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9885\n","Epoch 00068: val_accuracy did not improve from 0.90190\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0329 - accuracy: 0.9885 - val_loss: 0.4932 - val_accuracy: 0.8985\n","Epoch 69/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9887\n","Epoch 00069: val_accuracy improved from 0.90190 to 0.90220, saving model to models\\Epoch-69-0.9022\n","391/391 [==============================] - 25s 65ms/step - loss: 0.0319 - accuracy: 0.9887 - val_loss: 0.5141 - val_accuracy: 0.9022\n","Epoch 70/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9894\n","Epoch 00070: val_accuracy did not improve from 0.90220\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0307 - accuracy: 0.9894 - val_loss: 0.5530 - val_accuracy: 0.8987\n","Epoch 71/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9893\n","Epoch 00071: val_accuracy improved from 0.90220 to 0.90330, saving model to models\\Epoch-71-0.9033\n","391/391 [==============================] - 24s 62ms/step - loss: 0.0304 - accuracy: 0.9893 - val_loss: 0.4971 - val_accuracy: 0.9033\n","Epoch 72/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9899\n","Epoch 00072: val_accuracy did not improve from 0.90330\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0290 - accuracy: 0.9899 - val_loss: 0.5981 - val_accuracy: 0.8917\n","Epoch 73/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9902\n","Epoch 00073: val_accuracy did not improve from 0.90330\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0281 - accuracy: 0.9902 - val_loss: 0.5369 - val_accuracy: 0.9003\n","Epoch 74/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9901\n","Epoch 00074: val_accuracy improved from 0.90330 to 0.90430, saving model to models\\Epoch-74-0.9043\n","391/391 [==============================] - 25s 65ms/step - loss: 0.0284 - accuracy: 0.9901 - val_loss: 0.4994 - val_accuracy: 0.9043\n","Epoch 75/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9906\n","Epoch 00075: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.5489 - val_accuracy: 0.8968\n","Epoch 76/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9900\n","Epoch 00076: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0268 - accuracy: 0.9900 - val_loss: 0.5774 - val_accuracy: 0.8956\n","Epoch 77/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9911\n","Epoch 00077: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.5253 - val_accuracy: 0.9029\n","Epoch 78/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9913\n","Epoch 00078: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 0.4991 - val_accuracy: 0.9037\n","Epoch 79/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9911\n","Epoch 00079: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0252 - accuracy: 0.9911 - val_loss: 0.5878 - val_accuracy: 0.8938\n","Epoch 80/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9915\n","Epoch 00080: val_accuracy did not improve from 0.90430\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.6006 - val_accuracy: 0.8920\n","Epoch 81/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9924\n","Epoch 00081: val_accuracy did not improve from 0.90430\n","\n","Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.03600000143051147.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.5952 - val_accuracy: 0.8989\n","Epoch 82/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9941\n","Epoch 00082: val_accuracy improved from 0.90430 to 0.90920, saving model to models\\Epoch-82-0.9092\n"],"name":"stdout"},{"output_type":"stream","text":["391/391 [==============================] - 25s 63ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 0.4817 - val_accuracy: 0.9092\n","Epoch 83/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9944\n","Epoch 00083: val_accuracy did not improve from 0.90920\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.5430 - val_accuracy: 0.9044\n","Epoch 84/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n","Epoch 00084: val_accuracy improved from 0.90920 to 0.91090, saving model to models\\Epoch-84-0.9109\n","391/391 [==============================] - 25s 65ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.5007 - val_accuracy: 0.9109\n","Epoch 85/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9953\n","Epoch 00085: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 0.5702 - val_accuracy: 0.9013\n","Epoch 86/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n","Epoch 00086: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.5239 - val_accuracy: 0.9062\n","Epoch 87/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957\n","Epoch 00087: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.5326 - val_accuracy: 0.9081\n","Epoch 88/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9958\n","Epoch 00088: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.5360 - val_accuracy: 0.9058\n","Epoch 89/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9961\n","Epoch 00089: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.5496 - val_accuracy: 0.9086\n","Epoch 90/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9968\n","Epoch 00090: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.5481 - val_accuracy: 0.9079\n","Epoch 91/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9959\n","Epoch 00091: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.5448 - val_accuracy: 0.9077\n","Epoch 92/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9962\n","Epoch 00092: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.5631 - val_accuracy: 0.9077\n","Epoch 93/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9966\n","Epoch 00093: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.5653 - val_accuracy: 0.9051\n","Epoch 94/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9968\n","Epoch 00094: val_accuracy did not improve from 0.91090\n","\n","Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.02160000130534172.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.5982 - val_accuracy: 0.9060\n","Epoch 95/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9971\n","Epoch 00095: val_accuracy did not improve from 0.91090\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.5639 - val_accuracy: 0.9097\n","Epoch 96/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976\n","Epoch 00096: val_accuracy improved from 0.91090 to 0.91130, saving model to models\\Epoch-96-0.9113\n","391/391 [==============================] - 25s 63ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.5577 - val_accuracy: 0.9113\n","Epoch 97/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9973\n","Epoch 00097: val_accuracy did not improve from 0.91130\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.5519 - val_accuracy: 0.9105\n","Epoch 98/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9979\n","Epoch 00098: val_accuracy improved from 0.91130 to 0.91170, saving model to models\\Epoch-98-0.9117\n","391/391 [==============================] - 25s 65ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.5718 - val_accuracy: 0.9117\n","Epoch 99/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9973\n","Epoch 00099: val_accuracy did not improve from 0.91170\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 0.5730 - val_accuracy: 0.9099\n","Epoch 100/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9979\n","Epoch 00100: val_accuracy did not improve from 0.91170\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.5666 - val_accuracy: 0.9100\n","Epoch 101/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9978\n","Epoch 00101: val_accuracy did not improve from 0.91170\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.5729 - val_accuracy: 0.9105\n","Epoch 102/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9978\n","Epoch 00102: val_accuracy did not improve from 0.91170\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.5821 - val_accuracy: 0.9110\n","Epoch 103/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9980\n","Epoch 00103: val_accuracy improved from 0.91170 to 0.91200, saving model to models\\Epoch-103-0.9120\n","391/391 [==============================] - 25s 64ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.5482 - val_accuracy: 0.9120\n","Epoch 104/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n","Epoch 00104: val_accuracy improved from 0.91200 to 0.91260, saving model to models\\Epoch-104-0.9126\n","391/391 [==============================] - 24s 62ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.5730 - val_accuracy: 0.9126\n","Epoch 105/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9982\n","Epoch 00105: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.5682 - val_accuracy: 0.9107\n","Epoch 106/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9983\n","Epoch 00106: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.5680 - val_accuracy: 0.9106\n","Epoch 107/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9977\n","Epoch 00107: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.5932 - val_accuracy: 0.9094\n","Epoch 108/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n","Epoch 00108: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.5909 - val_accuracy: 0.9101\n","Epoch 109/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9979\n","Epoch 00109: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0056 - accuracy: 0.9979 - val_loss: 0.5999 - val_accuracy: 0.9110\n","Epoch 110/200\n"],"name":"stdout"},{"output_type":"stream","text":["391/391 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9983\n","Epoch 00110: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.5950 - val_accuracy: 0.9109\n","Epoch 111/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9982\n","Epoch 00111: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.5830 - val_accuracy: 0.9108\n","Epoch 112/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9983\n","Epoch 00112: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.5747 - val_accuracy: 0.9098\n","Epoch 113/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9984\n","Epoch 00113: val_accuracy did not improve from 0.91260\n","\n","Epoch 00113: ReduceLROnPlateau reducing learning rate to 0.012960000336170197.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.5645 - val_accuracy: 0.9112\n","Epoch 114/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9987\n","Epoch 00114: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.5764 - val_accuracy: 0.9116\n","Epoch 115/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9986\n","Epoch 00115: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.5836 - val_accuracy: 0.9118\n","Epoch 116/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9986\n","Epoch 00116: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.5724 - val_accuracy: 0.9124\n","Epoch 117/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9986\n","Epoch 00117: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.5706 - val_accuracy: 0.9116\n","Epoch 118/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9986\n","Epoch 00118: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.5785 - val_accuracy: 0.9109\n","Epoch 119/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9986\n","Epoch 00119: val_accuracy did not improve from 0.91260\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.5831 - val_accuracy: 0.9124\n","Epoch 120/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9988\n","Epoch 00120: val_accuracy improved from 0.91260 to 0.91500, saving model to models\\Epoch-120-0.9150\n","391/391 [==============================] - 25s 64ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.5749 - val_accuracy: 0.9150\n","Epoch 121/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9986\n","Epoch 00121: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.5852 - val_accuracy: 0.9137\n","Epoch 122/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n","Epoch 00122: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.5843 - val_accuracy: 0.9132\n","Epoch 123/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9987\n","Epoch 00123: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.5787 - val_accuracy: 0.9137\n","Epoch 124/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n","Epoch 00124: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.5803 - val_accuracy: 0.9146\n","Epoch 125/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9989\n","Epoch 00125: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.5812 - val_accuracy: 0.9125\n","Epoch 126/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\n","Epoch 00126: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.5839 - val_accuracy: 0.9133\n","Epoch 127/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n","Epoch 00127: val_accuracy did not improve from 0.91500\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.5716 - val_accuracy: 0.9131\n","Epoch 128/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9989\n","Epoch 00128: val_accuracy improved from 0.91500 to 0.91540, saving model to models\\Epoch-128-0.9154\n","391/391 [==============================] - 25s 64ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.5746 - val_accuracy: 0.9154\n","Epoch 129/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9989\n","Epoch 00129: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.5970 - val_accuracy: 0.9109\n","Epoch 130/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9987\n","Epoch 00130: val_accuracy did not improve from 0.91540\n","\n","Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.0077759999781846995.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.5939 - val_accuracy: 0.9126\n","Epoch 131/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00131: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5837 - val_accuracy: 0.9149\n","Epoch 132/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\n","Epoch 00132: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.5848 - val_accuracy: 0.9141\n","Epoch 133/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9989\n","Epoch 00133: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.5877 - val_accuracy: 0.9139\n","Epoch 134/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9992\n","Epoch 00134: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.5775 - val_accuracy: 0.9133\n","Epoch 135/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n","Epoch 00135: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.5750 - val_accuracy: 0.9129\n","Epoch 136/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9990\n","Epoch 00136: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.5698 - val_accuracy: 0.9146\n","Epoch 137/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n","Epoch 00137: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.5728 - val_accuracy: 0.9154\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 138/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n","Epoch 00138: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.5819 - val_accuracy: 0.9138\n","Epoch 139/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n","Epoch 00139: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.5784 - val_accuracy: 0.9137\n","Epoch 140/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n","Epoch 00140: val_accuracy did not improve from 0.91540\n","\n","Epoch 00140: ReduceLROnPlateau reducing learning rate to 0.004665600042790174.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.5752 - val_accuracy: 0.9151\n","Epoch 141/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n","Epoch 00141: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.5781 - val_accuracy: 0.9141\n","Epoch 142/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9990\n","Epoch 00142: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.5845 - val_accuracy: 0.9140\n","Epoch 143/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00143: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5871 - val_accuracy: 0.9142\n","Epoch 144/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n","Epoch 00144: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.5911 - val_accuracy: 0.9135\n","Epoch 145/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9990\n","Epoch 00145: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.5788 - val_accuracy: 0.9142\n","Epoch 146/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9992\n","Epoch 00146: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.5873 - val_accuracy: 0.9146\n","Epoch 147/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n","Epoch 00147: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.5859 - val_accuracy: 0.9139\n","Epoch 148/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9991\n","Epoch 00148: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.5938 - val_accuracy: 0.9133\n","Epoch 149/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9993\n","Epoch 00149: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.5849 - val_accuracy: 0.9144\n","Epoch 150/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n","Epoch 00150: val_accuracy did not improve from 0.91540\n","\n","Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.002799360081553459.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.5853 - val_accuracy: 0.9146\n","Epoch 151/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9991\n","Epoch 00151: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.5857 - val_accuracy: 0.9148\n","Epoch 152/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n","Epoch 00152: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.5891 - val_accuracy: 0.9142\n","Epoch 153/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\n","Epoch 00153: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.5870 - val_accuracy: 0.9143\n","Epoch 154/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\n","Epoch 00154: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.5907 - val_accuracy: 0.9136\n","Epoch 155/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n","Epoch 00155: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.5859 - val_accuracy: 0.9144\n","Epoch 156/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n","Epoch 00156: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.5903 - val_accuracy: 0.9139\n","Epoch 157/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9990\n","Epoch 00157: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.5896 - val_accuracy: 0.9135\n","Epoch 158/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n","Epoch 00158: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.5925 - val_accuracy: 0.9140\n","Epoch 159/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00159: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5899 - val_accuracy: 0.9148\n","Epoch 160/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9991\n","Epoch 00160: val_accuracy did not improve from 0.91540\n","\n","Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0016796160489320755.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.5905 - val_accuracy: 0.9142\n","Epoch 161/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\n","Epoch 00161: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5884 - val_accuracy: 0.9138\n","Epoch 162/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9990\n","Epoch 00162: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.5888 - val_accuracy: 0.9144\n","Epoch 163/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n","Epoch 00163: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.5857 - val_accuracy: 0.9144\n","Epoch 164/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n","Epoch 00164: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.5882 - val_accuracy: 0.9140\n","Epoch 165/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00165: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5897 - val_accuracy: 0.9144\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 166/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\n","Epoch 00166: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.5890 - val_accuracy: 0.9143\n","Epoch 167/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n","Epoch 00167: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.5898 - val_accuracy: 0.9145\n","Epoch 168/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9992\n","Epoch 00168: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.5892 - val_accuracy: 0.9144\n","Epoch 169/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n","Epoch 00169: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.5931 - val_accuracy: 0.9139\n","Epoch 170/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n","Epoch 00170: val_accuracy did not improve from 0.91540\n","\n","Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.0010077696293592452.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.5933 - val_accuracy: 0.9146\n","Epoch 171/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995\n","Epoch 00171: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.5916 - val_accuracy: 0.9143\n","Epoch 172/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9992\n","Epoch 00172: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.5943 - val_accuracy: 0.9142\n","Epoch 173/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9992\n","Epoch 00173: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.5897 - val_accuracy: 0.9141\n","Epoch 174/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n","Epoch 00174: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.5919 - val_accuracy: 0.9143\n","Epoch 175/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\n","Epoch 00175: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.5937 - val_accuracy: 0.9140\n","Epoch 176/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\n","Epoch 00176: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.5928 - val_accuracy: 0.9143\n","Epoch 177/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\n","Epoch 00177: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.5899 - val_accuracy: 0.9146\n","Epoch 178/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n","Epoch 00178: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.5919 - val_accuracy: 0.9143\n","Epoch 179/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\n","Epoch 00179: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.5912 - val_accuracy: 0.9142\n","Epoch 180/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9992\n","Epoch 00180: val_accuracy did not improve from 0.91540\n","\n","Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.0006046617636457086.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.5936 - val_accuracy: 0.9137\n","Epoch 181/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n","Epoch 00181: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.5934 - val_accuracy: 0.9137\n","Epoch 182/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9992\n","Epoch 00182: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.5916 - val_accuracy: 0.9140\n","Epoch 183/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\n","Epoch 00183: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.5901 - val_accuracy: 0.9143\n","Epoch 184/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9992\n","Epoch 00184: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.5899 - val_accuracy: 0.9147\n","Epoch 185/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n","Epoch 00185: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.5889 - val_accuracy: 0.9142\n","Epoch 186/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n","Epoch 00186: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.5917 - val_accuracy: 0.9143\n","Epoch 187/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9994\n","Epoch 00187: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.5891 - val_accuracy: 0.9144\n","Epoch 188/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9992\n","Epoch 00188: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.5918 - val_accuracy: 0.9145\n","Epoch 189/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00189: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5892 - val_accuracy: 0.9142\n","Epoch 190/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n","Epoch 00190: val_accuracy did not improve from 0.91540\n","\n","Epoch 00190: ReduceLROnPlateau reducing learning rate to 0.00036279705818742514.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.5918 - val_accuracy: 0.9143\n","Epoch 191/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n","Epoch 00191: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.5915 - val_accuracy: 0.9144\n","Epoch 192/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00192: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5894 - val_accuracy: 0.9144\n","Epoch 193/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n","Epoch 00193: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.5891 - val_accuracy: 0.9143\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 194/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n","Epoch 00194: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.5918 - val_accuracy: 0.9140\n","Epoch 195/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993\n","Epoch 00195: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.5905 - val_accuracy: 0.9142\n","Epoch 196/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00196: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5950 - val_accuracy: 0.9139\n","Epoch 197/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n","Epoch 00197: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.5910 - val_accuracy: 0.9141\n","Epoch 198/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n","Epoch 00198: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.5914 - val_accuracy: 0.9143\n","Epoch 199/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n","Epoch 00199: val_accuracy did not improve from 0.91540\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.5904 - val_accuracy: 0.9137\n","Epoch 200/200\n","391/391 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n","Epoch 00200: val_accuracy did not improve from 0.91540\n","\n","Epoch 00200: ReduceLROnPlateau reducing learning rate to 0.0002176782349124551.\n","391/391 [==============================] - 24s 60ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.5921 - val_accuracy: 0.9139\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GW_GR-Ey_AgH","outputId":"20ca890f-0e61-42dc-fdcc-eb2395497640"},"source":["plot_history(history)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"fI_5INQR-BPG"},"source":["## 3. Tensorflow 중간규모 Model\n","- https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n","- 여기 keras코드를 tensorflow코드로 재구성\n","- ImageDataGenerator로 data augmentation\n","\n","- lr=0.001로 시작해서,213 epoch 이후, val acc가 더 이상 좋아지지 않아(88.59%), lr=0.0001로 train 이어가기\n","- 288 epoch에 90.53%"]},{"cell_type":"code","metadata":{"id":"7N15JZWj-Le8"},"source":["import tensorflow as tf\n","from tensorflow.keras.applications import resnet\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_c3n38x-fIt"},"source":["class MyModel(tf.keras.Model):\n","    def __init__(self,input_shape):\n","        super(MyModel, self).__init__()\n","\n","        weight_decay = 1e-4\n","\n","        self.model = tf.keras.models.Sequential()\n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=input_shape))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())\n","        \n","        self.model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())        \n","        \n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n","        self.model.add(tf.keras.layers.Dropout(0.2))      \n","\n","\n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())\n","        \n","        self.model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())        \n","        \n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n","        self.model.add(tf.keras.layers.Dropout(0.3))    \n","\n","\n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())\n","        \n","        self.model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.ELU())        \n","        \n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n","        self.model.add(tf.keras.layers.Dropout(0.4))   \n","\n","        self.model.add(tf.keras.layers.Flatten())\n","\n","\n","        self.built = True  # summary가 제대로 작동한다.\n","\n","    def call(self,x,training=None):  # training의 default 값으로 None이 좋다(Ture/False보다)\n","        output = self.model(x)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJv42Qmu-p-E","executionInfo":{"elapsed":1304,"status":"ok","timestamp":1612571740880,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"6949e0b6-1f95-4b2b-b95c-df50b443c83e"},"source":["batch_size = 128\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  # numpy array \n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)  # (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n","\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","    rotation_range=15,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    horizontal_flip=True,\n","    )\n","train_dataset = train_datagen.flow(x_train, y_train.reshape(-1), batch_size=batch_size,shuffle=True)\n","\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n","valid_dataset = valid_datagen.flow(x_test, y_test.reshape(-1), batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dsOcVfpZNdVD"},"source":["# base_model = MyModel(input_shape=(32,32,3))\n","# model = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])\n","\n","base_model = ResNetX18()\n","model = tf.keras.Sequential([ base_model,tf.keras.layers.Dense(10)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLaF-6uz-smN"},"source":["initial_learning_rate = 0.1\n","lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n","                            monitor='val_accuracy',  # val loss를 기준으로...\n","                            factor=0.6,          # callback 호출시 lr = factor*lr\n","                            patience=10,         # patience epoch동안 monitor값이 좋아지지 않으면 callback 작동\n","                            cooldown=0,         # lr이 변경된 후, cooldonw동안에는 callback이 작동 안 한다.\n","                            min_lr=0.0001, \n","                            verbose = 1,min_delta=0.001)\n","es_callback = tf.keras.callbacks.EarlyStopping(\n","                            monitor=\"val_accuracy\",\n","                            min_delta=0.003,  # 이 값 이상으로 변화되어야 향상으로 인정.\n","                            patience=40,\n","                            verbose=1,\n","                            mode=\"auto\",\n","                            baseline=None,\n","                            restore_best_weights=True,  # stop될때 마지막이 아닌, best weight로 복원\n","                        )\n","\n","\n","\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, clipnorm=0.5)  # lr or learning_rate\n","\n","model.compile(optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n","model_filename = \"Epoch-{epoch:02d}-{val_accuracy:.4f}\"\n","checkpoint_path = os.path.join('models/', model_filename)\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,mode='auto',save_best_only=True,monitor='val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"N-X4GpPo-2fm","executionInfo":{"elapsed":56233,"status":"error","timestamp":1612571823769,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"930e504c-b41d-452c-b43b-add5a65f69d2"},"source":["history = model.fit(train_dataset,epochs=400,steps_per_epoch=len(x_train)/batch_size,validation_data=valid_dataset,validation_freq=1, verbose=1,callbacks=[cp_callback,lr_callback],initial_epoch=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/400\n","390/390 [==============================] - 48s 114ms/step - loss: 3.7592 - accuracy: 0.1604 - val_loss: 2.4506 - val_accuracy: 0.1491\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.14910, saving model to models/Epoch-01-0.1491\n","Epoch 2/400\n"," 65/390 [===>..........................] - ETA: 35s - loss: 1.8685 - accuracy: 0.2622"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-c548ee05c826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"tS0lKXTEFUtI"},"source":["## 4. pytorch로된 kuangliu의 코드를 tesorflow 코드로 동일하게 변환"]},{"cell_type":"code","metadata":{"id":"xID6cvIjGCRy"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from functools import partial\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAcsIuYaFVOG"},"source":["class BasicBlockX(tf.keras.Model): # 논문의 resnet구조로 다르다.\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlockX, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(planes,kernel_size=3, strides=stride, padding=\"SAME\", use_bias=False)\n","        \n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.conv2 = tf.keras.layers.Conv2D(planes,kernel_size=3, strides=1, padding=\"SAME\", use_bias=False)\n","        self.bn2 = tf.keras.layers.BatchNormalization()\n","\n","        self.shortcut = tf.keras.models.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut.add(tf.keras.layers.Conv2D(self.expansion*planes,kernel_size=1, strides=stride, padding=\"SAME\", use_bias=False))\n","            self.shortcut.add(tf.keras.layers.BatchNormalization())\n","\n","    def call(self, x, training=None):\n","        out = tf.nn.relu(self.bn1(self.conv1(x),training))\n","        out = self.bn2(self.conv2(out),training)\n","        out += self.shortcut(x,training)\n","        out = tf.nn.relu(out)\n","        return out\n","\n","\n","class ResNetX(tf.keras.Model):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNetX, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = tf.keras.layers.Conv2D(64,kernel_size=3, strides=1, padding=\"SAME\", use_bias=False) \n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        \n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        \n","        self.flatten = tf.keras.layers.Flatten()\n","        self.linear = tf.keras.layers.Dense(units=num_classes)\n","        \n","        self.built = True\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return tf.keras.models.Sequential(layers)\n","    def build(self,input_shape):\n","        #super(ResNetX, self).build(input_shape)\n","        super().build(input_shape)\n","    \n","    def call(self, x,training=None):\n","        out = tf.nn.relu(self.bn1(self.conv1(x),training))\n","        out = self.layer1(out,training)\n","        out = self.layer2(out,training)\n","        out = self.layer3(out,training)\n","        out = self.layer4(out,training)\n","        out = tf.nn.avg_pool2d(out, ksize=4,strides=4,padding='VALID')\n","        out = self.flatten(out)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNetX18():\n","    return ResNetX(BasicBlockX, [2, 2, 2, 2])\n","def ResNet34():\n","    return ResNetX(BasicBlockX, [3, 4, 6, 3])\n","def ResNet50():\n","    return ResNetX(BottleneckX, [3, 4, 6, 3])\n","def ResNet101():\n","    return ResNetX(BottleneckX, [3, 4, 23, 3])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxoYWVR_FVSs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlGO3zumFVWv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsW-8m6OFVaF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OX4B1MhFVeL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBlPha-tP5W_"},"source":["## TEST code"]},{"cell_type":"code","metadata":{"id":"t0RjFfG0EJey"},"source":["#! rm models/Epoch-4*\n","! rm -r models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CR8iiu0nRIFe","executionInfo":{"elapsed":1059,"status":"ok","timestamp":1612444866115,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"686c1df0-0a30-4af0-8537-316fbcd60445"},"source":["my_resnet"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8aITfETNBHz","executionInfo":{"elapsed":1243,"status":"ok","timestamp":1612445847613,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"c7791973-d137-4091-d4e1-5bfaab41ed84"},"source":["net = resnet_18()\n","net.build(input_shape=(None,32,32,3))\n","net.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"res_net_type_i_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_100 (Conv2D)          multiple                  9408      \n","_________________________________________________________________\n","batch_normalization_100 (Bat multiple                  256       \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 multiple                  0         \n","_________________________________________________________________\n","sequential_35 (Sequential)   (None, 8, 8, 64)          148480    \n","_________________________________________________________________\n","sequential_36 (Sequential)   (None, 4, 4, 128)         526848    \n","_________________________________________________________________\n","sequential_38 (Sequential)   (None, 2, 2, 256)         2102272   \n","_________________________________________________________________\n","sequential_40 (Sequential)   (None, 1, 1, 512)         8398848   \n","_________________________________________________________________\n","global_average_pooling2d_5 ( multiple                  0         \n","_________________________________________________________________\n","dense_5 (Dense)              multiple                  5130      \n","=================================================================\n","Total params: 11,191,242\n","Trainable params: 11,181,642\n","Non-trainable params: 9,600\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-U05nmFDNziy","executionInfo":{"elapsed":630,"status":"ok","timestamp":1612445253449,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"c6e3925f-ce8f-4fbc-f527-be118c4b9ed1"},"source":["from torchsummary import summary\n","\n","summary(my_resnet, input_size=(3, 32, 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           9,408\n","       BatchNorm2d-2           [-1, 64, 16, 16]             128\n","              ReLU-3           [-1, 64, 16, 16]               0\n","         MaxPool2d-4             [-1, 64, 8, 8]               0\n","            Conv2d-5             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-6             [-1, 64, 8, 8]             128\n","              ReLU-7             [-1, 64, 8, 8]               0\n","            Conv2d-8             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-9             [-1, 64, 8, 8]             128\n","             ReLU-10             [-1, 64, 8, 8]               0\n","       BasicBlock-11             [-1, 64, 8, 8]               0\n","           Conv2d-12             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-13             [-1, 64, 8, 8]             128\n","             ReLU-14             [-1, 64, 8, 8]               0\n","           Conv2d-15             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-16             [-1, 64, 8, 8]             128\n","             ReLU-17             [-1, 64, 8, 8]               0\n","       BasicBlock-18             [-1, 64, 8, 8]               0\n","           Conv2d-19            [-1, 128, 4, 4]          73,728\n","      BatchNorm2d-20            [-1, 128, 4, 4]             256\n","             ReLU-21            [-1, 128, 4, 4]               0\n","           Conv2d-22            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-23            [-1, 128, 4, 4]             256\n","           Conv2d-24            [-1, 128, 4, 4]           8,192\n","      BatchNorm2d-25            [-1, 128, 4, 4]             256\n","             ReLU-26            [-1, 128, 4, 4]               0\n","       BasicBlock-27            [-1, 128, 4, 4]               0\n","           Conv2d-28            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-29            [-1, 128, 4, 4]             256\n","             ReLU-30            [-1, 128, 4, 4]               0\n","           Conv2d-31            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-32            [-1, 128, 4, 4]             256\n","             ReLU-33            [-1, 128, 4, 4]               0\n","       BasicBlock-34            [-1, 128, 4, 4]               0\n","           Conv2d-35            [-1, 256, 2, 2]         294,912\n","      BatchNorm2d-36            [-1, 256, 2, 2]             512\n","             ReLU-37            [-1, 256, 2, 2]               0\n","           Conv2d-38            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-39            [-1, 256, 2, 2]             512\n","           Conv2d-40            [-1, 256, 2, 2]          32,768\n","      BatchNorm2d-41            [-1, 256, 2, 2]             512\n","             ReLU-42            [-1, 256, 2, 2]               0\n","       BasicBlock-43            [-1, 256, 2, 2]               0\n","           Conv2d-44            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-45            [-1, 256, 2, 2]             512\n","             ReLU-46            [-1, 256, 2, 2]               0\n","           Conv2d-47            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-48            [-1, 256, 2, 2]             512\n","             ReLU-49            [-1, 256, 2, 2]               0\n","       BasicBlock-50            [-1, 256, 2, 2]               0\n","           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n","             ReLU-53            [-1, 512, 1, 1]               0\n","           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n","           Conv2d-56            [-1, 512, 1, 1]         131,072\n","      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n","             ReLU-58            [-1, 512, 1, 1]               0\n","       BasicBlock-59            [-1, 512, 1, 1]               0\n","           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n","             ReLU-62            [-1, 512, 1, 1]               0\n","           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n","             ReLU-65            [-1, 512, 1, 1]               0\n","       BasicBlock-66            [-1, 512, 1, 1]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                   [-1, 10]           5,130\n","================================================================\n","Total params: 11,181,642\n","Trainable params: 11,181,642\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.29\n","Params size (MB): 42.65\n","Estimated Total Size (MB): 43.95\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"id":"qNVvHDnjOM7r","executionInfo":{"elapsed":643,"status":"error","timestamp":1612447897276,"user":{"displayName":"Heecheol Cho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjX329YmmK5EOCLjZGYupvhhyKPjyr-kS3DftPICg=s64","userId":"11416361730564561965"},"user_tz":-540},"outputId":"1b8a4a22-703e-4894-c431-1b93ca76b066"},"source":["model2 = ResNet18()\n","model2.build(input_shape=(None,32,32,3))\n","model2.summary()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-2e90d877a9d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    406\u001b[0m               \u001b[0;31m# Has invalid call signature with unknown positional arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m               raise ValueError(\n\u001b[0;32m--> 408\u001b[0;31m                   \u001b[0;34m'Currently, you cannot build your model if it has '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                   \u001b[0;34m'positional or keyword arguments that are not '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                   \u001b[0;34m'inputs to the model, but are required for its '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Currently, you cannot build your model if it has positional or keyword arguments that are not inputs to the model, but are required for its `call` method. Instead, in order to instantiate and build your model, `call` your model on real tensor data with all expected call arguments."]}]},{"cell_type":"code","metadata":{"id":"AGGeIlb3YdLY"},"source":[""],"execution_count":null,"outputs":[]}]}